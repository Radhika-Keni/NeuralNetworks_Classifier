{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClassificationUsingNeuralNetworks_MultiDigitNumbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set Provided\n",
    "\n",
    " https://drive.google.com/file/d/1PEXoZmxT1MH9AuatTG4JuuzjcCU0Q2oS/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context:\n",
    "A Recognising multi-digit numbers in photographs captured at street level is an \n",
    "important component of modern-day map making. A classic example of a corpus of such street-level \n",
    "photographs is Googleâ€™s Street View imagery composed of hundreds of millions of geo-located 360-degree \n",
    "panoramic images. \n",
    "The ability to automatically transcribe an address number from a geo-located patch of pixels and associate the \n",
    "transcribed number with a known street address helps pinpoint, with a high degree of accuracy, the location of \n",
    "the building it represents. More broadly, recognising numbers in photographs is a problem of interest to the \n",
    "optical character recognition community. \n",
    "While OCR on constrained domains like document processing is well studied, arbitrary multi-character text \n",
    "recognition in photographs is still highly challenging. This difficulty arises due to the wide variability in the \n",
    "visual appearance of text in the wild on account of a large range of fonts, colours, styles, orientations, and \n",
    "character arrangements. \n",
    "The recognition problem is further complicated by environmental factors such as lighting, shadows, \n",
    "specularity, and occlusions as well as by image acquisition factors such as resolution, motion, and focus blurs. \n",
    "In this project, we will use the dataset with images centred around a single digit (many of the images do \n",
    "contain some distractors at the sides). Although we are taking a sample of the data which is simpler, it is more \n",
    "complex than MNIST because of the distractors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data:\n",
    "The SVHN is a real-world image dataset for developing machine learning and object \n",
    "recognition algorithms with the minimal requirement on data formatting but comes from a significantly harder, \n",
    "unsolved, real-world problem (recognising digits and numbers in natural scene images). SVHN is obtained from \n",
    "house numbers in Google Street View images.\n",
    "Where the labels for each of this image are the prominent number in that image i.e. 2,6,7 and 4 respectively.\n",
    "The dataset has been provided in the form of h5py files. You can read about this file format here: http://\n",
    "docs.h5py.org/en/stable/high/dataset.html\n",
    "Acknowledgement: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng Reading \n",
    "Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop on Deep Learning and \n",
    "Unsupervised Feature Learning 2011. PDF\n",
    "http://ufldl.stanford.edu/housenumbers as the URL for this site when necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legend\n",
    "\n",
    " ### Insights/inferences/results have been displayed post each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statistics as stats\n",
    "sns.set(color_codes=True)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#Last import allows multiple outputs from one cell\n",
    "import warnings\n",
    "# Initialize the random number generator\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Configuration/Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress display of warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# display all dataframe columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# to set the limit to 3 decimals\n",
    "pd.options.display.float_format = '{:.7f}'.format\n",
    "\n",
    "# display all dataframe rows\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "#Setting to shows all entries in array displayed\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read the data as a data frame & make a copy of DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read The Data Set\n",
    "#signalDataDSOrig = pd.read_csv(\"../DataSet_Storage/SignalData_DontModify.csv\")\n",
    "#signalDataDS=signalDataDSOrig.copy()\n",
    "#signalDataDS.head(5)\n",
    "import h5py\n",
    "imageDataFile = h5py.File('Part - 4 - Autonomous_Vehicles_SVHN_single_grey1.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(imageDataFile.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets store in respective variables \n",
    "X_Train=imageDataFile['X_train']\n",
    "X_Test=imageDataFile['X_test']\n",
    "X_Val=imageDataFile['X_val']\n",
    "\n",
    "Y_Train=imageDataFile['y_train']\n",
    "Y_Test=imageDataFile['y_test']\n",
    "Y_Val=imageDataFile['y_val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Check Shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 32, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(18000, 32, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(60000, 32, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(42000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(18000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at all the shapes \n",
    "X_Train.shape\n",
    "X_Test.shape\n",
    "X_Val.shape\n",
    "\n",
    "Y_Train.shape\n",
    "Y_Test.shape\n",
    "Y_Val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Get information about the features/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<f4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('<f4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('<f4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "h5py._hl.dataset.Dataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "h5py._hl.dataset.Dataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the data type of elemtns aoctained in X_Train and simialrly others\n",
    "X_Train.dtype\n",
    "X_Test.dtype\n",
    "X_Val.dtype\n",
    "Y_Train.dtype\n",
    "Y_Test.dtype\n",
    "Y_Val.dtype\n",
    "\n",
    "# What is the type of X_Train and simialrly others\n",
    "type(X_Train)\n",
    "type(Y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.5 Lets view the data/Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cfa4215850>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgtklEQVR4nO2deXAU5/H3vwKEuM0lECAZ24BjDBgTTmFjISgWrMOYyxFgCCG2SQVI2UUVBRQuclSIiuBQwdgVlw0hBGxMEiCgcJY5AogjXOIIxoARhxEgLnNJK4H2/YNX8xsN072rkcSKPN/PP5rn6X1mnp2dnmfUPd0dEQgEAiCEGEOVcE+AEPJoodITYhhUekIMg0pPiGFQ6QkxDCo9IYZRJqVfvXo1kpKS4PP5sGTJkvKaEyGkAqnmdeClS5cwZ84cLF++HNWrV0daWhq6d++O1q1bhzR+//798Pv9iI+Px86dO0ObbDX36WqvGty/f1+UVa1a1ZOsShX3e6Xf7xfHaNj3161bN+zZs8dqx8TEiOOaN28uyrycq6KiIlGmnceIiAhRVlBQUOp53Lt3z9quX78+bty4YbWl7wV4O//Vq1cXZdLvXBa083ju3DnXfvs57NSpEw4cOGC1a9So8dDnIyMjVT30rPSZmZno0aMH6tevDwDo378/1q1bhwkTJoQ03u/3Iz8/HwCsv8GIjIx07Q/1AnKiXUDhUnrnfrT5Pw5Iv01pbj72ttebloS2v4p4b03bp3SDdF5X9rZ2nUp4VvrLly8jOjraajdp0gSHDh0KeXx8fLy1nZiY6HUa/5P06tWrwvatrcpen3w0pBt1aWjYsGFIn6tbt26ZjxVOOnXqFNLnevToUabjeFb6oqKiEhdQIBBQLygnO3fuRH5+PhITE7F58+aQxpiw0vfq1Qvbtm2z2i1atBDHPfnkk6KssjzeS+dEm0dhYaG13bBhQ1y7ds1qazeRUJ8Y7URFRYmyR/14/+2337r2289hjx49sGvXLqtdq1athz4fGRmJtm3bisfx/K1iYmKQm5trtXNzc9GkSROvuyOEPCI8r/Q9e/bEhx9+iGvXrqFmzZrYsGEDfvOb34Q8PhAIWHd7+11fWzWku6R2R9bu5NrKps3Dy/+ppdmfvV3e/9N7/T9Vm0dpnvBCmYfzKcve1o5Vs2ZNUSZdI15Xeu182J9UnHgxOjufYOxtt3lo3wkog9I3bdoU7733HkaPHo3CwkIMHToUL7zwgtfdEUIeEZ6VHgBSU1ORmppaXnMhhDwC+EYeIYZBpSfEMKj0hBgGlZ4QwyiTIa8sFBUVWS6zUF+zlNw12gsbXlxvwZD26cXdCDz8AsudO3esbe2FE/vnnLi9kw3o7hyvLkfp9VFAfsmpNG/q2d+P116aunLliig7e/asa7/mert9+7You3Xrlqd55OTklPp49mP17t27RHCbPS6hmOjoaHTt2lU8Dld6QgyDSk+IYVDpCTEMKj0hhkGlJ8Qwwma9r1KlihXQYA9s0KyzkgXca3YczYKsWaQl6+yJEyfEMcePHxdlV69etbZfffVV/OlPf7LamrX9mWeeEWVSbLZm1X366adFmZdgFkD2CGhWc+cY+2dv3rwpjvvLX/4iypYvXy7KJLQ5al4VrzLpXDk9JxkZGaIMAGJjY8VjAFzpCTEOKj0hhkGlJ8QwqPSEGAaVnhDDoNITYhhhc9lFRkZagTb2gAovecg1l5GWr8ye2NPJ/v37RZk9W62dU6dOiWO0fHzOOYZa/GPt2rWi7IknnnDt79atmzhm7Nixoqx3796izC0jazGa69MLWnEKzZ2nuUy9HEtzE3stoBFqNmW728/tusrLy1PHc6UnxDCo9IQYBpWeEMOg0hNiGFR6QgyDSk+IYYTNZZeXl2e5Hu7evWv1a7nYJFmdOnXEMVq+so0bN4oyzW0m5TnTctZpbhynzB4ZWJryT3akHG4bNmwQx0gFFAFg5MiRomzYsGGirGnTpq792vdyFt+0t7Xro1GjRqKsuKS6E82lqEUWei0EqkVNxsTEuPY3aNCgRNseQel2LGk/xZRJ6UeNGoVr165ZP8qvf/1rdOzYsSy7JIRUMJ6VPhAIIDs7G5s3b1ZLPhNCKhee/6cvfhQcO3YsXnvtNSxevLjcJkUIqTgiAh6Tvx84cABffPEF3n//fRQWFmL06NGYOnUqXnrppfKeIyGkHPGs9E4WLlyICxcuYNq0aSF9/t///jfy8/Ph8/lKGJcepSFvzZo1oixchrycnBw0a9bMams/j5fUS9o7461btxZl5W3I01JR2f9drFmzZol3ybX529OMOZk7d65rf2Uy5EnXsd2Qt2nTJvTp00c9VkxMDJYuXSoex/Pj/d69e0soRiAQ4P/2hDwGeNbSW7duYe7cuVi6dCkKCwuxYsUK/OpXvwp5vN/vt1Yq+4olRYcBciJLrfzQnj17RNnmzZtF2aVLl0SZdLdu0aKFOMbpdrFTt27dEu0OHTpY21oEmxZVdu7cOdd+7WkkOztblH3yySei7Pr166Ls5z//uWt/48aNxTHO1cu+qmqraEJCgiiTXHbayiuVBgP0aDntqU7bp/RkUbt27RLtDz74wNr+7rvvQt5PMZ6VPjExEVlZWXj99ddRVFSEESNGiBlYCSGVhzI9j7/77rt49913y2kqhJBHAV/DJcQwqPSEGAaVnhDDoNITYhhhc6xHRUVZrhi7G0NzQ0nuDs29prns7DXknGh17qSXKHw+nzjm5ZdfFmX2l3EA4P3337e2NReblthz+/btrv1ZWVnimJMnT4oybR7aeZRcW6VJFGpva0kf27ZtK8rat28vyiS0Y2kuO69IUZP2KFQAaN68ubW9ZcuWhz5fr1499Thc6QkxDCo9IYZBpSfEMKj0hBgGlZ4Qwwib9T4iIsIKldXCae1IFuSvv/5aHKMFkWjBFlrQRPfu3V377SGPTrRgHGe4qOY5sKMFrUihsJoXQSuTpYXC/vSnPxVl0hydFmntWHbrvddITikkV/temoVey0+o7VMrsyYFV124cMHabtq0aYm2W57HJk2aqL8JV3pCDINKT4hhUOkJMQwqPSGGQaUnxDCo9IQYRthcdkVFRVbQhT34QnOVSYEd33zzjThGC+DR3Cdarr7OnTu79mu5ybQ8fk4Xj70klRboomWGldw/8fHx4piePXuKMr/fL8qcAUN2pKCV0mTytf9OWs5A7VxJaOdQc52G6mZ2ol3fUmbeHTt2WNudOnUq0XbLkRcswTVXekIMg0pPiGFQ6QkxDCo9IYZBpSfEMKj0hBhG2Fx2VapUsaKY7NFMmktDcvPYo46caBFPmrtGK0MVLAeZG5obxRnNZW87S17Z0YovSmjnt2HDhqJMizjT5iFF02nuUs1Vpo3T5ii5HLVoOe03064dDW2fp06dcu3PzMy0tidMmFCi/f333z/0ee2aAUJc6W/fvo2UlBScP3/emkRqaip8Ph/mzJkTyi4IIZWEoEqflZWF4cOHW3Hp+fn5mDZtGj7++GOsWbMGR44cwdatWyt6noSQciKo0i9btgwzZsxAkyZNAACHDh1Cy5YtERcXh2rVqiE1NRXr1q2r8IkSQsqHiECwd/b+P3369MGiRYtw8OBBbNmyBbNnzwbw4FH/s88+w4IFCyp0ooSQ8qHUhryioqIS7x0HAgFP7yFv27YN+fn56NevX4mUP5qhSTJ0aDcc7b18zcjXrl07UTZmzBjX/tjYWHGMZpyy33f79u2Lr776ymprBiPNgCYZc55++mlxTEUY8qR370M15DVr1gw5OTmuMifa+uXFkKfJvKbt0pCub7vd7PPPP8eIESOstt2oV0xsbKxY7ATw4LKLiYkpUVklNzfXevQnhFR+Sn276tixI06fPo0zZ84gNjYWGRkZGDJkSKkPXFRUZK1i9tVMu1tfu3bNtf/GjRviGG2F0lb6pk2birJGjRq59msrnlaCyumKPH36dEj7tEfjOalfv75rv9fosNq1a4syrUSVtMJqY7y67LTfU3qC1K4Pr09Z2lOAdv7379/v2n/48GGx7XYNaBGdgAelj4qKQnp6OiZOnAi/34+EhAQMGDCgtLshhISJkJV+06ZN1nZ8fDxWrVpVIRMihFQsfA2XEMOg0hNiGFR6QgyDSk+IYYQtyu7evXuWi8XuatFcOVLEluZa0RIwai9faJFKkuswKytLHHPixAlRZk/e+dZbb2HhwoVWW3q5BdDrwUluI+0FHO2FpB/+8IeiTEoUCgDR0dGu/ZqrzOnWsrc1d5jmzpNk2jWgoc2/Ro0aokyrrbhlyxbXfmcknb3tNg9tbgBXekKMg0pPiGFQ6QkxDCo9IYZBpSfEMKj0hBhG2Fx2tWrVslwL9gguL24Xr0kWNZnmYpOiob799ltxjOYacrqojh49Kn42VKTj2WPTnWjH3bx5syjTAq5+8pOfuPa3aNFCHOM8H/YIOc0dpcmka0RzEWvuQS+uZQA4efKkKDtw4IBrv3Pu9nZUVNRDn9dyUgBc6QkxDio9IYZBpSfEMKj0hBgGlZ4Qwwib9T4iIsKy0tqttVruMS/BEZoFVjuWZomXxmn5z7RjOQM07Ptxs84Wo303KdBI2582fynICABWrlwpyqR8ggMHDhTHaJZ9LQ+eRrAglNIeSzv3bqWmitm9e7cok/IJatb7p5566qHPx8TEiMcAuNITYhxUekIMg0pPiGFQ6QkxDCo9IYZBpSfEMMLmspPwEgCh5bPTgh+0Elqae1Bye2kBPFqZrPbt25doJyUlWdtanUAtsEJyG126dEkcIxVQBICLFy+W+lgAsHTpUtd+zeVlL5PWsGFDXL161Wo3btxYHKchXVfa9aahXTtaAFjNmjVFmXTNaTkDO3To8NDnJTdpMSGt9Ldv30ZKSgrOnz8PAJg6dSp8Ph8GDhyIgQMHlqg6Swip3ARd6bOysjB9+vQSWTyPHDmCxYsXs1otIY8hQVf6ZcuWYcaMGZaC5+Xl4cKFC5g2bRpSU1Mxd+5cz49IhJBHT0RA++fERp8+fbBo0SIEAgGkp6djxowZqFu3LsaNG4eUlBS88cYbFT1XQkg5UGpDXlxcHD766COrPWrUKKxcubLUSr9r1y7k5+ejd+/eJZL82ws/ODl06JBr/z/+8Q9xjGa4kt51Bh5k9pHwYhQK1ZC3ePFivPnmm1b7cTDkaYar5s2bu/bbv6MTuyGvTZs2JbIYaYa8O3fulHqOXp9SNUOkNo+///3vomzRokWu/fbfMjc3t0QBkUGDBj30+UaNGuF3v/udeJxSu+yOHz+O9evXW+1AIKCeAEJI5aLU2hoIBDBz5kz06NEDtWrVwpdfful6twlGjRo1rMgn+6p669YtcYzkEtNuOtp/L1oUlbYCSCvsc889J4559dVXRdkPfvCDEu1Ro0ZZ21p0noY0f3s+QidayaXPP/9clEm53QDg9OnTrv3/+te/xDHPPvustd2mTRscPnzYasfHx4vjSlMqK1g/oF8f2nWluW7t7lgnzZo1c+0/duxYibb9+oiNjX3o8/Xq1ROPAXhQ+ueeew7vvPMOhg8fjnv37sHn8yElJaW0uyGEhImQlX7Tpk3W9siRIzFy5MgKmRAhpGLha7iEGAaVnhDDoNITYhhUekIMI2wO9oKCAuvlGPtLMlriRsnd5LX8kOau0dw/ffv2de3v1q2bOEZLVug8lr2tuYbu378vyiRXnxaRmJCQIMq0cb/85S9F2dmzZ137JVceUPIlrMGDB5dod+nSRRznBe131q4d7XfRrkft5aLu3bu79jtdeX369LG23VzcmlsW4EpPiHFQ6QkxDCo9IYZBpSfEMKj0hBgGlZ4Qwwiby66oqMhyidhdI5oLxVnzrRgt2aAWc665vLT4986dO7v2S1FSgO7+cUZz2dvaHLWIRGmclq9Akz3//POi7JVXXhFlUq4DLZmms46gvS3V6AP06DYpYs5LZF4wtMhI7ffU8hJIuLkHg9V85EpPiGFQ6QkxDCo9IYZBpSfEMKj0hBhG2Kz3N2/etEpOXb9+/f8mpAQrSNZZLcBAs6RqsgYNGogyyYug5VSTxgAPW1uDWV+L0SzBkkwr85WXlyfKtICbuLg4USZ9F83j4vRK2Nv2a8WJljlYstJrFnrtd9DOlTbOi/Xeee3Y227ejGBeB670hBgGlZ4Qw6DSE2IYVHpCDINKT4hhUOkJMYxKF3CjudHq1Knj2q/ln9NymXl1rUgyrzn3nONCDfTQimxK+9Bcopo7T3NHeuH27dshz8Pe1uav5VeU3GFeS5tpv5F27XhxETrnYW+7uQ61aw0IcaWfN28ekpOTkZycjFmzZgEAMjMzkZqaCp/Phzlz5oSyG0JIJSCo0mdmZmL79u1YsWIFVq5ciaNHjyIjIwPTpk3Dxx9/jDVr1uDIkSPYunXro5gvIaSMBFX66OhoTJkyBdWrV0dkZCRatWqF7OxstGzZEnFxcahWrRpSU1Oxbt26RzFfQkgZiQho//Q6yM7OxvDhw/Hmm2/i9OnTmD17NoAHTwOfffYZFixYUGETJYSUDyEb8k6cOIFx48Zh8uTJqFq1aola5oFAoNRZRtatW4e7d+9i8ODBWL58eUhjpPuT9pTxz3/+U5RpmUo6deokykaMGOHarxUy0N41txte+vXrh40bN1ptzeClGT29GPIkQyngXge9GO33+/DDD137taw/vXr1srbXr1+P/v37W+2ZM2eK47TMRV4MeV6KiQC68ViT2Yu+2LHHG/h8PmzYsMFqX7p06aHP165dG4MHDxaPE5Ihb9++fRgzZgwmTZqEQYMGISYmBrm5uZY8NzdXDXYghFQegq70OTk5GD9+PObMmYP4+HgAQMeOHXH69GmcOXMGsbGxyMjIwJAhQ0p1YMllp7nRpLuyFhGnubW0iC37Tc3J5cuXSz0PLSrLuSrbXVTaihLMNeOGtr8rV66IMu13uXbtmiiTniy0Jx+n683e9vKdAXnV1p44zpw5I8rq168vyqKjo0s9D03mfDooxX/krgRV+vnz58Pv9yM9Pd3qS0tLQ3p6OiZOnAi/34+EhAQMGDCgTBMhhDwagir99OnTMX36dFfZqlWryn1ChJCKha/hEmIYVHpCDINKT4hhUOkJMYywRdndv3/fclHYXRWaS0aKetJeimnUqJEou3HjhijLyckRZceOHXPt1xJEaq5D53cOtcyX9qKNlxJJ2gtW3333nSg7f/68KJPKUGmJQp966imxXa9ePXGc5g67c+eOa39GRoY45quvvhJl/fr1E2WDBg0SZdpvJsm0xKlu31mLDgS40hNiHFR6QgyDSk+IYVDpCTEMKj0hhkGlJ8Qwwuayu3fvnuVWCtW9JLl5tIgtTSbFLwN6JJPk6tO+h7Y/Z8y8va1Ft2nHk9w2mqtMqhUIAKdOnRJlx48fF2VSFJvmetNcdl6iMAH3uHMAapq3AwcOiDLN9da+fXtR1q5dO1EmuRWdx7K3GzZs+NDntWse4EpPiHFQ6QkxDCo9IYZBpSfEMKj0hBhG2Kz3tWvXtoJJ6tatG9IYyXKrBbq8+OKLoiwrK0uUaYEuFy9edO3XglK0TLNaQIUWBOMlk6v2vbT9/fe//xVlkmUcAKpXr+7a/+STT4pjNOu9FlTjpQxVaX4XO5plf9myZaIsLS1NlEmBY87vbG+7fedgOfS40hNiGFR6QgyDSk+IYVDpCTEMKj0hhkGlJ8Qwwuaya9GihVViye6+0YI+vKC5k2JiYkSZVLoKAE6ePOnar7lx2rRpI8qcLkd7O1jwhITk2tIKYm7btk2UaYEp2jl2CwgBgJdeekkc4zxX9naw/G8STZs2de3v3LmzOGbv3r2iTPvOWkFVyd0LAH379nXtb926dYm2vYyY5BLVCEnp582bh7Vr1wIAEhISMHnyZEydOhX79u2zLsoJEyaoyQIJIZWDoEqfmZmJ7du3Y8WKFYiIiMBbb72FjRs34siRI1i8eDGr1RLymBH0f/ro6GhMmTIF1atXR2RkJFq1aoULFy7gwoULmDZtGlJTUzF37lzPj12EkEdLRKAUdW+zs7MxfPhwLFmyBB988AFmzJiBunXrYty4cUhJScEbb7xRkXMlhJQDISv9iRMnMG7cOEycOPGhZP4bN27EypUr8dFHH4V84KNHj6KgoACdOnUqYQArb0Pejh07RNns2bNFmWbIk95fT0xMFMdoN8QWLVpY27169SphUKsshrwlS5aIMsmwCciGvHHjxoljhg8fbm136NABhw8fttpa5h8NKUvSypUrxTGffPKJKNMMeVpWIC0WJBRD3rBhw/C3v/3NarsZ8mrWrAmfzyceJySX3b59+zBmzBhMmjQJgwYNwvHjx7F+/XpLHggE1PRBhJDKQ1BNzcnJwfjx4zFnzhzEx8cDeKDkM2fORI8ePVCrVi18+eWXaikfN2rWrGmt6rVr1/6/CXko+3P37l1xjGZo1PKVaa4VyX5x6NAhcYzmsnO6k+zRbvZz40SLisvNzXXt11xv9hXEiVa6qn79+qJMcon17NlTHOOMfLO3te+s5TyU6NWrlyjTojC1kld5eXmibNeuXaLsm2++ce1/9tlnre1hw4aVeAJxi1Zs1KiRutIHVfr58+fD7/cjPT3d6ktLS8M777yD4cOH4969e/D5fEhJSQm2K0JIJSCo0k+fPh3Tp093lY0cObLcJ0QIqVj4Gi4hhkGlJ8QwqPSEGAaVnhDDqHTOde113u+//961X0seqSVglF6GAICrV6+Ksq+//tq1/8qVK+IY7SUQe3mqxMRErF692mpL0WEAcO7cOVF28OBB137te509e1aUaa6yZ555RpQlJSW59jsjx7Rj2dtekoECcgmwRo0aiWOGDh0qyqQSVIDulpPKfAGym/j69esl2rt377a29+/f/9Dn4+Li8Pvf/148Dld6QgyDSk+IYVDpCTEMKj0hhkGlJ8QwqPSEGEbYXHb379+33C92N4zmdpFqi3mpYQYAXbp0EWX5+fmi7ObNm679WiSa5G4EgBUrVljbf/jDH0q0tahDe4JEJ9L8tVhvLVZdc31qEZbSOS5NjT57W4tgK0U+GAstMq9Dhw6ibMKECaJMc2Fq0XlSDgenTtj1wC3fQrCcA1zpCTEMKj0hhkGlJ8QwqPSEGAaVnhDDoNITYhhhc9nduXPHqmVnT8ssueUAICoqyrVfG6O5cbRxmjtPqh9mzxDsRIp6Ax5O7Glva0k/tYgzydWn1T4rTnzqRu/evT2Nk1Kaay5R529md6tprj4vaCnXNVnbtm1FmZT2G9ATpGZmZrr2O9Nt29Nou7mCtchBgCs9IcZBpSfEMKj0hBgGlZ4Qw6DSE2IYYbPeR0REWJZYu0VWC7iRLL6alVWzcEt50wC5SCXwIAeZG1I+OEAP3nAGzrz22mvWthYQInkzAKBx48au/c8//7w4plWrVqJMs0hr+eKkYCjt/GoBN5r3wctv7bXEuhYIpf0uLVu2FGUNGjRw7XfOccyYMda2W0HSunXriscAQlzp//jHPyIpKQnJycn485//DOCBeyE1NRU+nw9z5swJZTeEkEpA0JV+z5492LVrF1atWoV79+4hKSkJ8fHxmDZtGv7617+iWbNmGDduHLZu3YqEhIRHMWdCSBkIutJ369YNixYtQrVq1XD16lXcv38fN2/eRMuWLREXF4dq1aohNTUV69atexTzJYSUkZD+p4+MjMTcuXOxYMECDBgwAJcvX0Z0dLQlb9KkyUNvDQWjY8eO1nbXrl1LNfZ/nU8//TTcU6hUaG9HPm506tSpzPuw/0/vhZANeb/4xS/w9ttv42c/+xmys7NLGN8CgUCpX4/MyspCQUEBunbtiv/85z8l9iUhGX80Q572uqdmyNMMilLBiAsXLohjtMIUdkPep59+irfffttqP+6GPOm6CNWQ16VLF+zdu9dqezXkFb/y7cTLa9+AbsjTfrMzZ86IMikjk92QN2bMGCxcuNBqS4a8H//4x+Jxgj7enzp1CseOHQPwIDWPz+fD7t27kZuba30mNzcXTZo0CbYrQkglIOhKf/78ecydOxdffPEFgAc5vtLS0jBr1iycOXMGsbGxyMjIwJAhQ0p1YL/fb90RtdXYjrQya3d/7cnB7S5ZjDYnyc3zxBNPiGO0vGXOJ5WXX37Z2tZWWO1GK50TbYXSnpi0Jznt/EvnUdufc/W1t7VxXoKrtJVew8tTBaA/XUrBVc7fxf4U6nZ91K5dWzwGEILSJyQk4NChQ3j99ddRtWpV+Hw+JCcno2HDhpg4cSL8fj8SEhIwYMCAYLsihFQCQvqffuLEiZg4cWKJvvj4eKxatapCJkUIqTj4Gi4hhkGlJ8QwqPSEGEbYAm7sFl/NH2pHsrRqVmfNoqv54jU/sjROG6PN0WlRt1tf3SqYFKNZzSWZZq3W5qihnWPJ06F5EZzYv4tXa7s0D21/Xo+lXVe1atUSZZLV33mu6tSpY227eYW0awYAIgJeagERQh5b+HhPiGFQ6QkxDCo9IYZBpSfEMKj0hBgGlZ4Qw6DSE2IYVHpCDINKT4hhUOkJMYywKv3q1auRlJQEn8+HJUuWhHMqYeP27dtISUnB+fPnAZhdT2DevHlITk5GcnIyZs2aBcDs8wFUUM2JQJi4ePFiIDExMXD9+vXAnTt3AqmpqYETJ06Eazph4eDBg4GUlJRAu3btAufOnQvk5eUFEhISAmfPng0UFhYGxo4dG9iyZUu4p/lI2LFjR+BHP/pRwO/3BwoKCgKjR48OrF692tjzEQgEArt37w6kpaUFCgsLA3l5eYHExMTAsWPHynxOwrbSZ2ZmokePHqhfvz5q1aqF/v37G5c7f9myZZgxY4aV6+7QoUPG1hOIjo7GlClTUL16dURGRqJVq1bIzs429nwAFVdzImxKXx658x93fvvb35bI6W7yOWnTpg1efPFFAEB2djbWrl2LiIgIY89HMcU1J5KTkxEfH18u10jYlL6oqKjMufP/1+A5AU6cOIGxY8di8uTJiIuLM/58AA9qTuzcuRM5OTnlUnMibEofExPD3PkOTD8n+/btw5gxYzBp0iQMGjTI+PNRUTUnwqb0PXv2xM6dO3Ht2jXk5eVhw4YNeOWVV8I1nUpBx44dcfr0aZw5cwb3799HRkaGMeckJycH48ePx+zZs5GcnAzA7PMBPKg5MX36dBQUFKCgoMCqOVHWcxK2dFlNmzbFe++9h9GjR6OwsBBDhw7FCy+8EK7pVAqioqKQnp5uZD2B+fPnw+/3Iz093epLS0sz9nwAFVdzgumyCDEMvpFHiGFQ6QkxDCo9IYZBpSfEMKj0hBgGlZ4Qw6DSE2IY/w942MZfgEVFwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets choose some random data and view it \n",
    "print(\"Label: {}\".format(Y_Train[1]))\n",
    "plt.imshow(X_Train[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 86.9591,  87.0685,  88.3735,  88.2272,  87.0424,  88.0746,\n",
       "         89.0637,  89.6399,  87.6401,  87.4121,  88.1239,  96.2632,\n",
       "        105.2515, 106.7306, 106.2037, 105.6598, 104.8386, 105.0281,\n",
       "        103.224 , 102.8003, 101.2627, 102.7248, 100.2951, 100.3813,\n",
       "        102.3811, 103.0436, 102.8695, 103.2994, 101.9144,  91.8014,\n",
       "         89.7477,  92.5302],\n",
       "       [ 86.688 ,  86.9114,  87.4337,  88.1517,  88.9236,  88.6508,\n",
       "         89.2639,  89.324 ,  87.2442,  87.5431,  87.922 ,  95.2293,\n",
       "        105.2884, 106.4425, 105.9587, 105.5288, 103.5829, 103.9465,\n",
       "        103.3703, 102.4736, 102.0499, 103.3379, 102.8048, 103.4287,\n",
       "        105.6134, 106.2282, 105.0003, 103.4735, 102.5337,  90.7306,\n",
       "         87.204 ,  88.5629],\n",
       "       [ 85.9654,  85.8145,  85.9239,  87.386 ,  88.7988,  88.1948,\n",
       "         89.4658,  89.7647,  87.6509,  86.5092,  84.6064,  92.6748,\n",
       "        102.19  , 104.441 , 106.6257, 106.2344, 103.7185, 102.0068,\n",
       "        102.8326, 103.7616, 103.8047, 104.8046,  98.7882,  89.5872,\n",
       "         88.3485,  84.785 ,  80.7423,  74.7968,  67.2644,  63.8626,\n",
       "         59.8199,  54.8805],\n",
       "       [ 86.6726,  87.4076,  86.7559,  86.6788,  88.8465,  89.6785,\n",
       "         87.9129,  81.9396,  70.4768,  59.1512,  52.93  ,  55.1901,\n",
       "         65.5141,  80.3277,  96.1798, 106.3422, 106.484 , 104.4195,\n",
       "        104.0775, 103.0067, 102.6908,  99.6911,  70.0469,  45.4237,\n",
       "         45.2881,  43.9955,  41.7677,  39.4305,  36.6095,  36.3214,\n",
       "         36.6804,  37.1103],\n",
       "       [ 88.5153,  88.1086,  87.2828,  87.9175,  87.0254,  82.0043,\n",
       "         67.0982,  48.5884,  38.0177,  37.3059,  35.9101,  35.7791,\n",
       "         33.0012,  37.6417,  54.053 ,  83.5339, 103.0328, 106.0495,\n",
       "        104.0775, 102.8927, 102.7509, 101.4521,  65.6945,  33.5667,\n",
       "         33.5667,  35.2137,  34.2631,  36.078 ,  36.6696,  34.4356,\n",
       "         34.3817,  33.8117],\n",
       "       [ 85.3523,  85.6897,  86.1627,  89.6893,  83.847 ,  61.3268,\n",
       "         44.4039,  36.437 ,  33.0244,  35.0242,  36.665 ,  36.534 ,\n",
       "         34.1752,  33.6591,  32.7732,  43.5872,  71.1284,  96.9795,\n",
       "        103.9034, 102.7186, 103.8217, 103.2347,  71.5799,  38.1533,\n",
       "         36.5664,  39.5122,  38.8775,  38.4045,  40.68  ,  43.9616,\n",
       "         49.3201,  48.4404],\n",
       "       [ 82.8687,  87.6294,  89.0484,  85.9239,  67.4141,  41.8233,\n",
       "         34.7423,  36.796 ,  41.1654,  46.3498,  50.3925,  53.3213,\n",
       "         49.3047,  40.3657,  37.024 ,  34.0351,  40.0884,  66.4987,\n",
       "         98.0781, 104.1484, 103.6538, 102.469 ,  90.595 ,  76.3514,\n",
       "         78.5684,  80.9102,  81.6282,  83.4108,  85.6432,  88.7399,\n",
       "         91.6749,  83.839 ],\n",
       "       [ 85.4169,  83.1999,  85.7436,  79.1356,  50.844 ,  35.9209,\n",
       "         36.095 ,  40.2903,  62.5439,  80.4559,  88.0099,  84.7652,\n",
       "         78.5639,  60.4301,  46.0571,  35.0951,  36.6265,  60.4131,\n",
       "         98.7961, 103.5676, 102.0731,  99.2798, 102.8881, 103.8387,\n",
       "        103.8926, 101.1039, 101.751 , 101.7079, 101.8327, 102.6477,\n",
       "         99.5232,  72.3302],\n",
       "       [ 85.6449,  83.2969,  82.4649,  66.5589,  41.2409,  37.9746,\n",
       "         39.9143,  57.0652,  82.4495,  90.5366,  89.7925,  84.7545,\n",
       "         84.5696,  87.8466,  78.5871,  56.6046,  55.1856,  90.27  ,\n",
       "        104.7416, 101.7249, 100.5724, 100.5077, 102.8665, 102.8172,\n",
       "        101.8928, 101.7726, 102.5337, 101.4799, 100.007 , 102.9959,\n",
       "         91.7582,  51.8762],\n",
       "       [ 84.9655,  85.1226,  79.4975,  55.1794,  37.0025,  35.4587,\n",
       "         42.5658,  72.5521,  85.0517,  87.0038,  85.836 ,  81.385 ,\n",
       "         80.7271,  87.5307,  95.6331,  95.9813,  94.0308, 101.5462,\n",
       "        102.127 ,  99.3383,  99.0286, 101.9636,  99.6649,  99.7296,\n",
       "         98.8052, 100.3428, 101.9898, 101.3489, 101.6478, 101.2179,\n",
       "         74.3238,  43.501 ],\n",
       "       [ 85.6773,  87.4383,  77.0479,  47.2465,  35.8347,  34.5297,\n",
       "         47.8103,  76.5625,  85.2412,  86.7742,  86.2966,  85.3568,\n",
       "         83.699 ,  87.4428,  97.659 , 102.9466, 103.4797, 101.7033,\n",
       "         99.0564,  97.6374,  99.1596, 100.6217, 100.2457,  98.0117,\n",
       "         99.1364,  99.8482, 100.9899, 101.0069, 101.1918,  90.6319,\n",
       "         56.3795,  38.9036],\n",
       "       [ 84.8515,  87.2165,  71.5986,  42.492 ,  36.4756,  33.3988,\n",
       "         54.6956,  82.4541,  83.4047,  79.0892,  76.2959,  74.4055,\n",
       "         70.7048,  75.3022,  87.7031,  97.5711, 100.6848, 100.1641,\n",
       "         96.7685,  96.3664,  99.0025,  98.9208,  97.7298,  99.7296,\n",
       "        100.1425,  99.4523,  99.3661, 100.6819, 100.9099,  75.4439,\n",
       "         42.7183,  34.9102],\n",
       "       [ 83.6236,  86.6896,  68.4679,  41.8279,  34.0998,  34.3278,\n",
       "         58.6305,  77.0802,  62.8366,  51.8099,  47.4297,  44.6041,\n",
       "         41.7893,  42.8   ,  48.2833,  61.1618,  76.4853,  91.8859,\n",
       "         98.1042,  98.5942,  99.4693, 100.3875,  97.4417,  98.2028,\n",
       "         99.0995,  98.9963,  99.7251, 103.0946,  93.2357,  56.0159,\n",
       "         38.1209,  35.4371],\n",
       "       [ 86.819 ,  85.5587,  66.864 ,  39.9359,  35.5126,  37.1596,\n",
       "         47.3928,  43.4471,  33.0891,  33.9642,  34.2092,  34.2092,\n",
       "         34.2693,  35.0843,  35.0951,  35.3339,  41.5982,  63.3941,\n",
       "         92.8211, 100.8543, 101.1594, 101.6262,  98.2675,  96.6097,\n",
       "         97.5773,  98.7621,  99.0071,  99.9639,  82.4586,  41.4241,\n",
       "         36.8868,  37.6156],\n",
       "       [ 87.2058,  83.8085,  67.8038,  40.1701,  36.8006,  35.1598,\n",
       "         37.1057,  35.2738,  33.388 ,  32.3342,  34.7531,  36.699 ,\n",
       "         37.9439,  35.6452,  37.6989,  37.9808,  36.078 ,  39.0607,\n",
       "         69.1008,  93.1154, 101.0176, 100.3875,  97.9794,  97.1582,\n",
       "         97.4571,  97.4741,  98.6481,  95.3665,  65.6945,  39.3103,\n",
       "         35.1258,  46.7934],\n",
       "       [ 87.2767,  82.9442,  69.4015,  43.9309,  36.9038,  34.9749,\n",
       "         38.0455,  37.3984,  36.7082,  43.7846,  57.0345,  63.9368,\n",
       "         65.5345,  60.8878,  54.3167,  43.404 ,  36.2198,  34.562 ,\n",
       "         42.3701,  70.4104,  95.5281, 101.5445,  98.0934,  96.6421,\n",
       "         96.126 ,  96.0613,  97.2353,  93.9537,  68.5263,  40.1639,\n",
       "         35.4032,  59.8954],\n",
       "       [ 90.3626,  83.1722,  70.4723,  45.4577,  36.7898,  33.616 ,\n",
       "         37.2566,  39.3103,  51.2445,  73.3472,  85.1843,  83.3802,\n",
       "         83.2708,  82.6838,  85.8899,  76.3684,  49.5713,  34.1968,\n",
       "         32.3649,  47.7655,  81.7683,  98.9577,  98.6696,  96.8655,\n",
       "         96.3063,  95.1924,  95.5514,  95.3234,  90.9648,  76.145 ,\n",
       "         63.5099,  72.2317],\n",
       "       [ 88.0299,  85.5032,  73.8849,  46.5824,  36.0888,  34.1429,\n",
       "         38.0824,  59.4501,  75.8569,  83.0859,  85.6789,  82.901 ,\n",
       "         81.9488,  80.1339,  86.6017,  95.3943,  80.351 ,  41.9788,\n",
       "         32.5498,  37.7126,  68.0039,  97.605 ,  97.5018,  96.8717,\n",
       "         96.9596,  94.6286,  94.9275,  95.9274,  96.7963,  92.5625,\n",
       "         83.699 ,  79.6501],\n",
       "       [ 87.5399,  85.9484,  76.3407,  49.3479,  37.9145,  35.0997,\n",
       "         44.8707,  74.8955,  83.3462,  82.673 ,  84.5049,  82.0151,\n",
       "         78.1772,  77.3622,  82.3571,  91.2098,  92.829 ,  60.2282,\n",
       "         35.0997,  33.2031,  57.706 ,  95.1152,  96.5666,  95.3557,\n",
       "         96.1446,  94.9167,  94.7426,  94.2696,  93.4376,  91.2036,\n",
       "         84.041 ,  80.9489],\n",
       "       [ 82.3   ,  83.5834,  77.9537,  52.3799,  36.0179,  33.4481,\n",
       "         49.9904,  77.1403,  82.2215,  84.5911,  85.5048,  82.1138,\n",
       "         77.39  ,  76.3731,  79.4113,  88.509 ,  95.0846,  69.1303,\n",
       "         36.0349,  35.7298,  53.8142,  89.8106,  96.5666,  96.0845,\n",
       "         96.2694,  95.6115,  92.8937,  93.8874,  92.4684,  89.7676,\n",
       "         85.9098,  81.1877],\n",
       "       [ 81.6699,  81.8332,  81.6051,  59.8414,  35.9532,  34.2092,\n",
       "         49.1646,  76.3792,  82.4002,  85.2428,  85.7436,  82.1847,\n",
       "         80.4237,  80.6948,  81.7162,  85.3352,  93.9645,  69.4723,\n",
       "         36.0457,  32.214 ,  50.5434,  91.0124,  98.6974,  97.0674,\n",
       "         95.9535,  95.0784,  93.3775,  92.8875,  92.6255,  90.4578,\n",
       "         85.3721,  78.5192],\n",
       "       [ 81.2155,  82.1984,  80.6222,  64.8687,  38.9807,  33.7963,\n",
       "         42.8062,  69.7219,  81.5143,  82.542 ,  82.6838,  80.1571,\n",
       "         80.7611,  80.7673,  79.9676,  79.4005,  86.372 ,  63.7009,\n",
       "         37.2906,  35.0027,  52.6912,  91.8013,  98.8653,  96.3324,\n",
       "         91.9199,  82.5788,  87.6554,  93.9968,  93.1047,  89.649 ,\n",
       "         85.1611,  78.4653],\n",
       "       [ 80.1617,  81.7917,  80.1123,  71.0207,  44.5287,  33.9211,\n",
       "         35.263 ,  57.2824,  78.5747,  81.5699,  82.1138,  81.0322,\n",
       "         82.1291,  80.6624,  80.0215,  79.7873,  78.2327,  55.2457,\n",
       "         34.8178,  37.2906,  62.5223,  96.0459,  97.6913,  93.0446,\n",
       "         69.4491,  48.7285,  69.6725,  94.0507,  94.1863,  90.7907,\n",
       "         86.1179,  77.4115],\n",
       "       [ 78.9599,  80.3897,  82.3679,  73.6291,  47.3651,  31.872 ,\n",
       "         33.6761,  52.5648,  76.8075,  81.2325,  78.581 ,  79.2542,\n",
       "         79.2803,  78.6933,  81.0691,  82.6191,  72.0115,  43.3008,\n",
       "         31.04  ,  39.3982,  72.227 ,  96.9364,  96.5343,  85.8944,\n",
       "         50.0613,  34.0351,  50.0812,  82.9254,  95.9519,  90.8446,\n",
       "         87.1501,  82.9872],\n",
       "       [ 82.7037,  82.7377,  83.9117,  82.2862,  61.9507,  33.16  ,\n",
       "         34.334 ,  45.0987,  68.9115,  80.6069,  79.2434,  79.6994,\n",
       "         78.3944,  77.7042,  81.8949,  81.4604,  59.31  ,  36.4694,\n",
       "         32.9258,  48.2833,  81.6498,  95.8764,  95.2894,  86.2534,\n",
       "         53.534 ,  34.8609,  35.2506,  57.0743,  83.8606,  89.5458,\n",
       "         86.2642,  82.4603],\n",
       "       [ 91.7413,  93.5964,  93.5964,  93.6225,  75.2976,  40.2302,\n",
       "         33.5899,  34.4758,  54.2396,  79.9058,  87.9929,  90.9818,\n",
       "         90.8508,  89.8617,  86.2103,  67.8315,  40.3442,  29.3391,\n",
       "         34.8116,  66.1567,  92.8659,  95.4573,  95.3433,  93.6486,\n",
       "         69.1241,  37.8113,  30.8551,  35.1428,  49.6144,  69.879 ,\n",
       "         81.0305,  82.7592],\n",
       "       [ 95.1493,  96.2155,  97.5574,  94.3666,  82.0859,  53.323 ,\n",
       "         33.1832,  33.0953,  39.9852,  65.2493,  87.0469,  94.2095,\n",
       "         92.8829,  86.5029,  67.1412,  42.0297,  32.0461,  34.3216,\n",
       "         50.2661,  83.9207,  96.5173,  95.3864,  93.6855,  93.4036,\n",
       "         87.4212,  54.8266,  34.6437,  32.5838,  32.7517,  38.1471,\n",
       "         49.6791,  58.8092],\n",
       "       [ 95.1862,  95.6484,  93.1047,  93.7025,  91.1066,  73.5876,\n",
       "         47.6055,  35.5188,  33.9858,  41.056 ,  63.6578,  70.5754,\n",
       "         61.7765,  47.4081,  41.572 ,  32.6546,  30.5562,  44.4777,\n",
       "         72.7738,  94.3865,  95.5174,  94.3326,  92.3867,  90.8599,\n",
       "         93.5947,  80.1831,  51.414 ,  34.6976,  30.8058,  31.1478,\n",
       "         33.012 ,  34.7021],\n",
       "       [ 93.9952,  92.9845,  93.8596,  93.6316,  91.6057,  89.2208,\n",
       "         71.0916,  43.5288,  35.0566,  36.4047,  39.1548,  40.3118,\n",
       "         35.0673,  32.5406,  36.8283,  34.9964,  41.8863,  68.9483,\n",
       "         88.6151,  94.7455,  94.1046,  91.5779,  91.034 ,  90.8491,\n",
       "         91.0402,  94.0138,  84.0857,  61.8968,  41.9788,  34.7946,\n",
       "         34.1429,  37.2458],\n",
       "       [ 90.2236,  91.0448,  93.4637,  94.1647,  91.2898,  90.3177,\n",
       "         86.9868,  73.0205,  51.4248,  37.0025,  36.095 ,  33.1384,\n",
       "         31.7795,  33.0244,  38.5678,  53.9191,  73.2099,  90.447 ,\n",
       "         92.1757,  92.3498,  92.1218,  91.638 ,  91.3822,  91.0232,\n",
       "         92.795 ,  95.4266,  96.5144,  89.569 ,  73.0097,  55.3535,\n",
       "         48.5822,  44.0557],\n",
       "       [ 90.6427,  90.4039,  90.937 ,  92.4099,  90.008 ,  88.4319,\n",
       "         87.5137,  81.8671,  73.9819,  65.0752,  59.6242,  57.6136,\n",
       "         59.6735,  65.6899,  78.4175,  89.0682,  94.3496,  91.2898,\n",
       "         91.3777,  92.0787,  91.0079,  92.584 ,  91.3884,  89.5134,\n",
       "         90.4532,  92.7088,  93.8998,  91.9108,  82.3185,  78.2696,\n",
       "         77.4977,  74.27  ],\n",
       "       [ 88.0236,  88.1977,  86.6709,  88.4319,  88.0082,  84.4215,\n",
       "         82.0304,  81.1661,  78.3513,  76.2806,  77.9645,  83.2844,\n",
       "         89.8662,  91.7582,  93.0139,  94.3666,  94.7625,  93.0616,\n",
       "         90.6767,  91.8615,  91.7798,  93.057 ,  91.2313,  90.5303,\n",
       "         91.3992,  92.6379,  92.5409,  89.9649,  80.9103,  75.2206,\n",
       "         76.6396,  79.2865]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_Train[1]\n",
    "X_Train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cfa497dfd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets view another image \n",
    "#Lets choose some random data and view it \n",
    "print(\"Label: {}\".format(Y_Train[2]))\n",
    "plt.imshow(X_Train[2], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cfa49d49a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets view another image \n",
    "#Lets choose some random data and view it \n",
    "print(\"Label: {}\".format(Y_Train[3]))\n",
    "plt.imshow(X_Train[3], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank Sapce\n",
    "\n",
    "# Blank Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Preparing Data for Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Check Target Variable\n",
    "-Is Data Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TrainDF=pd.DataFrame(Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_TrainDF.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   0       42000 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 328.2 KB\n"
     ]
    }
   ],
   "source": [
    "Y_TrainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    4281\n",
       "5    4232\n",
       "2    4197\n",
       "9    4196\n",
       "7    4192\n",
       "8    4188\n",
       "4    4188\n",
       "0    4186\n",
       "1    4172\n",
       "6    4168\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_TrainDF.iloc[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='0', ylabel='count'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(Y_TrainDF.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Convert to arrays and output to one hot vectors\n",
    "- Convert data to numpy arrays\n",
    "- Flatten all input arrays\n",
    "- Convert output labels to One hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33.0704,  30.2601,  26.852 ,  25.8844,  28.5421,  33.1673,\n",
       "         33.9069,  31.277 ,  27.3313,  23.9556,  21.3518,  22.5258,\n",
       "         24.2868,  31.2322,  37.2917,  38.1776,  30.8902,  24.4887,\n",
       "         27.983 ,  23.3641,  19.7774,  23.0159,  24.7168,  24.9941,\n",
       "         27.5701,  38.5474,  52.8341,  65.9037,  73.5609,  71.4471,\n",
       "         58.2204,  42.9939],\n",
       "       [ 25.2283,  25.5533,  29.9765,  40.96  ,  54.0358,  60.7317,\n",
       "         57.5965,  47.9611,  33.0103,  24.3362,  24.3793,  33.1997,\n",
       "         44.1339,  61.1815,  80.5278,  86.1744,  79.7621,  66.5893,\n",
       "         47.4926,  29.7548,  21.8804,  22.9943,  25.2822,  26.6303,\n",
       "         39.7923,  64.9531,  91.0815, 108.8795, 113.6079, 113.0209,\n",
       "        103.3639,  84.2949],\n",
       "       [ 26.2775,  22.6137,  40.4763,  69.6815,  91.8381, 101.2887,\n",
       "         97.0719,  72.8832,  41.1944,  23.0975,  32.0319,  55.8077,\n",
       "         82.2242, 104.4069, 113.9329, 116.5197, 117.7646, 111.6512,\n",
       "         83.8758,  53.9974,  32.4664,  22.3256,  23.4179,  32.2491,\n",
       "         61.2093,  99.8526, 117.9217, 117.4272, 110.1398, 113.3028,\n",
       "        121.775 , 115.4228],\n",
       "       [ 32.3478,  26.8044,  51.4383,  87.5396, 106.8321, 113.38  ,\n",
       "        104.0219,  70.8233,  38.3626,  24.8415,  46.8948,  82.9853,\n",
       "        107.8966, 112.3738,  97.9068,  93.7331, 108.9227, 119.4009,\n",
       "        116.9173,  89.2128,  50.6665,  25.2544,  25.3899,  53.6383,\n",
       "         92.8703, 121.9661, 121.1557,  95.4017,  66.5016,  75.0554,\n",
       "        106.8196, 122.5531],\n",
       "       [ 34.7605,  38.4181,  66.9268,  96.3107, 111.1414, 108.8751,\n",
       "         86.0776,  48.1506,  24.1729,  29.6669,  65.48  , 103.3962,\n",
       "        112.3091,  92.0168,  52.0192,  42.8891,  79.8916, 114.2103,\n",
       "        126.8131, 111.3308,  70.7739,  32.4386,  32.9978,  74.419 ,\n",
       "        111.9177, 123.7271, 114.8251,  77.9412,  39.1283,  50.8559,\n",
       "         91.0384, 121.5963],\n",
       "       [ 38.0483,  57.2591,  89.447 , 108.3311, 114.7712, 108.4514,\n",
       "         78.4312,  33.6127,  19.0424,  40.8399,  85.4349, 120.7535,\n",
       "        121.5362,  93.0706,  45.5899,  23.1021,  60.0337, 107.2263,\n",
       "        126.9163, 124.1616,  90.604 ,  45.9812,  43.5946,  89.1895,\n",
       "        123.2155, 125.8409, 111.4664,  69.8819,  25.6997,  38.3563,\n",
       "         82.5985, 118.5688],\n",
       "       [ 40.9232,  74.2143, 106.4346, 115.5369, 114.1626, 110.1415,\n",
       "         81.6913,  31.9272,  22.4981,  53.6599,  96.8098, 122.9552,\n",
       "        123.6778, 103.5534,  67.5877,  36.6169,  54.4795,  97.5585,\n",
       "        123.194 , 130.4168, 105.3961,  61.7516,  57.5285,  98.1886,\n",
       "        126.3616, 126.5034, 113.2813,  72.501 ,  25.7859,  34.3289,\n",
       "         85.3424, 116.4272],\n",
       "       [ 46.0475,  78.7408, 104.5596, 111.3632, 113.9777, 113.3584,\n",
       "         82.9901,  34.1658,  27.4976,  60.3773, 105.0818, 121.5271,\n",
       "        121.9508, 111.7545,  80.7174,  41.7474,  48.7852,  93.3802,\n",
       "        120.9554, 129.705 , 113.6942,  71.1097,  63.702 , 100.7754,\n",
       "        128.0733, 123.9166, 112.2105,  77.7285,  28.6007,  35.7848,\n",
       "         85.9833, 115.0683],\n",
       "       [ 44.4822,  66.9099,  87.9248, 103.0974, 115.2272, 115.597 ,\n",
       "         81.0719,  31.9657,  21.7432,  55.8724, 104.2776, 126.7223,\n",
       "        127.9179, 115.6078,  82.044 ,  37.4767,  44.9166,  93.7392,\n",
       "        124.368 , 130.2211, 110.7869,  68.5229,  60.1369,  97.4984,\n",
       "        127.1766, 123.3188, 108.6238,  77.6576,  30.4757,  33.329 ,\n",
       "         82.6524, 117.0358],\n",
       "       [ 35.2166,  45.422 ,  68.6323,  97.0163, 118.5366, 116.1455,\n",
       "         80.9363,  33.3092,  16.2877,  38.4766,  80.4417, 111.3586,\n",
       "        118.1515, 106.2004,  69.6261,  31.7053,  45.9165,  96.0379,\n",
       "        126.0088, 130.4922, 107.3034,  62.3494,  48.1597,  87.0973,\n",
       "        125.4649, 128.5463, 110.2646,  74.5331,  31.8947,  34.7588,\n",
       "         82.1471, 119.1882],\n",
       "       [ 26.1574,  29.3357,  58.9583,  96.8314, 118.5366, 115.0747,\n",
       "         83.9961,  40.0867,  17.4078,  19.4615,  47.1337,  80.3924,\n",
       "         86.8325,  72.9417,  43.0247,  22.9881,  48.3723,  93.7823,\n",
       "        120.4546, 122.134 ,  95.2553,  53.3503,  32.5634,  64.6265,\n",
       "        110.129 , 127.7852, 118.367 ,  85.2824,  43.8997,  47.6605,\n",
       "         84.4027, 117.113 ],\n",
       "       [ 24.2007,  26.8028,  57.9091,  97.9453, 116.5152, 114.2982,\n",
       "         86.529 ,  43.4454,  19.8267,  18.3368,  25.9339,  33.075 ,\n",
       "         34.021 ,  31.4234,  20.1625,  19.0316,  53.0837,  95.4401,\n",
       "        120.1126, 117.0313,  82.9361,  41.7321,  23.4611,  46.8609,\n",
       "         88.1188, 119.0079, 124.0567, 100.5752,  72.5071,  77.5066,\n",
       "        103.6027, 122.0586],\n",
       "       [ 26.1897,  28.5746,  58.8058,  99.885 , 116.3904, 113.5217,\n",
       "         86.2794,  43.4454,  22.0114,  20.1687,  25.8199,  20.6463,\n",
       "         22.5105,  22.9619,  18.7173,  28.08  ,  63.9856, 102.1252,\n",
       "        121.6672, 110.0428,  68.8235,  30.516 ,  21.8958,  35.2796,\n",
       "         58.2126,  94.6343, 118.8184, 121.4823, 116.1839, 113.765 ,\n",
       "        123.9058, 121.8629],\n",
       "       [ 24.1791,  25.635 ,  58.7411, 100.7709, 117.6183, 113.6357,\n",
       "         86.4042,  45.684 ,  22.8372,  21.4675,  23.5813,  24.0543,\n",
       "         26.7982,  24.9016,  22.4288,  36.5584,  77.5344, 110.3802,\n",
       "        118.2592,  88.3808,  48.0366,  26.0757,  24.9278,  26.9707,\n",
       "         44.9042,  80.9238, 111.4879, 129.8513, 118.6074, 108.5914,\n",
       "        120.8352, 120.0911],\n",
       "       [ 23.3533,  25.5703,  57.6873,  99.9451, 118.3193, 114.1626,\n",
       "         89.9308,  46.6238,  21.1902,  23.5921,  21.5877,  21.5769,\n",
       "         26.5594,  24.5534,  24.7275,  50.3953,  89.6704, 112.1628,\n",
       "         98.4507,  56.4317,  29.1786,  23.1514,  25.3083,  26.3082,\n",
       "         48.91  ,  87.2283, 111.0149, 110.3139,  89.282 ,  80.44  ,\n",
       "        104.5856, 121.319 ],\n",
       "       [ 25.0003,  26.2174,  55.0466,  97.9453, 118.3193, 115.5754,\n",
       "         92.8704,  46.1508,  19.1904,  24.3039,  20.7619,  23.9141,\n",
       "         27.0216,  22.3795,  36.3843,  73.1712, 100.3874, 102.5875,\n",
       "         70.0452,  31.0151,  19.7496,  25.6565,  27.5315,  41.0741,\n",
       "         77.6314, 112.4878, 116.1562,  91.7934,  53.6322,  39.7198,\n",
       "         73.6256, 115.9605],\n",
       "       [ 27.5162,  27.3205,  54.0359,  97.6464, 119.3192, 117.6892,\n",
       "         94.4573,  49.1397,  19.4184,  22.3041,  22.3596,  24.8   ,\n",
       "         24.0219,  26.6241,  55.3994,  93.1261, 102.1099,  79.6545,\n",
       "         42.2868,  22.13  ,  20.5646,  24.1297,  29.9504,  60.2094,\n",
       "        102.364 , 127.4494, 119.8308,  93.12  ,  48.0134,  22.7323,\n",
       "         59.5669, 107.4883],\n",
       "       [ 27.755 ,  27.6194,  53.737 ,  96.7605, 117.6722, 119.216 ,\n",
       "         95.9841,  51.3136,  19.7666,  21.8804,  24.4133,  23.5659,\n",
       "         23.3209,  40.9278,  79.3   , 103.6736,  89.7567,  51.2921,\n",
       "         29.2172,  23.8309,  22.5644,  22.9126,  35.5046,  76.2356,\n",
       "        117.9773, 130.064 , 118.6137,  93.8318,  47.5404,  25.128 ,\n",
       "         62.4356, 103.0157],\n",
       "       [ 26.2929,  26.2174,  53.1608,  96.5864, 118.6721, 120.2159,\n",
       "         97.2721,  53.3735,  21.0007,  22.7555,  25.3962,  22.5105,\n",
       "         32.7437,  65.1766,  95.0874,  95.337 ,  62.046 ,  30.744 ,\n",
       "         23.663 ,  26.5209,  25.4932,  22.6784,  42.2266,  88.142 ,\n",
       "        122.2866, 125.3033, 115.7819,  91.1849,  43.4206,  25.5948,\n",
       "         63.6034, 102.1298],\n",
       "       [ 25.7059,  25.7444,  52.5738,  95.9994, 118.0851, 119.33  ,\n",
       "         97.4462,  53.9605,  22.9897,  23.8694,  23.6352,  22.6892,\n",
       "         43.2265,  85.6538, 101.3318,  76.5176,  32.5096,  20.5431,\n",
       "         23.2223,  25.7382,  24.7213,  22.4334,  44.7641,  94.3802,\n",
       "        123.0415, 123.9875, 115.1841,  91.017 ,  44.9105,  24.427 ,\n",
       "         60.1908,  99.1301],\n",
       "       [ 24.5319,  28.7549,  54.1006,  93.6406, 114.3135, 116.2702,\n",
       "         96.0981,  52.6124,  20.3536,  22.2116,  18.5001,  27.3421,\n",
       "         65.1874,  99.4352,  91.9737,  50.8021,  19.195 ,  22.5151,\n",
       "         24.6566,  27.7056,  36.4274,  30.2647,  44.0523,  89.5378,\n",
       "        123.1277, 127.3184, 117.4011,  94.8918,  49.0411,  28.8134,\n",
       "         60.5067,  95.7175],\n",
       "       [ 23.2439,  28.8088,  53.2147,  93.2277, 115.6015, 119.33  ,\n",
       "         99.8096,  57.6766,  19.8313,  24.6073,  23.5319,  44.4929,\n",
       "         86.9895,  99.0592,  71.7676,  34.0471,  19.3243,  24.2421,\n",
       "         38.5026,  60.5236,  78.7992,  62.3494,  49.9916,  82.5215,\n",
       "        119.1281, 125.2046, 117.2162,  90.6364,  47.8024,  30.3033,\n",
       "         59.5238,  95.5326],\n",
       "       [ 23.8309,  24.5103,  46.2154,  89.5871, 116.9604, 120.39  ,\n",
       "         99.2719,  57.2637,  20.0593,  21.8356,  28.5745,  63.605 ,\n",
       "         98.3196,  91.8041,  56.5564,  27.4221,  24.1712,  29.0998,\n",
       "         45.8008,  79.5064,  96.4293,  74.718 ,  43.399 ,  71.4625,\n",
       "        112.5695, 123.0585, 115.1132,  93.3372,  52.3243,  30.9997,\n",
       "         60.573 ,  95.6358],\n",
       "       [ 24.8308,  24.3254,  44.3296,  87.9463, 117.3795, 120.6242,\n",
       "         97.2182,  55.8509,  20.5754,  20.4829,  37.4766,  79.3215,\n",
       "        107.2217,  96.9947,  70.0944,  58.1325,  64.5386,  67.8803,\n",
       "         71.1312, 100.4782, 112.1135,  75.9737,  33.041 ,  57.9908,\n",
       "        104.6412, 124.9012, 121.6395,  98.3798,  54.4704,  26.3807,\n",
       "         56.6119,  98.0978],\n",
       "       [ 25.0049,  27.9121,  48.2753,  89.7073, 117.6245, 120.766 ,\n",
       "         99.8759,  61.6115,  23.7492,  21.8526,  48.1443,  90.2773,\n",
       "        113.765 , 113.7111, 107.0538, 105.1079, 109.8624, 108.3895,\n",
       "        109.5204, 122.6178, 121.4115,  76.4467,  28.7595,  39.6937,\n",
       "         82.6156, 112.1736, 120.5256, 102.5643,  63.3447,  35.4291,\n",
       "         62.5466, 103.1574],\n",
       "       [ 22.1362,  23.3425,  47.8193,  88.0773, 115.2935, 118.2501,\n",
       "        101.4736,  63.6714,  26.1681,  21.0376,  57.98  ,  97.8143,\n",
       "        115.0037, 118.3085, 121.7812, 122.0801, 122.379 , 120.3792,\n",
       "        119.0804, 122.8027, 116.5799,  79.9194,  31.2493,  25.1682,\n",
       "         54.3564,  91.5007, 114.9391, 112.5264,  91.8489,  79.4156,\n",
       "         92.4035, 113.423 ],\n",
       "       [ 23.9681,  29.8319,  56.2484,  95.3324, 115.3043, 116.9621,\n",
       "        108.8319,  82.3876,  43.9553,  31.7115,  69.2517, 108.2109,\n",
       "        119.4009, 120.8199, 121.9939, 120.7059, 118.4781, 116.4783,\n",
       "        119.233 , 117.113 , 112.5911,  83.2889,  39.7924,  18.8807,\n",
       "         31.6406,  60.8396,  91.2234, 111.4942, 112.7175, 109.6253,\n",
       "        112.38  , 105.6842],\n",
       "       [ 31.2124,  49.1459,  75.9152, 108.3419, 117.244 , 116.4891,\n",
       "        115.1302,  97.343 ,  63.8563,  44.396 ,  74.0618, 107.2496,\n",
       "        115.9668, 112.8423, 110.7716, 111.1845, 107.7827, 108.7225,\n",
       "        111.1783, 108.1894, 106.1573,  79.0829,  43.0417,  19.4893,\n",
       "         20.6633,  30.9612,  46.6284,  77.7132,  92.5931,  93.7455,\n",
       "         89.2837,  71.817 ],\n",
       "       [ 31.4682,  51.5263,  76.1109,  98.6957,  95.468 ,  91.7134,\n",
       "         95.8979,  83.2412,  57.5149,  37.1086,  53.9436,  78.5991,\n",
       "         86.8756,  80.9193,  79.3755,  77.0167,  70.0282,  70.153 ,\n",
       "         69.224 ,  66.91  ,  66.7699,  46.3959,  25.8092,  19.1411,\n",
       "         22.8588,  23.4396,  18.7759,  25.5364,  39.4164,  41.2976,\n",
       "         39.3795,  33.9995],\n",
       "       [ 28.5502,  36.212 ,  45.0801,  43.8675,  38.4874,  38.9604,\n",
       "         38.8033,  34.853 ,  30.8318,  27.3206,  28.9768,  30.0799,\n",
       "         30.1723,  28.9058,  28.2541,  26.0263,  23.7492,  21.5969,\n",
       "         20.3798,  23.2224,  26.4132,  25.6197,  23.4781,  23.4072,\n",
       "         24.7168,  23.0097,  18.5802,  20.1455,  23.4287,  24.1359,\n",
       "         25.0927,  26.0603],\n",
       "       [ 38.4352,  26.4733,  23.2717,  23.9449,  21.6292,  19.3413,\n",
       "         20.7002,  22.2224,  26.7443,  21.6955,  17.5264,  15.2708,\n",
       "         14.5482,  16.0426,  18.0486,  22.777 ,  26.7381,  25.7598,\n",
       "         21.5322,  22.5213,  23.4565,  23.0929,  22.2671,  23.495 ,\n",
       "         25.1466,  25.5102,  24.2222,  23.1684,  24.9895,  28.1094,\n",
       "         29.4683,  30.0661],\n",
       "       [ 50.2984,  26.0773,  24.0389,  30.0922,  28.5808,  27.5316,\n",
       "         25.2221,  25.3854,  32.4448,  29.4451,  27.6132,  27.7272,\n",
       "         28.0754,  28.6516,  29.1416,  30.2833,  33.5865,  33.3092,\n",
       "         32.3756,  35.1257,  38.1146,  38.7016,  37.5877,  38.4736,\n",
       "         41.1852,  45.0816,  46.0923,  45.2172,  47.2062,  49.6682,\n",
       "         50.853 ,  53.0377]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 40.558 ,  46.7917,  48.9764,  55.0205,  67.3568,  71.6276,\n",
       "         64.9102,  61.9536,  70.8064,  74.8831,  67.8083,  61.037 ,\n",
       "         49.7114,  49.2599,  52.1887,  59.1387,  58.0787,  55.3671,\n",
       "         57.769 ,  60.6547,  67.7419,  80.6652,  92.7349,  89.2083,\n",
       "         90.8661,  99.3491, 106.9031, 109.1587, 112.2293, 112.1153,\n",
       "        112.9904, 112.1646],\n",
       "       [ 39.4379,  44.2911,  47.1768,  53.6015,  64.1121,  68.2581,\n",
       "         62.7147,  63.5297,  75.7843,  81.4479,  71.5475,  59.7166,\n",
       "         47.2062,  50.6511,  60.1987,  68.1163,  63.4265,  56.2423,\n",
       "         57.6766,  62.2524,  67.6926,  81.0997,  91.3545,  88.931 ,\n",
       "         91.1049,  99.2998, 106.0927, 108.5224, 110.6532, 111.0122,\n",
       "        110.9475, 109.9368],\n",
       "       [ 38.4488,  43.6394,  48.7098,  54.8572,  62.4112,  66.0734,\n",
       "         60.3451,  62.046 ,  74.8984,  81.2029,  72.6444,  60.6286,\n",
       "         47.5312,  52.0577,  63.448 ,  73.1867,  64.8563,  55.9712,\n",
       "         57.1174,  62.747 ,  66.1165,  79.5667,  86.0669,  88.5289,\n",
       "         93.3004,  99.9084, 105.6906, 109.0601, 110.419 , 109.8921,\n",
       "        109.9414, 109.1048],\n",
       "       [ 37.8788,  40.9448,  49.7868,  56.5428,  63.5529,  67.97  ,\n",
       "         62.6546,  63.7685,  74.675 ,  81.6204,  71.9418,  57.7521,\n",
       "         46.1815,  54.8709,  68.7895,  76.7518,  66.058 ,  55.0591,\n",
       "         57.0527,  62.6438,  64.8285,  79.6591,  82.5618,  85.4753,\n",
       "         93.1433,  98.1644, 103.1747, 107.1851, 108.185 , 108.4731,\n",
       "        109.3482, 110.0985],\n",
       "       [ 34.9222,  38.7   ,  45.3635,  55.1793,  62.0539,  65.4711,\n",
       "         60.4977,  57.954 ,  68.8605,  74.2128,  64.0011,  50.2135,\n",
       "         47.7191,  60.5391,  73.6319,  80.3339,  64.4171,  51.1303,\n",
       "         56.3301,  64.8285,  68.5508,  81.1859,  83.7789,  84.5894,\n",
       "         90.0296,  95.5668,  99.5171, 103.1685, 104.4072, 105.2823,\n",
       "        106.0326, 108.5655],\n",
       "       [ 31.9656,  36.0361,  37.1624,  52.6571,  67.9824,  68.5309,\n",
       "         57.0204,  45.7657,  57.3023,  67.007 ,  62.3756,  49.4092,\n",
       "         50.1765,  61.7471,  79.2031,  84.3613,  64.226 ,  45.7548,\n",
       "         53.9174,  67.3722,  71.8494,  79.6699,  86.0884,  88.6429,\n",
       "         89.0127,  96.3217,  99.6742, 100.5108, 101.3905, 101.7495,\n",
       "        101.1579, 107.8537],\n",
       "       [ 35.6787,  41.4717,  46.8733,  58.2204,  67.8021,  67.5849,\n",
       "         55.7216,  45.929 ,  53.8573,  64.1921,  57.2728,  44.9042,\n",
       "         47.2692,  60.3774,  78.3927,  81.1105,  61.5838,  44.1941,\n",
       "         52.1456,  63.2046,  68.4429,  77.5407,  85.2796,  86.9528,\n",
       "         87.7355,  96.7516, 101.9468, 104.5552, 103.962 , 103.1901,\n",
       "        102.0546, 106.5765],\n",
       "       [ 34.7451,  38.9512,  47.4711,  56.823 ,  64.7469,  66.7252,\n",
       "         56.3348,  50.3631,  56.042 ,  62.3664,  52.2841,  40.8661,\n",
       "         46.5836,  61.2571,  76.664 ,  75.8275,  55.5613,  38.9327,\n",
       "         50.1288,  60.6886,  65.3461,  77.6609,  86.617 ,  83.8176,\n",
       "         84.0025,  94.3775, 102.8066, 106.5289, 108.2344, 106.1036,\n",
       "        101.9514, 103.6415],\n",
       "       [ 35.842 ,  36.2226,  43.8305,  55.0251,  64.5728,  67.7189,\n",
       "         56.2855,  51.4878,  55.7   ,  60.0354,  45.0183,  34.2628,\n",
       "         44.2079,  60.4359,  71.5612,  71.5074,  52.6047,  40.5134,\n",
       "         51.0085,  59.0416,  65.1828,  80.1769,  88.7309,  87.9698,\n",
       "         87.3181,  95.3235, 101.5787, 104.8172, 108.5225, 105.9187,\n",
       "        101.3644, 102.7556],\n",
       "       [ 34.3028,  34.2597,  41.5919,  52.9776,  63.6006,  63.616 ,\n",
       "         54.0792,  50.2383,  54.4397,  55.0143,  36.9051,  25.1004,\n",
       "         39.4688,  53.4366,  67.1872,  69.8234,  53.9204,  42.4916,\n",
       "         52.8342,  59.9429,  65.3939,  80.3772,  90.6814,  91.2145,\n",
       "         88.6662,  94.1279,  97.8825,  99.8761, 102.5815, 102.9944,\n",
       "        100.1688, 102.1039],\n",
       "       [ 35.5523,  37.2594,  44.1679,  54.2117,  67.101 ,  64.8285,\n",
       "         55.1284,  51.1026,  56.8909,  58.5794,  39.9048,  24.9371,\n",
       "         36.7788,  47.2308,  65.0411,  70.4705,  55.8339,  42.4161,\n",
       "         52.3181,  59.6764,  63.1707,  76.4638,  85.7465,  85.4924,\n",
       "         82.656 ,  87.2749,  89.7738,  90.1096,  92.701 ,  95.4835,\n",
       "         94.4467,  98.9964],\n",
       "       [ 34.7543,  35.4507,  41.8261,  53.6247,  69.0469,  69.03  ,\n",
       "         58.4055,  55.0206,  63.6237,  66.7142,  47.8655,  28.4422,\n",
       "         33.8716,  46.1493,  63.0845,  70.6662,  55.4919,  40.5365,\n",
       "         50.0364,  59.3344,  61.8611,  70.9912,  77.2634,  77.0417,\n",
       "         75.2869,  79.8457,  81.6328,  81.4417,  86.093 ,  89.5334,\n",
       "         89.2084,  94.0139],\n",
       "       [ 32.7499,  33.7452,  39.4843,  53.2334,  73.3732,  72.7693,\n",
       "         60.8676,  56.8356,  68.1718,  72.1374,  52.6909,  29.4421,\n",
       "         34.7467,  48.9641,  61.9536,  69.0192,  53.9759,  39.1946,\n",
       "         50.0426,  61.6393,  64.7961,  70.4427,  73.8831,  71.9543,\n",
       "         71.2102,  79.3727,  84.1765,  84.7034,  89.6536,  93.6209,\n",
       "         92.8229,  96.3126],\n",
       "       [ 31.3155,  32.5496,  39.1962,  55.0268,  72.2162,  70.0146,\n",
       "         56.1562,  51.7544,  64.6775,  71.3439,  57.4793,  34.2197,\n",
       "         33.0519,  48.3339,  63.6868,  69.6493,  54.4597,  41.7383,\n",
       "         50.9285,  63.8024,  67.557 ,  70.6276,  72.894 ,  68.8344,\n",
       "         71.677 ,  83.6712,  91.1928,  94.9644, 101.0994, 105.0066,\n",
       "        102.1918, 103.8388],\n",
       "       [ 32.283 ,  35.8589,  45.516 ,  56.7924,  65.0366,  66.585 ,\n",
       "         50.8731,  40.2269,  58.8182,  70.686 ,  64.6866,  41.4054,\n",
       "         31.5636,  48.867 ,  63.807 ,  71.5459,  55.9434,  43.0263,\n",
       "         49.5202,  60.3342,  63.6544,  71.5181,  74.9585,  72.948 ,\n",
       "         78.4375,  87.6061,  97.3124, 105.1052, 110.3651, 111.0168,\n",
       "        109.4191, 107.9847],\n",
       "       [ 33.8853,  37.5644,  44.2002,  53.1779,  64.5466,  66.4971,\n",
       "         49.6004,  38.5521,  55.8554,  66.7403,  67.7957,  49.172 ,\n",
       "         35.7974,  51.3999,  63.465 ,  72.6814,  56.7369,  42.6781,\n",
       "         48.6173,  58.2573,  64.5341,  75.7673,  80.6806,  79.9473,\n",
       "         86.4259,  94.0677, 102.372 , 109.4037, 111.4898, 110.8427,\n",
       "        109.4622, 107.6149],\n",
       "       [ 35.3567,  37.6122,  41.4547,  48.297 ,  63.3017,  63.9426,\n",
       "         46.8826,  35.4322,  46.3833,  55.1436,  65.1658,  54.2532,\n",
       "         38.3195,  50.8406,  65.2799,  74.2082,  56.8617,  40.2161,\n",
       "         44.5684,  56.3931,  63.5666,  77.6747,  84.9899,  84.9468,\n",
       "         90.7136,  97.1814, 103.8988, 110.1047, 111.3049, 111.1847,\n",
       "        109.4021, 108.1418],\n",
       "       [ 30.5575,  33.7097,  38.0252,  44.547 ,  62.7039,  61.704 ,\n",
       "         49.0673,  36.9267,  38.6399,  44.7918,  60.362 ,  56.4487,\n",
       "         39.059 ,  49.7975,  67.084 ,  75.0232,  56.8016,  38.1562,\n",
       "         42.1064,  52.7032,  60.0508,  74.7567,  84.0609,  87.8325,\n",
       "         94.7132, 100.8929, 106.7244, 111.6315, 112.2339, 111.0106,\n",
       "        110.3419, 111.2062],\n",
       "       [ 36.1224,  40.6443,  45.8457,  52.6664,  62.895 ,  63.1939,\n",
       "         56.5135,  45.4976,  41.2545,  41.9231,  60.1448,  57.8723,\n",
       "         41.504 ,  48.6943,  64.8777,  75.2404,  57.6058,  39.6722,\n",
       "         43.5084,  52.4043,  59.5239,  73.9309,  82.719 ,  88.0174,\n",
       "         95.0229, 102.3165, 108.6749, 112.5821, 110.8858, 110.1355,\n",
       "        112.0644, 113.3524],\n",
       "       [ 37.1285,  40.1236,  42.0156,  48.8363,  62.456 ,  63.3527,\n",
       "         61.074 ,  55.2856,  49.5696,  45.1786,  60.7426,  61.1   ,\n",
       "         38.6506,  44.3141,  63.7422,  73.6211,  59.111 ,  40.4764,\n",
       "         45.7254,  53.0945,  59.926 ,  76.094 ,  82.7082,  87.0175,\n",
       "         92.8598, 101.8651, 109.1094, 111.5329, 110.5977, 109.2496,\n",
       "        111.9612, 113.847 ],\n",
       "       [ 41.0095,  39.1468,  40.3162,  48.3756,  65.1029,  66.733 ,\n",
       "         64.9704,  62.117 ,  57.6397,  52.3844,  62.0414,  57.0187,\n",
       "         34.6941,  39.8415,  58.6179,  70.3826,  59.1002,  42.5794,\n",
       "         49.1272,  56.7243,  61.8441,  77.5992,  86.2132,  85.0177,\n",
       "         91.1589, 100.7019, 108.3483, 111.935 , 109.701 , 107.5271,\n",
       "        109.9075, 114.2769],\n",
       "       [ 39.2008,  38.6153,  40.8385,  51.0764,  66.4063,  66.0905,\n",
       "         64.0675,  62.4913,  61.0954,  54.7478,  59.8567,  52.6986,\n",
       "         33.8791,  39.8631,  57.6889,  68.5677,  56.2854,  40.1066,\n",
       "         45.6545,  54.0666,  61.1862,  77.5283,  88.6151,  84.5339,\n",
       "         89.9741,  99.9901, 108.6364, 112.9241, 108.8536, 105.441 ,\n",
       "        108.6472, 115.5002],\n",
       "       [ 35.7127,  36.4907,  41.9139,  53.1024,  67.2906,  64.7793,\n",
       "         60.909 ,  59.208 ,  58.4916,  53.1116,  54.6723,  47.5635,\n",
       "         33.0641,  44.4282,  60.9983,  69.1655,  53.7695,  36.3027,\n",
       "         40.7798,  51.3057,  60.1154,  74.7566,  84.8435,  85.409 ,\n",
       "         90.7783, 100.9792, 109.3266, 113.3755, 107.8922, 104.3055,\n",
       "        107.9847, 113.8378],\n",
       "       [ 34.9731,  38.7123,  43.6302,  50.6512,  67.4863,  66.2522,\n",
       "         60.9583,  58.9091,  61.1539,  61.0507,  59.1279,  50.346 ,\n",
       "         40.1128,  46.8194,  59.5855,  66.7959,  51.0409,  30.7701,\n",
       "         37.4319,  52.8926,  62.4033,  72.6428,  80.8008,  85.654 ,\n",
       "         92.3113, 100.5986, 107.658 , 111.1908, 110.6639, 109.1479,\n",
       "        111.1693, 115.0765],\n",
       "       [ 35.6509,  41.145 ,  44.5531,  47.8965,  65.3679,  70.8835,\n",
       "         63.6607,  60.3559,  67.8929,  71.4688,  61.4158,  47.7592,\n",
       "         43.2481,  45.3573,  59.2157,  66.5185,  51.9421,  29.878 ,\n",
       "         35.3828,  52.7786,  67.5122,  73.464 ,  82.6435,  87.7956,\n",
       "         93.1972, 101.0007, 105.8754, 108.832 , 112.5821, 112.2509,\n",
       "        112.7177, 113.4726],\n",
       "       [ 36.8896,  45.8241,  50.1181,  54.3627,  66.3122,  69.4259,\n",
       "         63.7747,  60.9168,  68.8837,  72.2639,  61.422 ,  44.3789,\n",
       "         35.5523,  39.9109,  58.1988,  63.377 ,  51.0454,  32.0627,\n",
       "         36.7525,  49.7358,  61.5729,  71.1653,  84.2951,  90.0234,\n",
       "         95.67  , 103.6584, 110.6038, 112.6144, 112.8209, 111.5329,\n",
       "        111.5329, 112.163 ],\n",
       "       [ 35.4167,  43.6672,  49.0643,  53.694 ,  65.5233,  67.9468,\n",
       "         61.8504,  60.1234,  69.4384,  74.8785,  64.6775,  45.596 ,\n",
       "         35.1008,  39.3562,  56.9647,  63.0889,  53.6923,  35.8405,\n",
       "         36.9436,  46.7531,  57.9431,  69.7633,  82.7513,  87.1808,\n",
       "         92.7735, 101.7896, 109.7457, 112.6961, 112.4018, 111.9288,\n",
       "        111.2062, 110.5545],\n",
       "       [ 33.118 ,  39.059 ,  44.0324,  50.3091,  61.3342,  62.4096,\n",
       "         54.0083,  50.5634,  59.6118,  66.3399,  59.4545,  41.222 ,\n",
       "         32.021 ,  38.6182,  56.2529,  64.0565,  57.7089,  38.5968,\n",
       "         37.5861,  47.5312,  60.6825,  72.7738,  82.7405,  83.7404,\n",
       "         88.589 ,  97.334 , 105.8879, 110.5931, 111.1677, 110.4389,\n",
       "        110.4774, 110.006 ],\n",
       "       [ 34.0147,  36.0423,  39.5921,  48.8208,  67.1719,  66.91  ,\n",
       "         54.3719,  47.3556,  53.5614,  61.2894,  56.7906,  42.0047,\n",
       "         35.2827,  39.1683,  53.9003,  67.54  ,  58.8398,  38.7555,\n",
       "         35.2073,  49.8962,  65.4217,  74.6165,  82.7189,  82.3599,\n",
       "         87.2902,  96.519 , 105.6599, 110.892 , 110.6947, 108.6671,\n",
       "        108.6517, 110.838 ],\n",
       "       [ 34.9869,  35.4707,  39.6676,  50.0164,  69.8296,  67.9808,\n",
       "         52.785 ,  39.6984,  41.8938,  49.2197,  46.6283,  36.7387,\n",
       "         32.7884,  37.0653,  49.9115,  64.1382,  55.3132,  36.3212,\n",
       "         33.9578,  51.1842,  67.3892,  72.9865,  80.1814,  85.3596,\n",
       "         90.3438, 100.2736, 108.7135, 114.0057, 112.6406, 109.211 ,\n",
       "        109.9074, 112.7346],\n",
       "       [ 35.6602,  35.5462,  40.3193,  51.7712,  69.5738,  67.725 ,\n",
       "         52.8451,  37.5091,  35.879 ,  37.7432,  36.8142,  31.9996,\n",
       "         31.5759,  35.9776,  43.9984,  52.3073,  47.0582,  36.0762,\n",
       "         38.5382,  52.0639,  64.3725,  71.7524,  81.2029,  88.1529,\n",
       "         93.2233, 101.8974, 111.3264, 115.7497, 113.8855, 110.9998,\n",
       "        112.049 , 114.3431],\n",
       "       [ 36.1871,  35.4214,  40.6998,  52.4937,  70.0298,  69.9205,\n",
       "         55.6276,  37.8789,  34.1242,  37.2872,  35.6187,  32.8794,\n",
       "         33.8084,  36.3243,  38.6122,  40.6936,  39.3732,  36.2487,\n",
       "         41.3299,  50.8945,  59.7581,  71.0344,  84.5986,  87.8864,\n",
       "         92.3482, 100.6956, 109.2387, 114.494 , 112.6468, 110.0169,\n",
       "        111.2017, 114.1906]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 44.299 ,  45.9999,  51.3306,  52.7218,  49.4232,  44.9075,\n",
       "         36.0332,  32.8594,  39.2717,  46.157 ,  49.0966,  44.6841,\n",
       "         47.086 ,  50.5309,  54.8402,  60.2095,  59.6934,  60.4545,\n",
       "         58.618 ,  47.3202,  35.0826,  28.6981,  28.199 ,  28.5427,\n",
       "         26.9728,  28.3147,  28.2869,  26.8032,  24.6894,  25.2764,\n",
       "         27.515 ,  27.156 ],\n",
       "       [ 49.1351,  60.3081,  70.1222,  76.7086,  69.1115,  53.526 ,\n",
       "         40.6243,  30.1954,  33.3091,  42.7812,  50.8513,  51.8081,\n",
       "         58.5794,  70.1653,  80.36  ,  88.7891,  89.447 ,  91.0339,\n",
       "         88.9093,  75.4268,  52.6571,  32.1322,  21.215 ,  19.5311,\n",
       "         20.662 ,  25.3626,  27.3516,  25.5259,  23.4013,  23.7002,\n",
       "         25.2378,  24.2918],\n",
       "       [ 60.7595,  83.7141, 102.1961, 113.4939, 115.6077, 102.478 ,\n",
       "         74.9799,  41.9985,  26.4453,  35.5045,  54.5905,  71.3886,\n",
       "         88.2406, 106.6687, 117.7493, 121.6026, 124.8473, 127.733 ,\n",
       "        125.4343, 112.6035,  86.9912,  53.4506,  26.8061,  19.796 ,\n",
       "         22.7418,  26.6767,  27.9709,  26.6182,  24.0915,  24.5044,\n",
       "         24.9712,  22.8512],\n",
       "       [ 84.8773, 111.9024, 129.7543, 139.835 , 143.1875, 132.7586,\n",
       "        104.0048,  61.9534,  35.7325,  43.9767,  69.4211,  98.3598,\n",
       "        117.9404, 132.9775, 136.9879, 135.5751, 136.8631, 139.9337,\n",
       "        143.1183, 135.8848, 112.6852,  74.0742,  37.235 ,  21.1549,\n",
       "         22.03  ,  26.7368,  28.8029,  28.6242,  26.9233,  26.0913,\n",
       "         25.4981,  19.6065],\n",
       "       [109.0768, 134.8139, 145.5525, 149.1608, 146.9268, 137.9708,\n",
       "        117.3302,  77.3926,  50.8728,  62.4156,  93.8702, 122.054 ,\n",
       "        132.9344, 137.3146, 134.7709, 134.2054, 132.8958, 136.0481,\n",
       "        143.8902, 143.0151, 124.9999,  87.9049,  46.1093,  21.7311,\n",
       "         20.9654,  25.683 ,  27.2761,  27.5211,  25.6353,  24.2764,\n",
       "         23.9713,  20.0795],\n",
       "       [134.1021, 141.5529, 143.2754, 144.927 , 144.7529, 137.0849,\n",
       "        119.575 ,  79.5234,  53.7262,  75.7949, 109.4342, 134.3194,\n",
       "        135.7708, 125.4667, 117.1839, 111.8038, 106.4515, 121.8368,\n",
       "        139.3467, 141.3034, 133.7001,  98.7728,  48.6082,  22.9482,\n",
       "         19.5418,  21.0856,  22.6833,  21.9823,  20.7436,  20.6897,\n",
       "         21.3306,  22.8404],\n",
       "       [141.2971, 145.5633, 144.7421, 143.622 , 142.2523, 135.5303,\n",
       "        116.5044,  76.9366,  57.3237,  86.8755, 121.5856, 137.6288,\n",
       "        130.6403, 103.6521,  82.4245,  67.7464,  70.2793, 100.033 ,\n",
       "        130.1457, 137.7598, 133.754 ,  98.9407,  47.6622,  25.176 ,\n",
       "         22.6555,  19.5526,  20.4323,  19.4324,  19.8515,  20.4447,\n",
       "         21.9176,  24.9003],\n",
       "       [124.8796, 139.7318, 147.7957, 144.6758, 140.6053, 134.2962,\n",
       "        112.7867,  71.2191,  62.921 , 100.603 , 127.5311, 136.9709,\n",
       "        125.8518,  88.2668,  51.7418,  36.0037,  50.1225,  91.533 ,\n",
       "        125.9442, 136.0419, 131.9822,  94.1261,  44.4345,  27.8337,\n",
       "         26.498 ,  20.6773,  19.8992,  19.8992,  21.4923,  22.6124,\n",
       "         24.1993,  24.4273],\n",
       "       [ 97.8653, 124.1293, 143.9101, 143.5619, 140.4913, 134.3501,\n",
       "        108.009 ,  62.8008,  64.4047, 111.0319, 137.0741, 140.8996,\n",
       "        122.1664,  77.3541,  32.776 ,  28.9828,  49.1118,  94.6359,\n",
       "        129.346 , 139.6286, 131.9822,  92.0554,  40.25  ,  23.888 ,\n",
       "         26.9648,  22.6601,  22.3658,  22.4968,  23.2749,  24.1392,\n",
       "         23.6662,  22.4814],\n",
       "       [ 62.254 , 102.5767, 136.427 , 143.3231, 142.3771, 135.2899,\n",
       "        103.5903,  54.6814,  62.0028, 115.4614, 138.0309, 141.0136,\n",
       "        121.1387,  76.3264,  34.292 ,  36.6292,  59.117 , 104.0541,\n",
       "        135.8785, 143.2153, 134.2701,  92.9844,  36.6463,  15.5128,\n",
       "         22.2903,  24.9418,  25.0774,  23.3396,  21.819 ,  22.0362,\n",
       "         20.9053,  18.5958],\n",
       "       [ 51.2165,  94.8378, 133.8617, 142.4156, 141.5513, 135.4039,\n",
       "        102.2314,  49.9808,  62.1877, 116.9451, 141.9165, 140.2417,\n",
       "        123.5945,  78.0704,  37.2809,  39.803 ,  61.7639, 105.7011,\n",
       "        135.7645, 140.6886, 132.6832,  92.1524,  34.9284,  14.7948,\n",
       "         20.1595,  23.056 ,  24.5783,  22.4276,  22.1349,  21.1072,\n",
       "         20.0194,  19.0087],\n",
       "       [ 44.8473,  89.4685, 132.264 , 143.3015, 141.0244, 133.703 ,\n",
       "        105.889 ,  55.4641,  64.4972, 114.1842, 140.2587, 137.128 ,\n",
       "        121.7688,  74.8858,  35.0361,  34.4876,  58.4483, 102.8693,\n",
       "        132.9928, 138.6888, 130.5694,  94.5112,  39.8848,  20.2242,\n",
       "         21.5893,  19.7313,  21.1288,  21.8037,  23.2658,  23.352 ,\n",
       "         22.9652,  20.5956],\n",
       "       [ 39.837 ,  82.8713, 130.2642, 144.0734, 141.7963, 132.5352,\n",
       "        109.0197,  65.1812,  69.1978, 118.2269, 138.1988, 134.9002,\n",
       "        116.0853,  67.8326,  30.8516,  34.5846,  61.0828, 107.8626,\n",
       "        134.9325, 137.2868, 127.9395,  94.9411,  42.3037,  20.2412,\n",
       "         21.9653,  18.1075,  18.4234,  20.7391,  24.429 ,  26.5859,\n",
       "         24.7971,  21.4815],\n",
       "       [ 39.136 ,  80.3554, 128.8191, 142.8563, 139.9814, 132.6662,\n",
       "        114.8081,  75.1972,  73.3715, 116.395 , 140.4266, 136.3731,\n",
       "        113.9885,  65.0348,  29.8517,  40.2959,  71.3807, 116.5736,\n",
       "        139.4159, 135.8848, 126.1246,  91.0124,  36.0763,  18.1166,\n",
       "         23.8511,  20.1674,  18.4126,  20.1844,  24.4721,  26.5751,\n",
       "         25.9172,  21.1996],\n",
       "       [ 41.1466,  82.48  , 127.944 , 140.4544, 138.1665, 135.7368,\n",
       "        120.3407,  81.8437,  79.8978, 114.6232, 141.9534, 138.4438,\n",
       "        114.5324,  64.3939,  31.3955,  41.4977,  75.0661, 121.889 ,\n",
       "        140.6716, 135.6568, 121.538 ,  81.9532,  28.131 ,  15.5837,\n",
       "         22.4213,  20.6126,  20.2706,  21.1565,  23.3304,  23.8034,\n",
       "         22.8744,  21.3306],\n",
       "       [ 43.4175,  84.8649, 129.2258, 142.8671, 143.4218, 139.0955,\n",
       "        113.5694,  74.3005,  89.0001, 125.5682, 140.4975, 138.5147,\n",
       "        113.8314,  61.405 ,  32.5803,  41.2266,  78.1106, 125.5313,\n",
       "        141.1123, 136.0805, 122.4778,  78.5945,  29.7718,  23.2239,\n",
       "         25.3501,  22.1286,  21.6017,  21.6726,  23.0423,  22.4122,\n",
       "         21.0703,  20.5264],\n",
       "       [ 41.5039,  82.8373, 125.2585, 142.9811, 142.009 , 135.0959,\n",
       "         99.5538,  60.5021,  87.5164, 127.8731, 138.4007, 133.999 ,\n",
       "        111.3586,  58.7473,  36.3349,  48.6388,  86.9526, 128.3308,\n",
       "        138.9061, 134.6184, 122.1188,  70.1654,  25.2884,  25.9848,\n",
       "         23.2964,  21.2427,  19.8299,  21.1996,  23.7541,  22.8359,\n",
       "         20.1351,  19.7653],\n",
       "       [ 39.1343,  78.283 , 123.9597, 141.3834, 138.5964, 131.3952,\n",
       "         99.6678,  61.844 ,  86.2885, 124.7163, 139.4715, 132.1132,\n",
       "        105.6411,  54.3178,  36.3888,  56.6488,  95.0227, 131.4014,\n",
       "        141.2048, 134.9712, 119.6999,  65.8068,  21.5877,  22.1701,\n",
       "         21.0085,  20.0086,  20.6603,  22.7911,  25.4057,  23.8897,\n",
       "         21.1889,  19.8793],\n",
       "       [ 38.7214,  75.9412, 122.949 , 142.0844, 140.5962, 135.5088,\n",
       "        106.5531,  67.9852,  89.7181, 125.9011, 140.7703, 129.1674,\n",
       "         95.565 ,  43.4698,  32.1334,  60.9365, 100.7941, 135.173 ,\n",
       "        142.9057, 134.1993, 115.2273,  64.5188,  22.1146,  22.11  ,\n",
       "         21.0732,  20.4862,  21.8497,  23.9805,  25.6984,  25.4165,\n",
       "         22.292 ,  20.5695],\n",
       "       [ 38.3794,  74.5993, 122.02  , 143.2584, 142.123 , 138.7365,\n",
       "        113.2643,  76.3542,  96.0873, 130.8575, 142.8949, 126.5636,\n",
       "         87.7768,  35.7956,  30.1166,  62.5234, 104.0388, 137.6458,\n",
       "        142.7208, 127.542 , 102.4566,  58.4054,  22.8264,  21.9359,\n",
       "         20.725 ,  21.2797,  22.2904,  24.3503,  25.9434,  25.1777,\n",
       "         22.1564,  21.2597],\n",
       "       [ 36.4505,  73.2466, 121.7812, 143.7745, 141.7532, 135.5627,\n",
       "        114.4491,  80.0118, 101.0437, 133.7432, 145.0087, 127.2646,\n",
       "         85.8201,  37.0127,  33.4044,  62.6544, 105.6966, 136.6459,\n",
       "        138.2482, 120.5858,  87.0883,  48.6945,  21.4136,  20.463 ,\n",
       "         20.3121,  21.9977,  22.1826,  24.0145,  26.1237,  25.472 ,\n",
       "         22.2766,  20.7328],\n",
       "       [ 36.6354,  75.4313, 121.1942, 140.6608, 139.3944, 131.7911,\n",
       "        109.9765,  79.3108, 102.9295, 136.1559, 143.297 , 124.08  ,\n",
       "         84.6954,  39.5887,  34.9975,  62.3725, 108.0554, 133.7602,\n",
       "        133.1778, 108.929 ,  68.4322,  33.3971,  22.4135,  18.4632,\n",
       "         20.4261,  22.9329,  22.537 ,  22.0702,  24.3042,  23.3644,\n",
       "         21.7066,  22.1025],\n",
       "       [ 40.048 ,  77.73  , 121.1511, 136.3731, 137.0356, 132.1331,\n",
       "        107.6177,  78.9518, 104.3423, 135.857 , 137.9385, 119.0096,\n",
       "         82.0547,  40.8937,  35.8896,  68.8018, 112.6528, 132.8142,\n",
       "        127.2215,  96.3863,  49.7653,  23.5013,  19.762 ,  17.2245,\n",
       "         21.6602,  23.1671,  21.1735,  21.2937,  22.5386,  21.8268,\n",
       "         20.2938,  21.1735],\n",
       "       [ 47.3031,  84.9851, 125.4496, 137.0849, 135.5627, 128.6604,\n",
       "         99.4336,  75.1802, 105.3853, 135.4271, 135.7969, 118.4118,\n",
       "         87.4024,  52.7416,  45.0367,  76.704 , 117.2394, 133.0422,\n",
       "        126.8625,  93.1308,  40.9233,  22.0777,  22.11  ,  18.3877,\n",
       "         21.5354,  23.7433,  22.3906,  22.7496,  23.1795,  22.4677,\n",
       "         20.7067,  18.9996],\n",
       "       [ 54.7862,  92.4682, 129.1611, 139.0246, 132.905 , 119.7044,\n",
       "         86.364 ,  68.224 , 105.2543, 137.1819, 136.6119, 123.9875,\n",
       "        101.6352,  75.1046,  62.998 ,  85.5352, 121.9508, 132.1563,\n",
       "        128.5033,  96.5865,  45.325 ,  27.7243,  26.1589,  21.7356,\n",
       "         23.6985,  22.6016,  21.2427,  21.1996,  21.9284,  22.5755,\n",
       "         19.5866,  18.2277],\n",
       "       [ 63.0304, 100.4135, 134.4056, 142.0952, 130.6772, 110.8193,\n",
       "         75.3651,  60.1108, 103.5965, 138.1109, 139.6008, 131.7048,\n",
       "        120.0094, 100.3641,  86.9094,  99.0177, 130.0748, 134.683 ,\n",
       "        131.617 , 102.5751,  48.6128,  28.4253,  25.518 ,  22.9697,\n",
       "         26.9755,  23.5737,  22.0407,  22.8835,  23.6832,  23.6293,\n",
       "         21.2274,  19.9286],\n",
       "       [ 73.9431, 112.9131, 138.7211, 139.6394, 131.993 , 108.6085,\n",
       "         70.5675,  61.8395,  98.6679, 135.5842, 140.4328, 129.266 ,\n",
       "        122.2389, 125.314 , 118.2115, 119.7769, 134.1669, 139.8782,\n",
       "        131.161 ,  96.1798,  50.7328,  26.7845,  27.051 ,  33.2738,\n",
       "         30.7363,  30.7902,  30.784 ,  30.3989,  30.4437,  28.8029,\n",
       "         24.9281,  21.8683],\n",
       "       [ 78.4543, 119.4349, 142.7593, 142.7701, 136.2977, 114.098 ,\n",
       "         76.302 ,  62.9982,  93.4682, 129.4385, 140.1725, 137.9617,\n",
       "        136.0158, 141.6993, 137.4655, 134.7324, 142.5853, 145.3893,\n",
       "        131.8359,  94.0291,  58.3962,  38.4305,  37.0069,  43.3545,\n",
       "         41.1159,  39.5721,  38.3272,  35.6434,  33.3294,  29.6996,\n",
       "         25.5259,  22.2812],\n",
       "       [ 86.2471, 123.6302, 144.471 , 144.7591, 140.8134, 121.2113,\n",
       "         82.4262,  61.4822,  81.8931, 116.0916, 133.0638, 139.2651,\n",
       "        142.2047, 144.4864, 142.2524, 138.9215, 141.1663, 139.8459,\n",
       "        122.8691,  89.7198,  56.8586,  40.1314,  37.6155,  42.6751,\n",
       "         42.0342,  41.5073,  40.2085,  37.3937,  34.8778,  31.0631,\n",
       "         25.7755,  20.0041],\n",
       "       [ 67.1072,  93.2464, 109.2017, 110.0275, 109.0214,  97.2336,\n",
       "         64.4479,  41.2052,  48.7915,  71.2084,  88.1097,  94.2401,\n",
       "         97.1797,  96.6358,  95.9178,  97.6295,  94.4018,  89.2775,\n",
       "         77.7301,  56.2376,  34.0549,  24.5289,  22.8881,  25.6167,\n",
       "         26.2207,  27.0635,  27.1775,  27.7044,  28.6442,  26.6444,\n",
       "         24.6015,  22.9607],\n",
       "       [ 24.7569,  36.6417,  48.9071,  50.7436,  45.439 ,  38.341 ,\n",
       "         27.0262,  16.3693,  17.6573,  25.0048,  28.0215,  27.9075,\n",
       "         30.1353,  31.1783,  30.4064,  28.1616,  23.934 ,  24.8908,\n",
       "         25.7228,  20.9082,  13.7949,  13.154 ,  13.986 ,  16.465 ,\n",
       "         17.542 ,  16.7548,  17.1785,  19.6451,  22.8127,  21.9268,\n",
       "         21.5309,  21.5479],\n",
       "       [ 22.6584,  22.7724,  27.2666,  25.1744,  22.3919,  23.5273,\n",
       "         26.3852,  26.3852,  26.983 ,  27.684 ,  26.5701,  26.9399,\n",
       "         28.3527,  27.1679,  30.3848,  25.5594,  22.8047,  23.4734,\n",
       "         27.8428,  29.7394,  25.7398,  24.099 ,  22.5614,  24.4426,\n",
       "         25.9325,  21.2705,  18.5204,  19.813 ,  22.2688,  21.443 ,\n",
       "         20.8191,  20.0812]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets convert all Dataframes to arrays\n",
    "NN_X_Train=np.array(X_Train)\n",
    "NN_X_Test=np.array(X_Test)\n",
    "NN_Y_Train=np.array(Y_Train)\n",
    "NN_Y_Test=np.array(Y_Test)\n",
    "NN_X_Val=np.array(X_Val)\n",
    "NN_Y_Val=np.array(Y_Val)\n",
    "\n",
    "# Display\n",
    "NN_X_Train[0]\n",
    "NN_Y_Train[0]\n",
    "NN_X_Test[0]\n",
    "NN_Y_Test[0]\n",
    "NN_X_Val[0]\n",
    "NN_Y_Val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 32, 32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(18000, 32, 32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(60000, 32, 32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(42000, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(18000, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(60000, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets flatten all the input vectors\n",
    "\n",
    "NN_X_Train.shape\n",
    "NN_X_Test.shape\n",
    "NN_X_Val.shape\n",
    "\n",
    "NN_X_Train = NN_X_Train.reshape((NN_X_Train.shape[0], -1))\n",
    "NN_X_Test = NN_X_Test.reshape((NN_X_Test.shape[0], -1))\n",
    "NN_X_Val = NN_X_Val.reshape((NN_X_Val.shape[0], -1))\n",
    "\n",
    "NN_X_Train.shape\n",
    "NN_X_Test.shape\n",
    "NN_X_Val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG value of y_train: 2\n",
      "OG  hot value of y_test: 1\n",
      "OG hot value of y_val: 0\n",
      "One hot value of y_train: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "One hot value of y_test: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "One hot value of y_val: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Lets convert all outputs to One hot vectors\n",
    "#Converting our target variables to one hot vetors\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"OG value of y_train:\", NN_Y_Train[0])\n",
    "print(\"OG  hot value of y_test:\", NN_Y_Test[0])\n",
    "print(\"OG hot value of y_val:\", NN_Y_Val[0])\n",
    "\n",
    "NN_Y_Train = to_categorical(NN_Y_Train, num_classes=10)\n",
    "NN_Y_Test = to_categorical(NN_Y_Test, num_classes=10)\n",
    "NN_Y_Val = to_categorical(NN_Y_Val, num_classes=10)\n",
    "\n",
    "\n",
    "print(\"One hot value of y_train:\", NN_Y_Train[0])\n",
    "print(\"One hot value of y_test:\", NN_Y_Test[0])\n",
    "print(\"One hot value of y_val:\", NN_Y_Val[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3 Normalisation/Scaling\n",
    "#### Normalising input data before feeding into Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254.9745\n",
      "0.0\n",
      "254.9745\n",
      "0.0\n",
      "254.9745\n",
      "0.0\n",
      "0.9999\n",
      "0.0\n",
      "0.9999\n",
      "0.0\n",
      "0.9999\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Dividing by 255 to make it all between 0 1nd 1 \n",
    "print(NN_X_Train.max())\n",
    "print(NN_X_Train.min())\n",
    "\n",
    "print(NN_X_Test.max())\n",
    "print(NN_X_Test.min())\n",
    "\n",
    "print(NN_X_Val.max())\n",
    "print(NN_X_Val.min())\n",
    "\n",
    "\n",
    "\n",
    "NN_X_Train=NN_X_Train/255.0\n",
    "NN_X_Test=NN_X_Test/255.0\n",
    "NN_X_Val= NN_X_Val/255.0\n",
    "\n",
    "print(NN_X_Train.max())\n",
    "print(NN_X_Train.min())\n",
    "\n",
    "print(NN_X_Test.max())\n",
    "print(NN_X_Test.min())\n",
    "\n",
    "print(NN_X_Val.max())\n",
    "print(NN_X_Val.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Blank Space\n",
    "\n",
    "\n",
    "### Blank Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Type</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "      <th>Test_Acc</th>\n",
       "      <th>Hidden layers</th>\n",
       "      <th>Hyper Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model_Type, Train_Acc, Val_Acc, Test_Acc, Hidden layers, Hyper Params]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets create a dataframe to store results from each FINAL model that we arrive at \n",
    "ResultsDF=pd.DataFrame(columns=['Model_Type','Train_Acc','Val_Acc','Test_Acc','Hidden layers','Hyper Params'])\n",
    "ResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank Space \n",
    "\n",
    "# Blank Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Build Neural Network\n",
    "-Start with building  a basic network  \n",
    "-Based on results , see whether it should be tuned further\n",
    "-Test The network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a  model & Establish baseline score\n",
    "- Start with 2 hidden layers\n",
    "- We will go with best practices such as relu activation function , he s weights and adam optimizer(with lr =0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 3.9340 - accuracy: 0.1027\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.10980, saving model to best_model.h5\n",
      "10/10 [==============================] - 14s 1s/step - loss: 3.9340 - accuracy: 0.1027 - val_loss: 2.3403 - val_accuracy: 0.1098\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.3153 - accuracy: 0.1169\n",
      "Epoch 00002: val_accuracy improved from 0.10980 to 0.16492, saving model to best_model.h5\n",
      "10/10 [==============================] - 15s 2s/step - loss: 2.3153 - accuracy: 0.1169 - val_loss: 2.2887 - val_accuracy: 0.1649\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.2756 - accuracy: 0.1720\n",
      "Epoch 00003: val_accuracy improved from 0.16492 to 0.18915, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 2.2756 - accuracy: 0.1720 - val_loss: 2.2486 - val_accuracy: 0.1892\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.2090 - accuracy: 0.2321\n",
      "Epoch 00004: val_accuracy improved from 0.18915 to 0.27047, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 2.2090 - accuracy: 0.2321 - val_loss: 2.1543 - val_accuracy: 0.2705\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.0934 - accuracy: 0.3054\n",
      "Epoch 00005: val_accuracy improved from 0.27047 to 0.34973, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 2.0934 - accuracy: 0.3054 - val_loss: 2.0147 - val_accuracy: 0.3497\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.9512 - accuracy: 0.3674\n",
      "Epoch 00006: val_accuracy improved from 0.34973 to 0.41073, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.9512 - accuracy: 0.3674 - val_loss: 1.8573 - val_accuracy: 0.4107\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.7967 - accuracy: 0.4274\n",
      "Epoch 00007: val_accuracy improved from 0.41073 to 0.46777, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.7967 - accuracy: 0.4274 - val_loss: 1.7060 - val_accuracy: 0.4678\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6621 - accuracy: 0.4723\n",
      "Epoch 00008: val_accuracy improved from 0.46777 to 0.50163, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.6621 - accuracy: 0.4723 - val_loss: 1.5960 - val_accuracy: 0.5016\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5469 - accuracy: 0.5169\n",
      "Epoch 00009: val_accuracy improved from 0.50163 to 0.53170, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.5469 - accuracy: 0.5169 - val_loss: 1.4941 - val_accuracy: 0.5317\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5107 - accuracy: 0.5177\n",
      "Epoch 00010: val_accuracy improved from 0.53170 to 0.56228, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.5107 - accuracy: 0.5177 - val_loss: 1.4335 - val_accuracy: 0.5623\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4154 - accuracy: 0.5615\n",
      "Epoch 00011: val_accuracy did not improve from 0.56228\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.4154 - accuracy: 0.5615 - val_loss: 1.4022 - val_accuracy: 0.5559\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3479 - accuracy: 0.5901\n",
      "Epoch 00012: val_accuracy improved from 0.56228 to 0.61057, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.3479 - accuracy: 0.5901 - val_loss: 1.2948 - val_accuracy: 0.6106\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2813 - accuracy: 0.6130\n",
      "Epoch 00013: val_accuracy did not improve from 0.61057\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.2813 - accuracy: 0.6130 - val_loss: 1.2654 - val_accuracy: 0.6069\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2417 - accuracy: 0.6220\n",
      "Epoch 00014: val_accuracy improved from 0.61057 to 0.64817, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.2417 - accuracy: 0.6220 - val_loss: 1.1937 - val_accuracy: 0.6482\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2242 - accuracy: 0.6238\n",
      "Epoch 00015: val_accuracy did not improve from 0.64817\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.2242 - accuracy: 0.6238 - val_loss: 1.1932 - val_accuracy: 0.6345\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1947 - accuracy: 0.6323\n",
      "Epoch 00016: val_accuracy did not improve from 0.64817\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.1947 - accuracy: 0.6323 - val_loss: 1.1618 - val_accuracy: 0.6429\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1368 - accuracy: 0.6549\n",
      "Epoch 00017: val_accuracy improved from 0.64817 to 0.66792, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.1368 - accuracy: 0.6549 - val_loss: 1.1086 - val_accuracy: 0.6679\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0884 - accuracy: 0.6738\n",
      "Epoch 00018: val_accuracy improved from 0.66792 to 0.67897, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.0884 - accuracy: 0.6738 - val_loss: 1.0742 - val_accuracy: 0.6790\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0456 - accuracy: 0.6899\n",
      "Epoch 00019: val_accuracy improved from 0.67897 to 0.69065, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.0456 - accuracy: 0.6899 - val_loss: 1.0403 - val_accuracy: 0.6906\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0302 - accuracy: 0.6913\n",
      "Epoch 00020: val_accuracy did not improve from 0.69065\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.0302 - accuracy: 0.6913 - val_loss: 1.0282 - val_accuracy: 0.6850\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0090 - accuracy: 0.6963\n",
      "Epoch 00021: val_accuracy improved from 0.69065 to 0.70053, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.0090 - accuracy: 0.6963 - val_loss: 0.9992 - val_accuracy: 0.7005\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9872 - accuracy: 0.7065\n",
      "Epoch 00022: val_accuracy improved from 0.70053 to 0.70300, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.9872 - accuracy: 0.7065 - val_loss: 0.9846 - val_accuracy: 0.7030\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9898 - accuracy: 0.7003\n",
      "Epoch 00023: val_accuracy improved from 0.70300 to 0.72055, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.9898 - accuracy: 0.7003 - val_loss: 0.9445 - val_accuracy: 0.7206\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9586 - accuracy: 0.7118\n",
      "Epoch 00024: val_accuracy improved from 0.72055 to 0.72192, saving model to best_model.h5\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.9586 - accuracy: 0.7118 - val_loss: 0.9392 - val_accuracy: 0.7219\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9295 - accuracy: 0.7231\n",
      "Epoch 00025: val_accuracy improved from 0.72192 to 0.72863, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.9295 - accuracy: 0.7231 - val_loss: 0.9163 - val_accuracy: 0.7286\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9117 - accuracy: 0.7285\n",
      "Epoch 00026: val_accuracy improved from 0.72863 to 0.72897, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.9117 - accuracy: 0.7285 - val_loss: 0.9113 - val_accuracy: 0.7290\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8976 - accuracy: 0.7308\n",
      "Epoch 00027: val_accuracy improved from 0.72897 to 0.73617, saving model to best_model.h5\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.8976 - accuracy: 0.7308 - val_loss: 0.8912 - val_accuracy: 0.7362\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8742 - accuracy: 0.7402\n",
      "Epoch 00028: val_accuracy did not improve from 0.73617\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.8742 - accuracy: 0.7402 - val_loss: 0.9062 - val_accuracy: 0.7219\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9130 - accuracy: 0.7192\n",
      "Epoch 00029: val_accuracy did not improve from 0.73617\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.9130 - accuracy: 0.7192 - val_loss: 0.9558 - val_accuracy: 0.7109\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8943 - accuracy: 0.7288\n",
      "Epoch 00030: val_accuracy improved from 0.73617 to 0.73887, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.8943 - accuracy: 0.7288 - val_loss: 0.8746 - val_accuracy: 0.7389\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8503 - accuracy: 0.7470\n",
      "Epoch 00031: val_accuracy did not improve from 0.73887\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.8503 - accuracy: 0.7470 - val_loss: 0.8785 - val_accuracy: 0.7368\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8748 - accuracy: 0.7338\n",
      "Epoch 00032: val_accuracy did not improve from 0.73887\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.8748 - accuracy: 0.7338 - val_loss: 0.8786 - val_accuracy: 0.7339\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8338 - accuracy: 0.7496\n",
      "Epoch 00033: val_accuracy did not improve from 0.73887\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.8338 - accuracy: 0.7496 - val_loss: 0.8824 - val_accuracy: 0.7349\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8322 - accuracy: 0.7498\n",
      "Epoch 00034: val_accuracy improved from 0.73887 to 0.74943, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.8322 - accuracy: 0.7498 - val_loss: 0.8330 - val_accuracy: 0.7494\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8117 - accuracy: 0.7549\n",
      "Epoch 00035: val_accuracy improved from 0.74943 to 0.75105, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.8117 - accuracy: 0.7549 - val_loss: 0.8300 - val_accuracy: 0.7510\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8006 - accuracy: 0.7585\n",
      "Epoch 00036: val_accuracy improved from 0.75105 to 0.76360, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.8006 - accuracy: 0.7585 - val_loss: 0.7925 - val_accuracy: 0.7636\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7755 - accuracy: 0.7671\n",
      "Epoch 00037: val_accuracy did not improve from 0.76360\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.7755 - accuracy: 0.7671 - val_loss: 0.7956 - val_accuracy: 0.7632\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7580 - accuracy: 0.7735\n",
      "Epoch 00038: val_accuracy improved from 0.76360 to 0.77228, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.7580 - accuracy: 0.7735 - val_loss: 0.7711 - val_accuracy: 0.7723\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7455 - accuracy: 0.7780\n",
      "Epoch 00039: val_accuracy did not improve from 0.77228\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.7455 - accuracy: 0.7780 - val_loss: 0.7699 - val_accuracy: 0.7707\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7405 - accuracy: 0.7788\n",
      "Epoch 00040: val_accuracy improved from 0.77228 to 0.77275, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.7405 - accuracy: 0.7788 - val_loss: 0.7645 - val_accuracy: 0.7728\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7340 - accuracy: 0.7817\n",
      "Epoch 00041: val_accuracy improved from 0.77275 to 0.77367, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.7340 - accuracy: 0.7817 - val_loss: 0.7589 - val_accuracy: 0.7737\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7253 - accuracy: 0.7850\n",
      "Epoch 00042: val_accuracy improved from 0.77367 to 0.77952, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.7253 - accuracy: 0.7850 - val_loss: 0.7399 - val_accuracy: 0.7795\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7106 - accuracy: 0.7875\n",
      "Epoch 00043: val_accuracy improved from 0.77952 to 0.78355, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.7106 - accuracy: 0.7875 - val_loss: 0.7345 - val_accuracy: 0.7836\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6973 - accuracy: 0.7939\n",
      "Epoch 00044: val_accuracy improved from 0.78355 to 0.78603, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.6973 - accuracy: 0.7939 - val_loss: 0.7248 - val_accuracy: 0.7860\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7093 - accuracy: 0.7861\n",
      "Epoch 00045: val_accuracy improved from 0.78603 to 0.79137, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.7093 - accuracy: 0.7861 - val_loss: 0.7076 - val_accuracy: 0.7914\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6753 - accuracy: 0.7986\n",
      "Epoch 00046: val_accuracy improved from 0.79137 to 0.79800, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.6753 - accuracy: 0.7986 - val_loss: 0.6861 - val_accuracy: 0.7980\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6795 - accuracy: 0.7993\n",
      "Epoch 00047: val_accuracy did not improve from 0.79800\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.6795 - accuracy: 0.7993 - val_loss: 0.7171 - val_accuracy: 0.7847\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6679 - accuracy: 0.8011\n",
      "Epoch 00048: val_accuracy did not improve from 0.79800\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.6679 - accuracy: 0.8011 - val_loss: 0.6905 - val_accuracy: 0.7964\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.8023\n",
      "Epoch 00049: val_accuracy improved from 0.79800 to 0.80472, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.6610 - accuracy: 0.8023 - val_loss: 0.6672 - val_accuracy: 0.8047\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6393 - accuracy: 0.8099\n",
      "Epoch 00050: val_accuracy improved from 0.80472 to 0.80577, saving model to best_model.h5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.6393 - accuracy: 0.8099 - val_loss: 0.6626 - val_accuracy: 0.8058\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6404 - accuracy: 0.8075\n",
      "Epoch 00051: val_accuracy did not improve from 0.80577\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.6404 - accuracy: 0.8075 - val_loss: 0.6654 - val_accuracy: 0.8051\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6329 - accuracy: 0.8120\n",
      "Epoch 00052: val_accuracy improved from 0.80577 to 0.81387, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.6329 - accuracy: 0.8120 - val_loss: 0.6394 - val_accuracy: 0.8139\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6249 - accuracy: 0.8145\n",
      "Epoch 00053: val_accuracy did not improve from 0.81387\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.6249 - accuracy: 0.8145 - val_loss: 0.6374 - val_accuracy: 0.8126\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6218 - accuracy: 0.8144\n",
      "Epoch 00054: val_accuracy did not improve from 0.81387\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.6218 - accuracy: 0.8144 - val_loss: 0.6551 - val_accuracy: 0.8068\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.8135\n",
      "Epoch 00055: val_accuracy did not improve from 0.81387\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.6269 - accuracy: 0.8135 - val_loss: 0.6738 - val_accuracy: 0.7986\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6602 - accuracy: 0.8012\n",
      "Epoch 00056: val_accuracy did not improve from 0.81387\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.6602 - accuracy: 0.8012 - val_loss: 0.6496 - val_accuracy: 0.8084\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6175 - accuracy: 0.8163\n",
      "Epoch 00057: val_accuracy improved from 0.81387 to 0.81410, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.6175 - accuracy: 0.8163 - val_loss: 0.6321 - val_accuracy: 0.8141\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5914 - accuracy: 0.8248\n",
      "Epoch 00058: val_accuracy improved from 0.81410 to 0.82102, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5914 - accuracy: 0.8248 - val_loss: 0.6125 - val_accuracy: 0.8210\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5920 - accuracy: 0.8212\n",
      "Epoch 00059: val_accuracy improved from 0.82102 to 0.82180, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5920 - accuracy: 0.8212 - val_loss: 0.6052 - val_accuracy: 0.8218\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5800 - accuracy: 0.8280\n",
      "Epoch 00060: val_accuracy did not improve from 0.82180\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5800 - accuracy: 0.8280 - val_loss: 0.6206 - val_accuracy: 0.8171\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5889 - accuracy: 0.8248\n",
      "Epoch 00061: val_accuracy did not improve from 0.82180\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5889 - accuracy: 0.8248 - val_loss: 0.6192 - val_accuracy: 0.8187\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5702 - accuracy: 0.8320\n",
      "Epoch 00062: val_accuracy improved from 0.82180 to 0.82335, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5702 - accuracy: 0.8320 - val_loss: 0.6008 - val_accuracy: 0.8234\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5643 - accuracy: 0.8332\n",
      "Epoch 00063: val_accuracy did not improve from 0.82335\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.5643 - accuracy: 0.8332 - val_loss: 0.6346 - val_accuracy: 0.8114\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.8242\n",
      "Epoch 00064: val_accuracy improved from 0.82335 to 0.82838, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5880 - accuracy: 0.8242 - val_loss: 0.5898 - val_accuracy: 0.8284\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5812 - accuracy: 0.8249\n",
      "Epoch 00065: val_accuracy did not improve from 0.82838\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5812 - accuracy: 0.8249 - val_loss: 0.6888 - val_accuracy: 0.7917\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6518 - accuracy: 0.7999\n",
      "Epoch 00066: val_accuracy did not improve from 0.82838\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.6518 - accuracy: 0.7999 - val_loss: 0.6268 - val_accuracy: 0.8159\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5882 - accuracy: 0.8246\n",
      "Epoch 00067: val_accuracy did not improve from 0.82838\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5882 - accuracy: 0.8246 - val_loss: 0.6125 - val_accuracy: 0.8192\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5580 - accuracy: 0.8340\n",
      "Epoch 00068: val_accuracy improved from 0.82838 to 0.82987, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5580 - accuracy: 0.8340 - val_loss: 0.5793 - val_accuracy: 0.8299\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5428 - accuracy: 0.8379\n",
      "Epoch 00069: val_accuracy improved from 0.82987 to 0.83315, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5428 - accuracy: 0.8379 - val_loss: 0.5715 - val_accuracy: 0.8332\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5413 - accuracy: 0.8388\n",
      "Epoch 00070: val_accuracy did not improve from 0.83315\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5413 - accuracy: 0.8388 - val_loss: 0.5877 - val_accuracy: 0.8264\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.8401\n",
      "Epoch 00071: val_accuracy did not improve from 0.83315\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5372 - accuracy: 0.8401 - val_loss: 0.5765 - val_accuracy: 0.8301\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.8450\n",
      "Epoch 00072: val_accuracy did not improve from 0.83315\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5256 - accuracy: 0.8450 - val_loss: 0.5798 - val_accuracy: 0.8280\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5260 - accuracy: 0.8439\n",
      "Epoch 00073: val_accuracy improved from 0.83315 to 0.84015, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5260 - accuracy: 0.8439 - val_loss: 0.5491 - val_accuracy: 0.8401\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5172 - accuracy: 0.8457\n",
      "Epoch 00074: val_accuracy did not improve from 0.84015\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5172 - accuracy: 0.8457 - val_loss: 0.5824 - val_accuracy: 0.8276\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.8418\n",
      "Epoch 00075: val_accuracy did not improve from 0.84015\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5278 - accuracy: 0.8418 - val_loss: 0.5627 - val_accuracy: 0.8338\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5126 - accuracy: 0.8486\n",
      "Epoch 00076: val_accuracy improved from 0.84015 to 0.84282, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5126 - accuracy: 0.8486 - val_loss: 0.5404 - val_accuracy: 0.8428\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5016 - accuracy: 0.8527\n",
      "Epoch 00077: val_accuracy improved from 0.84282 to 0.84528, saving model to best_model.h5\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.5016 - accuracy: 0.8527 - val_loss: 0.5332 - val_accuracy: 0.8453\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4876 - accuracy: 0.8564\n",
      "Epoch 00078: val_accuracy improved from 0.84528 to 0.84908, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4876 - accuracy: 0.8564 - val_loss: 0.5229 - val_accuracy: 0.8491\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4770 - accuracy: 0.8592\n",
      "Epoch 00079: val_accuracy improved from 0.84908 to 0.84915, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4770 - accuracy: 0.8592 - val_loss: 0.5220 - val_accuracy: 0.8492\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4714 - accuracy: 0.8619\n",
      "Epoch 00080: val_accuracy did not improve from 0.84915\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4714 - accuracy: 0.8619 - val_loss: 0.5289 - val_accuracy: 0.8457\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.8581\n",
      "Epoch 00081: val_accuracy improved from 0.84915 to 0.84958, saving model to best_model.h5\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.4806 - accuracy: 0.8581 - val_loss: 0.5155 - val_accuracy: 0.8496\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4819 - accuracy: 0.8576\n",
      "Epoch 00082: val_accuracy did not improve from 0.84958\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4819 - accuracy: 0.8576 - val_loss: 0.5228 - val_accuracy: 0.8465\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.8518\n",
      "Epoch 00083: val_accuracy did not improve from 0.84958\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.4952 - accuracy: 0.8518 - val_loss: 0.5482 - val_accuracy: 0.8375\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4906 - accuracy: 0.8534\n",
      "Epoch 00084: val_accuracy did not improve from 0.84958\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4906 - accuracy: 0.8534 - val_loss: 0.5440 - val_accuracy: 0.8385\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.8570\n",
      "Epoch 00085: val_accuracy did not improve from 0.84958\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4847 - accuracy: 0.8570 - val_loss: 0.5440 - val_accuracy: 0.8400\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4843 - accuracy: 0.8548\n",
      "Epoch 00086: val_accuracy improved from 0.84958 to 0.84998, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4843 - accuracy: 0.8548 - val_loss: 0.5120 - val_accuracy: 0.8500\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4590 - accuracy: 0.8643\n",
      "Epoch 00087: val_accuracy improved from 0.84998 to 0.85838, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4590 - accuracy: 0.8643 - val_loss: 0.4901 - val_accuracy: 0.8584\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.8619\n",
      "Epoch 00088: val_accuracy did not improve from 0.85838\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4635 - accuracy: 0.8619 - val_loss: 0.5269 - val_accuracy: 0.8457\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4706 - accuracy: 0.8592\n",
      "Epoch 00089: val_accuracy did not improve from 0.85838\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.4706 - accuracy: 0.8592 - val_loss: 0.5271 - val_accuracy: 0.8439\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4546 - accuracy: 0.8665\n",
      "Epoch 00090: val_accuracy did not improve from 0.85838\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4546 - accuracy: 0.8665 - val_loss: 0.5003 - val_accuracy: 0.8530\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4399 - accuracy: 0.8696\n",
      "Epoch 00091: val_accuracy improved from 0.85838 to 0.86098, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4399 - accuracy: 0.8696 - val_loss: 0.4841 - val_accuracy: 0.8610\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4371 - accuracy: 0.8706\n",
      "Epoch 00092: val_accuracy did not improve from 0.86098\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4371 - accuracy: 0.8706 - val_loss: 0.4850 - val_accuracy: 0.8592\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.8731\n",
      "Epoch 00093: val_accuracy did not improve from 0.86098\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4310 - accuracy: 0.8731 - val_loss: 0.4925 - val_accuracy: 0.8569\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8740\n",
      "Epoch 00094: val_accuracy improved from 0.86098 to 0.86367, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4268 - accuracy: 0.8740 - val_loss: 0.4728 - val_accuracy: 0.8637\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8774\n",
      "Epoch 00095: val_accuracy improved from 0.86367 to 0.86442, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4204 - accuracy: 0.8774 - val_loss: 0.4710 - val_accuracy: 0.8644\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4097 - accuracy: 0.8808\n",
      "Epoch 00096: val_accuracy improved from 0.86442 to 0.86908, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4097 - accuracy: 0.8808 - val_loss: 0.4571 - val_accuracy: 0.8691\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.8826\n",
      "Epoch 00097: val_accuracy did not improve from 0.86908\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4051 - accuracy: 0.8826 - val_loss: 0.4624 - val_accuracy: 0.8673\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4016 - accuracy: 0.8832\n",
      "Epoch 00098: val_accuracy did not improve from 0.86908\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4016 - accuracy: 0.8832 - val_loss: 0.4646 - val_accuracy: 0.8671\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.8765\n",
      "Epoch 00099: val_accuracy improved from 0.86908 to 0.87135, saving model to best_model.h5\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.4187 - accuracy: 0.8765 - val_loss: 0.4535 - val_accuracy: 0.8713\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4004 - accuracy: 0.8842\n",
      "Epoch 00100: val_accuracy improved from 0.87135 to 0.87285, saving model to best_model.h5\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.4004 - accuracy: 0.8842 - val_loss: 0.4459 - val_accuracy: 0.8729\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# Initializing the model\n",
    "regressorModel = Sequential()\n",
    "# Add one layer to begin with \n",
    "regressorModel.add(Dense(input_dim = 1024, units=2048,activation='relu',kernel_initializer='he_normal'))\n",
    "regressorModel.add(Dense(units=2048,activation='relu',kernel_initializer='he_normal'))\n",
    "regressorModel.add(Dense(units=10,activation='softmax',kernel_initializer='he_normal'))\n",
    "\n",
    "# Configuration for compiler\n",
    "from tensorflow.keras import optimizers\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "regressorModel.compile(optimizer=adam ,loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# simple early stopping\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1 , patience=25)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# Fit the model\n",
    "historyRegressorModel=regressorModel.fit(NN_X_Train,NN_Y_Train, epochs=100,batch_size=4200,validation_data=(NN_X_Val,NN_Y_Val), callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################  Evaluate Model On Test Data ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 6s 11ms/step - loss: 0.5868 - accuracy: 0.8372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5867778062820435, 0.8371666669845581]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressorModel.evaluate(np.array(NN_X_Test),np.array(NN_Y_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################  Evaluate Model On Test Data ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8841904997825623 0.8728500008583069\n"
     ]
    }
   ],
   "source": [
    "best_train_score = max(historyRegressorModel.history['accuracy'])\n",
    "best_val_score = max(historyRegressorModel.history['val_accuracy'])\n",
    "print(best_train_score,best_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.4459 - accuracy: 0.8729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44588539004325867, 0.8728500008583069]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross verify score on Validation data of the best model\n",
    "\n",
    "saved_model.evaluate(np.array(NN_X_Val),np.array(NN_Y_Val))\n",
    "\n",
    "# Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 6s 10ms/step - loss: 0.5868 - accuracy: 0.8372 0s - loss: 0.5863 - accu\n"
     ]
    }
   ],
   "source": [
    "# Lets Look at score on Test Data\n",
    "\n",
    "(Test_Loss,Test_acc)=saved_model.evaluate(np.array(NN_X_Test),np.array(NN_Y_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturing best results from above epoch\n",
    "#ResultsDF=ResultsDF.append({ 'Model_Type' :\"Neural Network\",'Train_Acc':best_train_score,'Val_Acc':best_val_score,'Test_Acc':Test_acc,'Hidden layers':\"3 layer(2048 units) with relu activation\" ,'Hyper Params':'optimizer-Adam with lr 0.001,Weights-He'},ignore_index=True)\n",
    "#ResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Blank Space \n",
    "\n",
    "########### Blank Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhance/ Tune  model & Capture accuracy\n",
    "- Add a \"third\" hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 4.8047 - accuracy: 0.1000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.10432, saving model to best_model.h5\n",
      "10/10 [==============================] - 24s 2s/step - loss: 4.8047 - accuracy: 0.1000 - val_loss: 2.3271 - val_accuracy: 0.1043\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.3117 - accuracy: 0.1115\n",
      "Epoch 00002: val_accuracy improved from 0.10432 to 0.15158, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 2.3117 - accuracy: 0.1115 - val_loss: 2.3019 - val_accuracy: 0.1516\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.2890 - accuracy: 0.1459\n",
      "Epoch 00003: val_accuracy improved from 0.15158 to 0.21523, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 2.2890 - accuracy: 0.1459 - val_loss: 2.2671 - val_accuracy: 0.2152\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.2369 - accuracy: 0.2187\n",
      "Epoch 00004: val_accuracy improved from 0.21523 to 0.27137, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 2.2369 - accuracy: 0.2187 - val_loss: 2.1760 - val_accuracy: 0.2714\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.1293 - accuracy: 0.2717\n",
      "Epoch 00005: val_accuracy improved from 0.27137 to 0.29465, saving model to best_model.h5\n",
      "10/10 [==============================] - 27s 3s/step - loss: 2.1293 - accuracy: 0.2717 - val_loss: 2.0409 - val_accuracy: 0.2946\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.9571 - accuracy: 0.3476\n",
      "Epoch 00006: val_accuracy improved from 0.29465 to 0.36263, saving model to best_model.h5\n",
      "10/10 [==============================] - 27s 3s/step - loss: 1.9571 - accuracy: 0.3476 - val_loss: 1.9052 - val_accuracy: 0.3626\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.8054 - accuracy: 0.3956\n",
      "Epoch 00007: val_accuracy improved from 0.36263 to 0.44998, saving model to best_model.h5\n",
      "10/10 [==============================] - 27s 3s/step - loss: 1.8054 - accuracy: 0.3956 - val_loss: 1.6824 - val_accuracy: 0.4500\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6251 - accuracy: 0.4650\n",
      "Epoch 00008: val_accuracy improved from 0.44998 to 0.49248, saving model to best_model.h5\n",
      "10/10 [==============================] - 25s 3s/step - loss: 1.6251 - accuracy: 0.4650 - val_loss: 1.5459 - val_accuracy: 0.4925\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5365 - accuracy: 0.4878\n",
      "Epoch 00009: val_accuracy improved from 0.49248 to 0.49392, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.5365 - accuracy: 0.4878 - val_loss: 1.4962 - val_accuracy: 0.4939\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4359 - accuracy: 0.5343\n",
      "Epoch 00010: val_accuracy improved from 0.49392 to 0.57067, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.4359 - accuracy: 0.5343 - val_loss: 1.3617 - val_accuracy: 0.5707\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3410 - accuracy: 0.5713\n",
      "Epoch 00011: val_accuracy improved from 0.57067 to 0.59328, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.3410 - accuracy: 0.5713 - val_loss: 1.3018 - val_accuracy: 0.5933\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2831 - accuracy: 0.5929\n",
      "Epoch 00012: val_accuracy improved from 0.59328 to 0.60552, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.2831 - accuracy: 0.5929 - val_loss: 1.2424 - val_accuracy: 0.6055\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2088 - accuracy: 0.6202\n",
      "Epoch 00013: val_accuracy improved from 0.60552 to 0.62988, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.2088 - accuracy: 0.6202 - val_loss: 1.1883 - val_accuracy: 0.6299\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1593 - accuracy: 0.6352\n",
      "Epoch 00014: val_accuracy improved from 0.62988 to 0.64297, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.1593 - accuracy: 0.6352 - val_loss: 1.1353 - val_accuracy: 0.6430\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1321 - accuracy: 0.6455\n",
      "Epoch 00015: val_accuracy did not improve from 0.64297\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.1321 - accuracy: 0.6455 - val_loss: 1.1414 - val_accuracy: 0.6370\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0750 - accuracy: 0.6648\n",
      "Epoch 00016: val_accuracy improved from 0.64297 to 0.67403, saving model to best_model.h5\n",
      "10/10 [==============================] - 27s 3s/step - loss: 1.0750 - accuracy: 0.6648 - val_loss: 1.0622 - val_accuracy: 0.6740\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0429 - accuracy: 0.6761\n",
      "Epoch 00017: val_accuracy improved from 0.67403 to 0.69287, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.0429 - accuracy: 0.6761 - val_loss: 1.0166 - val_accuracy: 0.6929\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0576 - accuracy: 0.6683\n",
      "Epoch 00018: val_accuracy did not improve from 0.69287\n",
      "10/10 [==============================] - 27s 3s/step - loss: 1.0576 - accuracy: 0.6683 - val_loss: 1.0091 - val_accuracy: 0.6881\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9819 - accuracy: 0.6991\n",
      "Epoch 00019: val_accuracy improved from 0.69287 to 0.70327, saving model to best_model.h5\n",
      "10/10 [==============================] - 27s 3s/step - loss: 0.9819 - accuracy: 0.6991 - val_loss: 0.9687 - val_accuracy: 0.7033\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9437 - accuracy: 0.7107\n",
      "Epoch 00020: val_accuracy improved from 0.70327 to 0.72120, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.9437 - accuracy: 0.7107 - val_loss: 0.9230 - val_accuracy: 0.7212\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9111 - accuracy: 0.7194\n",
      "Epoch 00021: val_accuracy improved from 0.72120 to 0.72417, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.9111 - accuracy: 0.7194 - val_loss: 0.9021 - val_accuracy: 0.7242\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8854 - accuracy: 0.7283\n",
      "Epoch 00022: val_accuracy improved from 0.72417 to 0.73533, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8854 - accuracy: 0.7283 - val_loss: 0.8695 - val_accuracy: 0.7353\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8344 - accuracy: 0.7465\n",
      "Epoch 00023: val_accuracy did not improve from 0.73533\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8344 - accuracy: 0.7465 - val_loss: 0.9331 - val_accuracy: 0.7120\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8881 - accuracy: 0.7235\n",
      "Epoch 00024: val_accuracy improved from 0.73533 to 0.74508, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8881 - accuracy: 0.7235 - val_loss: 0.8429 - val_accuracy: 0.7451\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7993 - accuracy: 0.7573\n",
      "Epoch 00025: val_accuracy improved from 0.74508 to 0.75812, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7993 - accuracy: 0.7573 - val_loss: 0.7999 - val_accuracy: 0.7581\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7749 - accuracy: 0.7636\n",
      "Epoch 00026: val_accuracy improved from 0.75812 to 0.76215, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7749 - accuracy: 0.7636 - val_loss: 0.7781 - val_accuracy: 0.7621\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7764 - accuracy: 0.7634\n",
      "Epoch 00027: val_accuracy did not improve from 0.76215\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.7764 - accuracy: 0.7634 - val_loss: 0.8400 - val_accuracy: 0.7372\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7770 - accuracy: 0.7603\n",
      "Epoch 00028: val_accuracy improved from 0.76215 to 0.76352, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.7770 - accuracy: 0.7603 - val_loss: 0.7719 - val_accuracy: 0.7635\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7279 - accuracy: 0.7779\n",
      "Epoch 00029: val_accuracy improved from 0.76352 to 0.77413, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.7279 - accuracy: 0.7779 - val_loss: 0.7432 - val_accuracy: 0.7741\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7145 - accuracy: 0.7810\n",
      "Epoch 00030: val_accuracy improved from 0.77413 to 0.78132, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.7145 - accuracy: 0.7810 - val_loss: 0.7186 - val_accuracy: 0.7813\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.7898\n",
      "Epoch 00031: val_accuracy improved from 0.78132 to 0.78297, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.6909 - accuracy: 0.7898 - val_loss: 0.7104 - val_accuracy: 0.7830\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6778 - accuracy: 0.7913\n",
      "Epoch 00032: val_accuracy did not improve from 0.78297\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.6778 - accuracy: 0.7913 - val_loss: 0.7566 - val_accuracy: 0.7651\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6673 - accuracy: 0.7959\n",
      "Epoch 00033: val_accuracy improved from 0.78297 to 0.79807, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.6673 - accuracy: 0.7959 - val_loss: 0.6692 - val_accuracy: 0.7981\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6447 - accuracy: 0.8025\n",
      "Epoch 00034: val_accuracy did not improve from 0.79807\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.6447 - accuracy: 0.8025 - val_loss: 0.7018 - val_accuracy: 0.7854\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6306 - accuracy: 0.8076\n",
      "Epoch 00035: val_accuracy did not improve from 0.79807\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.6306 - accuracy: 0.8076 - val_loss: 0.6783 - val_accuracy: 0.7919\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6082 - accuracy: 0.8135\n",
      "Epoch 00036: val_accuracy improved from 0.79807 to 0.81153, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.6082 - accuracy: 0.8135 - val_loss: 0.6248 - val_accuracy: 0.8115\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5820 - accuracy: 0.8240\n",
      "Epoch 00037: val_accuracy improved from 0.81153 to 0.82317, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5820 - accuracy: 0.8240 - val_loss: 0.5945 - val_accuracy: 0.8232\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.8255\n",
      "Epoch 00038: val_accuracy did not improve from 0.82317\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5741 - accuracy: 0.8255 - val_loss: 0.6027 - val_accuracy: 0.8199\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5659 - accuracy: 0.8254\n",
      "Epoch 00039: val_accuracy did not improve from 0.82317\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.5659 - accuracy: 0.8254 - val_loss: 0.5929 - val_accuracy: 0.8223\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5740 - accuracy: 0.8228\n",
      "Epoch 00040: val_accuracy did not improve from 0.82317\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.5740 - accuracy: 0.8228 - val_loss: 0.6367 - val_accuracy: 0.8044\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5818 - accuracy: 0.8195\n",
      "Epoch 00041: val_accuracy improved from 0.82317 to 0.83032, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.5818 - accuracy: 0.8195 - val_loss: 0.5714 - val_accuracy: 0.8303\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5462 - accuracy: 0.8310\n",
      "Epoch 00042: val_accuracy did not improve from 0.83032\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.5462 - accuracy: 0.8310 - val_loss: 0.5692 - val_accuracy: 0.8284\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.8405\n",
      "Epoch 00043: val_accuracy did not improve from 0.83032\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.5223 - accuracy: 0.8405 - val_loss: 0.5665 - val_accuracy: 0.8290\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5142 - accuracy: 0.8414\n",
      "Epoch 00044: val_accuracy did not improve from 0.83032\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.5142 - accuracy: 0.8414 - val_loss: 0.5723 - val_accuracy: 0.8264\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4950 - accuracy: 0.8508\n",
      "Epoch 00045: val_accuracy improved from 0.83032 to 0.83930, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4950 - accuracy: 0.8508 - val_loss: 0.5331 - val_accuracy: 0.8393\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.8529\n",
      "Epoch 00046: val_accuracy improved from 0.83930 to 0.84422, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4791 - accuracy: 0.8529 - val_loss: 0.5162 - val_accuracy: 0.8442\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4721 - accuracy: 0.8558\n",
      "Epoch 00047: val_accuracy improved from 0.84422 to 0.84767, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.4721 - accuracy: 0.8558 - val_loss: 0.5117 - val_accuracy: 0.8477\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4758 - accuracy: 0.8526\n",
      "Epoch 00048: val_accuracy did not improve from 0.84767\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.4758 - accuracy: 0.8526 - val_loss: 0.5813 - val_accuracy: 0.8212\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4894 - accuracy: 0.8463\n",
      "Epoch 00049: val_accuracy did not improve from 0.84767\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4894 - accuracy: 0.8463 - val_loss: 0.5084 - val_accuracy: 0.8461\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4457 - accuracy: 0.8647\n",
      "Epoch 00050: val_accuracy improved from 0.84767 to 0.85567, saving model to best_model.h5\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.4457 - accuracy: 0.8647 - val_loss: 0.4837 - val_accuracy: 0.8557\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4208 - accuracy: 0.8735\n",
      "Epoch 00051: val_accuracy improved from 0.85567 to 0.86213, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4208 - accuracy: 0.8735 - val_loss: 0.4634 - val_accuracy: 0.8621\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4022 - accuracy: 0.8787\n",
      "Epoch 00052: val_accuracy did not improve from 0.86213\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4022 - accuracy: 0.8787 - val_loss: 0.4692 - val_accuracy: 0.8589\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3953 - accuracy: 0.8800\n",
      "Epoch 00053: val_accuracy improved from 0.86213 to 0.86843, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3953 - accuracy: 0.8800 - val_loss: 0.4467 - val_accuracy: 0.8684\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3963 - accuracy: 0.8798\n",
      "Epoch 00054: val_accuracy did not improve from 0.86843\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3963 - accuracy: 0.8798 - val_loss: 0.4455 - val_accuracy: 0.8681\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3840 - accuracy: 0.8820\n",
      "Epoch 00055: val_accuracy improved from 0.86843 to 0.87430, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3840 - accuracy: 0.8820 - val_loss: 0.4293 - val_accuracy: 0.8743\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.8904\n",
      "Epoch 00056: val_accuracy did not improve from 0.87430\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.3621 - accuracy: 0.8904 - val_loss: 0.4286 - val_accuracy: 0.8741\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3458 - accuracy: 0.8959\n",
      "Epoch 00057: val_accuracy improved from 0.87430 to 0.87838, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3458 - accuracy: 0.8959 - val_loss: 0.4140 - val_accuracy: 0.8784\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3344 - accuracy: 0.9009\n",
      "Epoch 00058: val_accuracy improved from 0.87838 to 0.88230, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.3344 - accuracy: 0.9009 - val_loss: 0.4057 - val_accuracy: 0.8823\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3392 - accuracy: 0.8964\n",
      "Epoch 00059: val_accuracy did not improve from 0.88230\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3392 - accuracy: 0.8964 - val_loss: 0.4249 - val_accuracy: 0.8764\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3300 - accuracy: 0.9005\n",
      "Epoch 00060: val_accuracy improved from 0.88230 to 0.88455, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.3300 - accuracy: 0.9005 - val_loss: 0.3948 - val_accuracy: 0.8845\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.9066\n",
      "Epoch 00061: val_accuracy improved from 0.88455 to 0.88990, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.3101 - accuracy: 0.9066 - val_loss: 0.3828 - val_accuracy: 0.8899\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2937 - accuracy: 0.9122\n",
      "Epoch 00062: val_accuracy improved from 0.88990 to 0.89008, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.2937 - accuracy: 0.9122 - val_loss: 0.3829 - val_accuracy: 0.8901\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2835 - accuracy: 0.9168\n",
      "Epoch 00063: val_accuracy improved from 0.89008 to 0.89988, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.2835 - accuracy: 0.9168 - val_loss: 0.3556 - val_accuracy: 0.8999\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.9175\n",
      "Epoch 00064: val_accuracy did not improve from 0.89988\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.2755 - accuracy: 0.9175 - val_loss: 0.3985 - val_accuracy: 0.8827\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3473 - accuracy: 0.8885\n",
      "Epoch 00065: val_accuracy did not improve from 0.89988\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.3473 - accuracy: 0.8885 - val_loss: 0.4650 - val_accuracy: 0.8578\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3673 - accuracy: 0.8818\n",
      "Epoch 00066: val_accuracy did not improve from 0.89988\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.3673 - accuracy: 0.8818 - val_loss: 0.4147 - val_accuracy: 0.8769\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3624 - accuracy: 0.8840\n",
      "Epoch 00067: val_accuracy did not improve from 0.89988\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.3624 - accuracy: 0.8840 - val_loss: 0.4215 - val_accuracy: 0.8725\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.9017\n",
      "Epoch 00068: val_accuracy did not improve from 0.89988\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.3136 - accuracy: 0.9017 - val_loss: 0.3854 - val_accuracy: 0.8873\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2747 - accuracy: 0.9165\n",
      "Epoch 00069: val_accuracy did not improve from 0.89988\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.2747 - accuracy: 0.9165 - val_loss: 0.3554 - val_accuracy: 0.8986\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9254\n",
      "Epoch 00070: val_accuracy improved from 0.89988 to 0.90062, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.2509 - accuracy: 0.9254 - val_loss: 0.3487 - val_accuracy: 0.9006\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9312\n",
      "Epoch 00071: val_accuracy did not improve from 0.90062\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.2339 - accuracy: 0.9312 - val_loss: 0.3651 - val_accuracy: 0.8957\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9205\n",
      "Epoch 00072: val_accuracy did not improve from 0.90062\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.2569 - accuracy: 0.9205 - val_loss: 0.3722 - val_accuracy: 0.8929\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.9142\n",
      "Epoch 00073: val_accuracy did not improve from 0.90062\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.2731 - accuracy: 0.9142 - val_loss: 0.3677 - val_accuracy: 0.8946\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9290\n",
      "Epoch 00074: val_accuracy improved from 0.90062 to 0.90980, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.2351 - accuracy: 0.9290 - val_loss: 0.3256 - val_accuracy: 0.9098\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 0.9383\n",
      "Epoch 00075: val_accuracy improved from 0.90980 to 0.91313, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.2113 - accuracy: 0.9383 - val_loss: 0.3177 - val_accuracy: 0.9131\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1998 - accuracy: 0.9425\n",
      "Epoch 00076: val_accuracy improved from 0.91313 to 0.91580, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1998 - accuracy: 0.9425 - val_loss: 0.3112 - val_accuracy: 0.9158\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1837 - accuracy: 0.9480\n",
      "Epoch 00077: val_accuracy improved from 0.91580 to 0.91663, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1837 - accuracy: 0.9480 - val_loss: 0.3098 - val_accuracy: 0.9166\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9509\n",
      "Epoch 00078: val_accuracy improved from 0.91663 to 0.91715, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1770 - accuracy: 0.9509 - val_loss: 0.3094 - val_accuracy: 0.9172\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9504\n",
      "Epoch 00079: val_accuracy improved from 0.91715 to 0.91878, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1724 - accuracy: 0.9504 - val_loss: 0.3035 - val_accuracy: 0.9188\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9494\n",
      "Epoch 00080: val_accuracy did not improve from 0.91878\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1757 - accuracy: 0.9494 - val_loss: 0.3085 - val_accuracy: 0.9172\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.9532\n",
      "Epoch 00081: val_accuracy improved from 0.91878 to 0.92238, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1638 - accuracy: 0.9532 - val_loss: 0.2937 - val_accuracy: 0.9224\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9523\n",
      "Epoch 00082: val_accuracy did not improve from 0.92238\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1642 - accuracy: 0.9523 - val_loss: 0.3054 - val_accuracy: 0.9186\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9521\n",
      "Epoch 00083: val_accuracy improved from 0.92238 to 0.92427, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1650 - accuracy: 0.9521 - val_loss: 0.2902 - val_accuracy: 0.9243\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9562\n",
      "Epoch 00084: val_accuracy did not improve from 0.92427\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1551 - accuracy: 0.9562 - val_loss: 0.2992 - val_accuracy: 0.9222\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9561\n",
      "Epoch 00085: val_accuracy improved from 0.92427 to 0.92818, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1557 - accuracy: 0.9561 - val_loss: 0.2836 - val_accuracy: 0.9282\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9618\n",
      "Epoch 00086: val_accuracy improved from 0.92818 to 0.92905, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1390 - accuracy: 0.9618 - val_loss: 0.2791 - val_accuracy: 0.9291\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.9623\n",
      "Epoch 00087: val_accuracy improved from 0.92905 to 0.92927, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1353 - accuracy: 0.9623 - val_loss: 0.2821 - val_accuracy: 0.9293\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9635\n",
      "Epoch 00088: val_accuracy improved from 0.92927 to 0.93292, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1318 - accuracy: 0.9635 - val_loss: 0.2739 - val_accuracy: 0.9329\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9676\n",
      "Epoch 00089: val_accuracy did not improve from 0.93292\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1220 - accuracy: 0.9676 - val_loss: 0.2748 - val_accuracy: 0.9322\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9647\n",
      "Epoch 00090: val_accuracy did not improve from 0.93292\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1252 - accuracy: 0.9647 - val_loss: 0.2785 - val_accuracy: 0.9311\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1137 - accuracy: 0.9701\n",
      "Epoch 00091: val_accuracy improved from 0.93292 to 0.93293, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1137 - accuracy: 0.9701 - val_loss: 0.2770 - val_accuracy: 0.9329\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9678\n",
      "Epoch 00092: val_accuracy did not improve from 0.93293\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1172 - accuracy: 0.9678 - val_loss: 0.2847 - val_accuracy: 0.9289\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9680\n",
      "Epoch 00093: val_accuracy did not improve from 0.93293\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1154 - accuracy: 0.9680 - val_loss: 0.2826 - val_accuracy: 0.9309\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9661\n",
      "Epoch 00094: val_accuracy improved from 0.93293 to 0.93342, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1198 - accuracy: 0.9661 - val_loss: 0.2770 - val_accuracy: 0.9334\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9706\n",
      "Epoch 00095: val_accuracy improved from 0.93342 to 0.93507, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1087 - accuracy: 0.9706 - val_loss: 0.2717 - val_accuracy: 0.9351\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9715\n",
      "Epoch 00096: val_accuracy improved from 0.93507 to 0.93707, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.1081 - accuracy: 0.9715 - val_loss: 0.2693 - val_accuracy: 0.9371\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9754\n",
      "Epoch 00097: val_accuracy improved from 0.93707 to 0.93925, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.0955 - accuracy: 0.9754 - val_loss: 0.2641 - val_accuracy: 0.9392\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9793\n",
      "Epoch 00098: val_accuracy improved from 0.93925 to 0.94103, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.0851 - accuracy: 0.9793 - val_loss: 0.2604 - val_accuracy: 0.9410\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9763\n",
      "Epoch 00099: val_accuracy did not improve from 0.94103\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.0905 - accuracy: 0.9763 - val_loss: 0.2812 - val_accuracy: 0.9342\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9759\n",
      "Epoch 00100: val_accuracy did not improve from 0.94103\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.0901 - accuracy: 0.9759 - val_loss: 0.2722 - val_accuracy: 0.9381\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initializing the model\n",
    "regressorModel = Sequential()\n",
    "# Add one layer to begin with \n",
    "regressorModel.add(Dense(input_dim = 1024, units=2048,activation='relu',kernel_initializer='he_normal'))\n",
    "regressorModel.add(Dense(units=2048,activation='relu',kernel_initializer='he_normal'))\n",
    "regressorModel.add(Dense(units=2048,activation='relu',kernel_initializer='he_normal'))\n",
    "regressorModel.add(Dense(units=10,activation='softmax',kernel_initializer='he_normal'))\n",
    "\n",
    "# Configuration for compiler\n",
    "from tensorflow.keras import optimizers\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "regressorModel.compile(optimizer=adam ,loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# simple early stopping\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1 , patience=25)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# Fit the model\n",
    "historyRegressorModel=regressorModel.fit(NN_X_Train,NN_Y_Train, epochs=100,batch_size=4200,validation_data=(NN_X_Val,NN_Y_Val), callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################  Evaluate Model On Test Data ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979285717010498 0.9410333037376404\n"
     ]
    }
   ],
   "source": [
    "best_train_score = max(historyRegressorModel.history['accuracy'])\n",
    "best_val_score = max(historyRegressorModel.history['val_accuracy'])\n",
    "print(best_train_score,best_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2604 - accuracy: 0.9410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2604422867298126, 0.9410333037376404]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross verify score on Validation data of the best model\n",
    "\n",
    "saved_model.evaluate(np.array(NN_X_Val),np.array(NN_Y_Val))\n",
    "\n",
    "# Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 10s 17ms/step - loss: 0.6800 - accuracy: 0.8471\n"
     ]
    }
   ],
   "source": [
    "# Lets Look at score on Test Data\n",
    "\n",
    "(Test_Loss,Test_acc)=saved_model.evaluate(np.array(NN_X_Test),np.array(NN_Y_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Type</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "      <th>Test_Acc</th>\n",
       "      <th>Hidden layers</th>\n",
       "      <th>Hyper Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.9792857</td>\n",
       "      <td>0.9410333</td>\n",
       "      <td>0.8470556</td>\n",
       "      <td>3 layer(2048 units) with relu activation</td>\n",
       "      <td>optimizer-Adam with lr 0.001,Weights-He</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model_Type  Train_Acc   Val_Acc  Test_Acc  \\\n",
       "0  Neural Network  0.9792857 0.9410333 0.8470556   \n",
       "\n",
       "                              Hidden layers  \\\n",
       "0  3 layer(2048 units) with relu activation   \n",
       "\n",
       "                              Hyper Params  \n",
       "0  optimizer-Adam with lr 0.001,Weights-He  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capturing best results from above epoch\n",
    "ResultsDF=ResultsDF.append({ 'Model_Type' :\"Neural Network\",'Train_Acc':best_train_score,'Val_Acc':best_val_score,'Test_Acc':Test_acc,'Hidden layers':\"3 layer(2048 units) with relu activation\" ,'Hyper Params':'optimizer-Adam with lr 0.001,Weights-He'},ignore_index=True)\n",
    "ResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Blank Space \n",
    "\n",
    "########### Blank Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization\n",
    "- Check for various values of Lambda to look for bet results\n",
    "- Since we are using L2  lets not use Batch normalisation here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 4.9354 - accuracy: 0.1031\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.10015, saving model to best_model.h5\n",
      "10/10 [==============================] - 25s 2s/step - loss: 4.9354 - accuracy: 0.1031 - val_loss: 2.5008 - val_accuracy: 0.1001\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.4717 - accuracy: 0.1118\n",
      "Epoch 00002: val_accuracy improved from 0.10015 to 0.13975, saving model to best_model.h5\n",
      "10/10 [==============================] - 25s 2s/step - loss: 2.4717 - accuracy: 0.1118 - val_loss: 2.4485 - val_accuracy: 0.1398\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.4341 - accuracy: 0.1409\n",
      "Epoch 00003: val_accuracy improved from 0.13975 to 0.15893, saving model to best_model.h5\n",
      "10/10 [==============================] - 25s 2s/step - loss: 2.4341 - accuracy: 0.1409 - val_loss: 2.4089 - val_accuracy: 0.1589\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.3661 - accuracy: 0.2157\n",
      "Epoch 00004: val_accuracy improved from 0.15893 to 0.24768, saving model to best_model.h5\n",
      "10/10 [==============================] - 25s 2s/step - loss: 2.3661 - accuracy: 0.2157 - val_loss: 2.3022 - val_accuracy: 0.2477\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.2243 - accuracy: 0.2720\n",
      "Epoch 00005: val_accuracy improved from 0.24768 to 0.29550, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 2.2243 - accuracy: 0.2720 - val_loss: 2.1226 - val_accuracy: 0.2955\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.0511 - accuracy: 0.3337\n",
      "Epoch 00006: val_accuracy improved from 0.29550 to 0.33553, saving model to best_model.h5\n",
      "10/10 [==============================] - 27s 3s/step - loss: 2.0511 - accuracy: 0.3337 - val_loss: 1.9931 - val_accuracy: 0.3355\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.8871 - accuracy: 0.3946\n",
      "Epoch 00007: val_accuracy improved from 0.33553 to 0.43595, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.8871 - accuracy: 0.3946 - val_loss: 1.7880 - val_accuracy: 0.4360\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.7404 - accuracy: 0.4580\n",
      "Epoch 00008: val_accuracy improved from 0.43595 to 0.47223, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.7404 - accuracy: 0.4580 - val_loss: 1.6985 - val_accuracy: 0.4722\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6507 - accuracy: 0.4909\n",
      "Epoch 00009: val_accuracy improved from 0.47223 to 0.51750, saving model to best_model.h5\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.6507 - accuracy: 0.4909 - val_loss: 1.5730 - val_accuracy: 0.5175\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5518 - accuracy: 0.5305\n",
      "Epoch 00010: val_accuracy improved from 0.51750 to 0.52923, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.5518 - accuracy: 0.5305 - val_loss: 1.5170 - val_accuracy: 0.5292\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4648 - accuracy: 0.5592\n",
      "Epoch 00011: val_accuracy improved from 0.52923 to 0.58002, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.4648 - accuracy: 0.5592 - val_loss: 1.4128 - val_accuracy: 0.5800\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3804 - accuracy: 0.5963\n",
      "Epoch 00012: val_accuracy improved from 0.58002 to 0.61037, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 1.3804 - accuracy: 0.5963 - val_loss: 1.3449 - val_accuracy: 0.6104\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3671 - accuracy: 0.5936\n",
      "Epoch 00013: val_accuracy improved from 0.61037 to 0.62112, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.3671 - accuracy: 0.5936 - val_loss: 1.3146 - val_accuracy: 0.6211\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3103 - accuracy: 0.6223\n",
      "Epoch 00014: val_accuracy improved from 0.62112 to 0.63702, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.3103 - accuracy: 0.6223 - val_loss: 1.2735 - val_accuracy: 0.6370\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2496 - accuracy: 0.6438\n",
      "Epoch 00015: val_accuracy improved from 0.63702 to 0.64625, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.2496 - accuracy: 0.6438 - val_loss: 1.2253 - val_accuracy: 0.6463\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1898 - accuracy: 0.6647\n",
      "Epoch 00016: val_accuracy improved from 0.64625 to 0.68315, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.1898 - accuracy: 0.6647 - val_loss: 1.1477 - val_accuracy: 0.6831\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1391 - accuracy: 0.6818\n",
      "Epoch 00017: val_accuracy improved from 0.68315 to 0.68670, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.1391 - accuracy: 0.6818 - val_loss: 1.1178 - val_accuracy: 0.6867\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1106 - accuracy: 0.6882\n",
      "Epoch 00018: val_accuracy improved from 0.68670 to 0.69347, saving model to best_model.h5\n",
      "10/10 [==============================] - 27s 3s/step - loss: 1.1106 - accuracy: 0.6882 - val_loss: 1.0957 - val_accuracy: 0.6935\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0828 - accuracy: 0.6960\n",
      "Epoch 00019: val_accuracy improved from 0.69347 to 0.71018, saving model to best_model.h5\n",
      "10/10 [==============================] - 27s 3s/step - loss: 1.0828 - accuracy: 0.6960 - val_loss: 1.0543 - val_accuracy: 0.7102\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0679 - accuracy: 0.6992\n",
      "Epoch 00020: val_accuracy did not improve from 0.71018\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.0679 - accuracy: 0.6992 - val_loss: 1.0595 - val_accuracy: 0.7066\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0086 - accuracy: 0.7231\n",
      "Epoch 00021: val_accuracy improved from 0.71018 to 0.72320, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 1.0086 - accuracy: 0.7231 - val_loss: 0.9949 - val_accuracy: 0.7232\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9795 - accuracy: 0.7282\n",
      "Epoch 00022: val_accuracy improved from 0.72320 to 0.73580, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.9795 - accuracy: 0.7282 - val_loss: 0.9647 - val_accuracy: 0.7358\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9453 - accuracy: 0.7410\n",
      "Epoch 00023: val_accuracy improved from 0.73580 to 0.73658, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.9453 - accuracy: 0.7410 - val_loss: 0.9590 - val_accuracy: 0.7366\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9371 - accuracy: 0.7390\n",
      "Epoch 00024: val_accuracy did not improve from 0.73658\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.9371 - accuracy: 0.7390 - val_loss: 0.9654 - val_accuracy: 0.7279\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9112 - accuracy: 0.7478\n",
      "Epoch 00025: val_accuracy did not improve from 0.73658\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.9112 - accuracy: 0.7478 - val_loss: 0.9527 - val_accuracy: 0.7312\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.7523\n",
      "Epoch 00026: val_accuracy improved from 0.73658 to 0.76595, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.9004 - accuracy: 0.7523 - val_loss: 0.8728 - val_accuracy: 0.7660\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8723 - accuracy: 0.7610\n",
      "Epoch 00027: val_accuracy did not improve from 0.76595\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.8723 - accuracy: 0.7610 - val_loss: 0.8982 - val_accuracy: 0.7488\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8462 - accuracy: 0.7709\n",
      "Epoch 00028: val_accuracy improved from 0.76595 to 0.77157, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8462 - accuracy: 0.7709 - val_loss: 0.8459 - val_accuracy: 0.7716\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.7751\n",
      "Epoch 00029: val_accuracy improved from 0.77157 to 0.77323, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8213 - accuracy: 0.7751 - val_loss: 0.8309 - val_accuracy: 0.7732\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8012 - accuracy: 0.7839\n",
      "Epoch 00030: val_accuracy improved from 0.77323 to 0.78225, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8012 - accuracy: 0.7839 - val_loss: 0.8117 - val_accuracy: 0.7822\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7843 - accuracy: 0.7875\n",
      "Epoch 00031: val_accuracy improved from 0.78225 to 0.79088, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7843 - accuracy: 0.7875 - val_loss: 0.7875 - val_accuracy: 0.7909\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7645 - accuracy: 0.7943\n",
      "Epoch 00032: val_accuracy did not improve from 0.79088\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.7645 - accuracy: 0.7943 - val_loss: 0.7820 - val_accuracy: 0.7892\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7650 - accuracy: 0.7919\n",
      "Epoch 00033: val_accuracy did not improve from 0.79088\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7650 - accuracy: 0.7919 - val_loss: 0.7859 - val_accuracy: 0.7855\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7555 - accuracy: 0.7963\n",
      "Epoch 00034: val_accuracy improved from 0.79088 to 0.79488, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.7555 - accuracy: 0.7963 - val_loss: 0.7644 - val_accuracy: 0.7949\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7296 - accuracy: 0.8039\n",
      "Epoch 00035: val_accuracy improved from 0.79488 to 0.80183, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7296 - accuracy: 0.8039 - val_loss: 0.7390 - val_accuracy: 0.8018\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7068 - accuracy: 0.8111\n",
      "Epoch 00036: val_accuracy improved from 0.80183 to 0.80325, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7068 - accuracy: 0.8111 - val_loss: 0.7323 - val_accuracy: 0.8033\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7050 - accuracy: 0.8115\n",
      "Epoch 00037: val_accuracy improved from 0.80325 to 0.80640, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7050 - accuracy: 0.8115 - val_loss: 0.7231 - val_accuracy: 0.8064\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.8183\n",
      "Epoch 00038: val_accuracy improved from 0.80640 to 0.80953, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6838 - accuracy: 0.8183 - val_loss: 0.7087 - val_accuracy: 0.8095\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6710 - accuracy: 0.8219\n",
      "Epoch 00039: val_accuracy did not improve from 0.80953\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6710 - accuracy: 0.8219 - val_loss: 0.7106 - val_accuracy: 0.8086\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6721 - accuracy: 0.8179\n",
      "Epoch 00040: val_accuracy did not improve from 0.80953\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6721 - accuracy: 0.8179 - val_loss: 0.7461 - val_accuracy: 0.7957\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6802 - accuracy: 0.8143\n",
      "Epoch 00041: val_accuracy did not improve from 0.80953\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.6802 - accuracy: 0.8143 - val_loss: 0.7206 - val_accuracy: 0.8048\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6717 - accuracy: 0.8182\n",
      "Epoch 00042: val_accuracy improved from 0.80953 to 0.82565, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.6717 - accuracy: 0.8182 - val_loss: 0.6631 - val_accuracy: 0.8256\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6380 - accuracy: 0.8298\n",
      "Epoch 00043: val_accuracy improved from 0.82565 to 0.82617, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6380 - accuracy: 0.8298 - val_loss: 0.6573 - val_accuracy: 0.8262\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.8378\n",
      "Epoch 00044: val_accuracy did not improve from 0.82617\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6135 - accuracy: 0.8378 - val_loss: 0.6564 - val_accuracy: 0.8242\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.8353\n",
      "Epoch 00045: val_accuracy improved from 0.82617 to 0.82925, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.6194 - accuracy: 0.8353 - val_loss: 0.6416 - val_accuracy: 0.8292\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5940 - accuracy: 0.8426\n",
      "Epoch 00046: val_accuracy improved from 0.82925 to 0.83850, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.5940 - accuracy: 0.8426 - val_loss: 0.6166 - val_accuracy: 0.8385\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5887 - accuracy: 0.8451\n",
      "Epoch 00047: val_accuracy improved from 0.83850 to 0.84510, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.5887 - accuracy: 0.8451 - val_loss: 0.5981 - val_accuracy: 0.8451\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5712 - accuracy: 0.8504\n",
      "Epoch 00048: val_accuracy did not improve from 0.84510\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.5712 - accuracy: 0.8504 - val_loss: 0.5983 - val_accuracy: 0.8426\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5627 - accuracy: 0.8515\n",
      "Epoch 00049: val_accuracy improved from 0.84510 to 0.85242, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.5627 - accuracy: 0.8515 - val_loss: 0.5755 - val_accuracy: 0.8524\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.8603\n",
      "Epoch 00050: val_accuracy improved from 0.85242 to 0.85252, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.5422 - accuracy: 0.8603 - val_loss: 0.5711 - val_accuracy: 0.8525\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5370 - accuracy: 0.8606\n",
      "Epoch 00051: val_accuracy did not improve from 0.85252\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.5370 - accuracy: 0.8606 - val_loss: 0.5840 - val_accuracy: 0.8475\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5417 - accuracy: 0.8593\n",
      "Epoch 00052: val_accuracy did not improve from 0.85252\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.5417 - accuracy: 0.8593 - val_loss: 0.6170 - val_accuracy: 0.8335\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5682 - accuracy: 0.8470\n",
      "Epoch 00053: val_accuracy did not improve from 0.85252\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5682 - accuracy: 0.8470 - val_loss: 0.5724 - val_accuracy: 0.8499\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5407 - accuracy: 0.8568\n",
      "Epoch 00054: val_accuracy improved from 0.85252 to 0.85530, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5407 - accuracy: 0.8568 - val_loss: 0.5621 - val_accuracy: 0.8553\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.8687\n",
      "Epoch 00055: val_accuracy improved from 0.85530 to 0.86233, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.5071 - accuracy: 0.8687 - val_loss: 0.5388 - val_accuracy: 0.8623\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.8731\n",
      "Epoch 00056: val_accuracy did not improve from 0.86233\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.4947 - accuracy: 0.8731 - val_loss: 0.5398 - val_accuracy: 0.8601\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5014 - accuracy: 0.8699\n",
      "Epoch 00057: val_accuracy did not improve from 0.86233\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.5014 - accuracy: 0.8699 - val_loss: 0.5477 - val_accuracy: 0.8595\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4898 - accuracy: 0.8737\n",
      "Epoch 00058: val_accuracy improved from 0.86233 to 0.86450, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.4898 - accuracy: 0.8737 - val_loss: 0.5309 - val_accuracy: 0.8645\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4836 - accuracy: 0.8747\n",
      "Epoch 00059: val_accuracy improved from 0.86450 to 0.86842, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.4836 - accuracy: 0.8747 - val_loss: 0.5222 - val_accuracy: 0.8684\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.8803\n",
      "Epoch 00060: val_accuracy improved from 0.86842 to 0.86862, saving model to best_model.h5\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.4675 - accuracy: 0.8803 - val_loss: 0.5153 - val_accuracy: 0.8686\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4518 - accuracy: 0.8865\n",
      "Epoch 00061: val_accuracy did not improve from 0.86862\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.4518 - accuracy: 0.8865 - val_loss: 0.5152 - val_accuracy: 0.8674\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4478 - accuracy: 0.8874\n",
      "Epoch 00062: val_accuracy did not improve from 0.86862\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.4478 - accuracy: 0.8874 - val_loss: 0.5157 - val_accuracy: 0.8675\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.8894\n",
      "Epoch 00063: val_accuracy improved from 0.86862 to 0.87827, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.4430 - accuracy: 0.8894 - val_loss: 0.4857 - val_accuracy: 0.8783\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4348 - accuracy: 0.8917\n",
      "Epoch 00064: val_accuracy improved from 0.87827 to 0.87880, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.4348 - accuracy: 0.8917 - val_loss: 0.4860 - val_accuracy: 0.8788\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4290 - accuracy: 0.8935\n",
      "Epoch 00065: val_accuracy did not improve from 0.87880\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4290 - accuracy: 0.8935 - val_loss: 0.5183 - val_accuracy: 0.8661\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.8757\n",
      "Epoch 00066: val_accuracy did not improve from 0.87880\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4708 - accuracy: 0.8757 - val_loss: 0.5176 - val_accuracy: 0.8659\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4415 - accuracy: 0.8866\n",
      "Epoch 00067: val_accuracy did not improve from 0.87880\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4415 - accuracy: 0.8866 - val_loss: 0.4848 - val_accuracy: 0.8780\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4373 - accuracy: 0.8870\n",
      "Epoch 00068: val_accuracy did not improve from 0.87880\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.4373 - accuracy: 0.8870 - val_loss: 0.5121 - val_accuracy: 0.8677\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.8502\n",
      "Epoch 00069: val_accuracy did not improve from 0.87880\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5437 - accuracy: 0.8502 - val_loss: 0.6960 - val_accuracy: 0.8028\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.8381\n",
      "Epoch 00070: val_accuracy did not improve from 0.87880\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.5754 - accuracy: 0.8381 - val_loss: 0.5788 - val_accuracy: 0.8431\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4802 - accuracy: 0.8732\n",
      "Epoch 00071: val_accuracy did not improve from 0.87880\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.4802 - accuracy: 0.8732 - val_loss: 0.5043 - val_accuracy: 0.8700\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4308 - accuracy: 0.8889\n",
      "Epoch 00072: val_accuracy improved from 0.87880 to 0.87940, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4308 - accuracy: 0.8889 - val_loss: 0.4758 - val_accuracy: 0.8794\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3963 - accuracy: 0.9006\n",
      "Epoch 00073: val_accuracy improved from 0.87940 to 0.88938, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3963 - accuracy: 0.9006 - val_loss: 0.4467 - val_accuracy: 0.8894\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3800 - accuracy: 0.9076\n",
      "Epoch 00074: val_accuracy improved from 0.88938 to 0.89258, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3800 - accuracy: 0.9076 - val_loss: 0.4405 - val_accuracy: 0.8926\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3680 - accuracy: 0.9116\n",
      "Epoch 00075: val_accuracy improved from 0.89258 to 0.89335, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3680 - accuracy: 0.9116 - val_loss: 0.4360 - val_accuracy: 0.8934\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3751 - accuracy: 0.9069\n",
      "Epoch 00076: val_accuracy did not improve from 0.89335\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3751 - accuracy: 0.9069 - val_loss: 0.4511 - val_accuracy: 0.8866\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.9125\n",
      "Epoch 00077: val_accuracy improved from 0.89335 to 0.89383, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3621 - accuracy: 0.9125 - val_loss: 0.4339 - val_accuracy: 0.8938\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.9159\n",
      "Epoch 00078: val_accuracy improved from 0.89383 to 0.89872, saving model to best_model.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.3523 - accuracy: 0.9159 - val_loss: 0.4213 - val_accuracy: 0.8987\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3499 - accuracy: 0.9173\n",
      "Epoch 00079: val_accuracy did not improve from 0.89872\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3499 - accuracy: 0.9173 - val_loss: 0.4311 - val_accuracy: 0.8955\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3449 - accuracy: 0.9190\n",
      "Epoch 00080: val_accuracy did not improve from 0.89872\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3449 - accuracy: 0.9190 - val_loss: 0.4212 - val_accuracy: 0.8975\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3428 - accuracy: 0.9179\n",
      "Epoch 00081: val_accuracy improved from 0.89872 to 0.90432, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3428 - accuracy: 0.9179 - val_loss: 0.4078 - val_accuracy: 0.9043\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.9225\n",
      "Epoch 00082: val_accuracy did not improve from 0.90432\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3330 - accuracy: 0.9225 - val_loss: 0.4291 - val_accuracy: 0.8945\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3373 - accuracy: 0.9193\n",
      "Epoch 00083: val_accuracy did not improve from 0.90432\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3373 - accuracy: 0.9193 - val_loss: 0.4133 - val_accuracy: 0.9003\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3242 - accuracy: 0.9247\n",
      "Epoch 00084: val_accuracy did not improve from 0.90432\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3242 - accuracy: 0.9247 - val_loss: 0.4059 - val_accuracy: 0.9038\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3285 - accuracy: 0.9225\n",
      "Epoch 00085: val_accuracy did not improve from 0.90432\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3285 - accuracy: 0.9225 - val_loss: 0.4214 - val_accuracy: 0.8957\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3317 - accuracy: 0.9193\n",
      "Epoch 00086: val_accuracy improved from 0.90432 to 0.90580, saving model to best_model.h5\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3317 - accuracy: 0.9193 - val_loss: 0.4004 - val_accuracy: 0.9058\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.9247\n",
      "Epoch 00087: val_accuracy did not improve from 0.90580\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3230 - accuracy: 0.9247 - val_loss: 0.4222 - val_accuracy: 0.8964\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.9214\n",
      "Epoch 00088: val_accuracy did not improve from 0.90580\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3268 - accuracy: 0.9214 - val_loss: 0.4137 - val_accuracy: 0.8990\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.9235\n",
      "Epoch 00089: val_accuracy improved from 0.90580 to 0.90668, saving model to best_model.h5\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.3228 - accuracy: 0.9235 - val_loss: 0.3951 - val_accuracy: 0.9067\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3137 - accuracy: 0.9263\n",
      "Epoch 00090: val_accuracy did not improve from 0.90668\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3137 - accuracy: 0.9263 - val_loss: 0.3972 - val_accuracy: 0.9061\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.9308\n",
      "Epoch 00091: val_accuracy improved from 0.90668 to 0.90727, saving model to best_model.h5\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.2999 - accuracy: 0.9308 - val_loss: 0.3922 - val_accuracy: 0.9073\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2970 - accuracy: 0.9322\n",
      "Epoch 00092: val_accuracy did not improve from 0.90727\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2970 - accuracy: 0.9322 - val_loss: 0.3900 - val_accuracy: 0.9072\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.9321\n",
      "Epoch 00093: val_accuracy did not improve from 0.90727\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.2926 - accuracy: 0.9321 - val_loss: 0.4053 - val_accuracy: 0.9024\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3032 - accuracy: 0.9286\n",
      "Epoch 00094: val_accuracy improved from 0.90727 to 0.90897, saving model to best_model.h5\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.3032 - accuracy: 0.9286 - val_loss: 0.3854 - val_accuracy: 0.9090\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.9339\n",
      "Epoch 00095: val_accuracy improved from 0.90897 to 0.91148, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2873 - accuracy: 0.9339 - val_loss: 0.3800 - val_accuracy: 0.9115\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.9377\n",
      "Epoch 00096: val_accuracy did not improve from 0.91148\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2797 - accuracy: 0.9377 - val_loss: 0.3819 - val_accuracy: 0.9102\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2758 - accuracy: 0.9384\n",
      "Epoch 00097: val_accuracy improved from 0.91148 to 0.91477, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.2758 - accuracy: 0.9384 - val_loss: 0.3721 - val_accuracy: 0.9148\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9445\n",
      "Epoch 00098: val_accuracy improved from 0.91477 to 0.91610, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2640 - accuracy: 0.9445 - val_loss: 0.3666 - val_accuracy: 0.9161\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.9415\n",
      "Epoch 00099: val_accuracy did not improve from 0.91610\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.2685 - accuracy: 0.9415 - val_loss: 0.4003 - val_accuracy: 0.9043\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.9398\n",
      "Epoch 00100: val_accuracy did not improve from 0.91610\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.2701 - accuracy: 0.9398 - val_loss: 0.3766 - val_accuracy: 0.9126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initializing the model\n",
    "regressorModel = Sequential()\n",
    "# Add one layer to begin with \n",
    "regressorModel.add(Dense(input_dim = 1024, units=2048,activation='relu',kernel_initializer='he_normal'))\n",
    "regressorModel.add(Dense(units=2048,activation='relu',kernel_initializer='he_normal'))\n",
    "regressorModel.add(Dense(units=2048,activation='relu',kernel_initializer='he_normal'))\n",
    "regressorModel.add(Dense(units=10,activation='softmax',kernel_initializer='he_normal',kernel_regularizer='l2'))\n",
    "\n",
    "# Configuration for compiler\n",
    "from tensorflow.keras import optimizers\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "regressorModel.compile(optimizer=adam ,loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# simple early stopping\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1 , patience=25)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# Fit the model\n",
    "historyRegressorModel=regressorModel.fit(NN_X_Train,NN_Y_Train, epochs=100,batch_size=4200,validation_data=(NN_X_Val,NN_Y_Val), callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################  Evaluate Model On Test Data ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444761872291565 0.916100025177002\n"
     ]
    }
   ],
   "source": [
    "best_train_score = max(historyRegressorModel.history['accuracy'])\n",
    "best_val_score = max(historyRegressorModel.history['val_accuracy'])\n",
    "print(best_train_score,best_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.3666 - accuracy: 0.9161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36664631962776184, 0.916100025177002]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross verify score on Validation data of the best model\n",
    "\n",
    "saved_model.evaluate(np.array(NN_X_Val),np.array(NN_Y_Val))\n",
    "\n",
    "# Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 11s 19ms/step - loss: 0.6168 - accuracy: 0.8511\n"
     ]
    }
   ],
   "source": [
    "# Lets Look at score on Test Data\n",
    "\n",
    "(Test_Loss,Test_acc)=saved_model.evaluate(np.array(NN_X_Test),np.array(NN_Y_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Type</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "      <th>Test_Acc</th>\n",
       "      <th>Hidden layers</th>\n",
       "      <th>Hyper Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.9792857</td>\n",
       "      <td>0.9410333</td>\n",
       "      <td>0.8470556</td>\n",
       "      <td>3 layer(2048 units) with relu activation</td>\n",
       "      <td>optimizer-Adam with lr 0.001,Weights-He</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.9444762</td>\n",
       "      <td>0.9161000</td>\n",
       "      <td>0.8511111</td>\n",
       "      <td>3 layer(2048 units) with relu activation</td>\n",
       "      <td>optimizer-Adam with lr 0.001,Weights-He,L2 Reg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model_Type  Train_Acc   Val_Acc  Test_Acc  \\\n",
       "0  Neural Network  0.9792857 0.9410333 0.8470556   \n",
       "1  Neural Network  0.9444762 0.9161000 0.8511111   \n",
       "\n",
       "                              Hidden layers  \\\n",
       "0  3 layer(2048 units) with relu activation   \n",
       "1  3 layer(2048 units) with relu activation   \n",
       "\n",
       "                                        Hyper Params  \n",
       "0            optimizer-Adam with lr 0.001,Weights-He  \n",
       "1  optimizer-Adam with lr 0.001,Weights-He,L2 Reg...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capturing best results from above epoch\n",
    "ResultsDF=ResultsDF.append({ 'Model_Type' :\"Neural Network\",'Train_Acc':best_train_score,'Val_Acc':best_val_score,'Test_Acc':Test_acc,'Hidden layers':\"3 layer(2048 units) with relu activation\" ,'Hyper Params':'optimizer-Adam with lr 0.001,Weights-He,L2 Regularization'},ignore_index=True)\n",
    "ResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not much improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Blank Space\n",
    "\n",
    "\n",
    "#### Blank Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalisation\n",
    "- Lets add Batch normalisation after every linearity \n",
    "- Everything else remains same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.6444 - accuracy: 0.3254\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.10532, saving model to best_model.h5\n",
      "10/10 [==============================] - 42s 4s/step - loss: 2.6444 - accuracy: 0.3254 - val_loss: 4.7126 - val_accuracy: 0.1053\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2425 - accuracy: 0.6060\n",
      "Epoch 00002: val_accuracy improved from 0.10532 to 0.18565, saving model to best_model.h5\n",
      "10/10 [==============================] - 39s 4s/step - loss: 1.2425 - accuracy: 0.6060 - val_loss: 2.9000 - val_accuracy: 0.1857\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8995 - accuracy: 0.7214\n",
      "Epoch 00003: val_accuracy did not improve from 0.18565\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.8995 - accuracy: 0.7214 - val_loss: 3.0742 - val_accuracy: 0.1519\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7206 - accuracy: 0.7762\n",
      "Epoch 00004: val_accuracy improved from 0.18565 to 0.27482, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.7206 - accuracy: 0.7762 - val_loss: 2.3731 - val_accuracy: 0.2748\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6063 - accuracy: 0.8130\n",
      "Epoch 00005: val_accuracy improved from 0.27482 to 0.39270, saving model to best_model.h5\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.6063 - accuracy: 0.8130 - val_loss: 1.9653 - val_accuracy: 0.3927\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.8356\n",
      "Epoch 00006: val_accuracy improved from 0.39270 to 0.41437, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.5297 - accuracy: 0.8356 - val_loss: 1.8575 - val_accuracy: 0.4144\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4339 - accuracy: 0.8704\n",
      "Epoch 00007: val_accuracy improved from 0.41437 to 0.44515, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.4339 - accuracy: 0.8704 - val_loss: 1.6770 - val_accuracy: 0.4451\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.8710\n",
      "Epoch 00008: val_accuracy improved from 0.44515 to 0.49888, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.4245 - accuracy: 0.8710 - val_loss: 1.5069 - val_accuracy: 0.4989\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.9034\n",
      "Epoch 00009: val_accuracy improved from 0.49888 to 0.58023, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.3323 - accuracy: 0.9034 - val_loss: 1.3007 - val_accuracy: 0.5802\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3574 - accuracy: 0.8946\n",
      "Epoch 00010: val_accuracy did not improve from 0.58023\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.3574 - accuracy: 0.8946 - val_loss: 1.6161 - val_accuracy: 0.4648\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.9088\n",
      "Epoch 00011: val_accuracy improved from 0.58023 to 0.68907, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.3072 - accuracy: 0.9088 - val_loss: 1.0933 - val_accuracy: 0.6891\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2388 - accuracy: 0.9343\n",
      "Epoch 00012: val_accuracy did not improve from 0.68907\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.2388 - accuracy: 0.9343 - val_loss: 1.3676 - val_accuracy: 0.5183\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.9200\n",
      "Epoch 00013: val_accuracy improved from 0.68907 to 0.71980, saving model to best_model.h5\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.2754 - accuracy: 0.9200 - val_loss: 0.9378 - val_accuracy: 0.7198\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.9357\n",
      "Epoch 00014: val_accuracy did not improve from 0.71980\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.2298 - accuracy: 0.9357 - val_loss: 0.9586 - val_accuracy: 0.6749\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9524\n",
      "Epoch 00015: val_accuracy did not improve from 0.71980\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.1790 - accuracy: 0.9524 - val_loss: 0.8603 - val_accuracy: 0.7159\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1584 - accuracy: 0.9589\n",
      "Epoch 00016: val_accuracy did not improve from 0.71980\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.1584 - accuracy: 0.9589 - val_loss: 2.3678 - val_accuracy: 0.4456\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 0.9370\n",
      "Epoch 00017: val_accuracy did not improve from 0.71980\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.2256 - accuracy: 0.9370 - val_loss: 0.9167 - val_accuracy: 0.6926\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9575\n",
      "Epoch 00018: val_accuracy improved from 0.71980 to 0.74342, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.1606 - accuracy: 0.9575 - val_loss: 0.8009 - val_accuracy: 0.7434\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9714\n",
      "Epoch 00019: val_accuracy improved from 0.74342 to 0.80113, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.1201 - accuracy: 0.9714 - val_loss: 0.6340 - val_accuracy: 0.8011\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9789\n",
      "Epoch 00020: val_accuracy improved from 0.80113 to 0.81818, saving model to best_model.h5\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.0962 - accuracy: 0.9789 - val_loss: 0.5867 - val_accuracy: 0.8182\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9775\n",
      "Epoch 00021: val_accuracy did not improve from 0.81818\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0926 - accuracy: 0.9775 - val_loss: 0.6550 - val_accuracy: 0.7763\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9831\n",
      "Epoch 00022: val_accuracy improved from 0.81818 to 0.84063, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0773 - accuracy: 0.9831 - val_loss: 0.5321 - val_accuracy: 0.8406\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9881\n",
      "Epoch 00023: val_accuracy improved from 0.84063 to 0.86367, saving model to best_model.h5\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.0629 - accuracy: 0.9881 - val_loss: 0.4528 - val_accuracy: 0.8637\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9879\n",
      "Epoch 00024: val_accuracy did not improve from 0.86367\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.0583 - accuracy: 0.9879 - val_loss: 1.0880 - val_accuracy: 0.6954\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9866\n",
      "Epoch 00025: val_accuracy did not improve from 0.86367\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.0608 - accuracy: 0.9866 - val_loss: 0.5008 - val_accuracy: 0.8389\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9915\n",
      "Epoch 00026: val_accuracy did not improve from 0.86367\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.0439 - accuracy: 0.9915 - val_loss: 0.4988 - val_accuracy: 0.8374\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9928\n",
      "Epoch 00027: val_accuracy improved from 0.86367 to 0.88112, saving model to best_model.h5\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0384 - accuracy: 0.9928 - val_loss: 0.4004 - val_accuracy: 0.8811\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9957\n",
      "Epoch 00028: val_accuracy did not improve from 0.88112\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0292 - accuracy: 0.9957 - val_loss: 0.4323 - val_accuracy: 0.8671\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9959\n",
      "Epoch 00029: val_accuracy did not improve from 0.88112\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0273 - accuracy: 0.9959 - val_loss: 0.4545 - val_accuracy: 0.8670\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9955\n",
      "Epoch 00030: val_accuracy improved from 0.88112 to 0.88480, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0266 - accuracy: 0.9955 - val_loss: 0.4001 - val_accuracy: 0.8848\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9975\n",
      "Epoch 00031: val_accuracy did not improve from 0.88480\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0205 - accuracy: 0.9975 - val_loss: 0.4158 - val_accuracy: 0.8785\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9975\n",
      "Epoch 00032: val_accuracy did not improve from 0.88480\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0187 - accuracy: 0.9975 - val_loss: 0.5827 - val_accuracy: 0.8354\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9985\n",
      "Epoch 00033: val_accuracy improved from 0.88480 to 0.89642, saving model to best_model.h5\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0151 - accuracy: 0.9985 - val_loss: 0.3614 - val_accuracy: 0.8964\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9986\n",
      "Epoch 00034: val_accuracy improved from 0.89642 to 0.90515, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0129 - accuracy: 0.9986 - val_loss: 0.3340 - val_accuracy: 0.9051\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9978\n",
      "Epoch 00035: val_accuracy did not improve from 0.90515\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0148 - accuracy: 0.9978 - val_loss: 0.3691 - val_accuracy: 0.8960\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9990\n",
      "Epoch 00036: val_accuracy improved from 0.90515 to 0.90882, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0109 - accuracy: 0.9990 - val_loss: 0.3372 - val_accuracy: 0.9088\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9993\n",
      "Epoch 00037: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.0091 - accuracy: 0.9993 - val_loss: 0.3430 - val_accuracy: 0.9073\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9986\n",
      "Epoch 00038: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0106 - accuracy: 0.9986 - val_loss: 0.4779 - val_accuracy: 0.8695\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9940\n",
      "Epoch 00039: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0249 - accuracy: 0.9940 - val_loss: 4.4812 - val_accuracy: 0.5679\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4506 - accuracy: 0.6870\n",
      "Epoch 00040: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 1.4506 - accuracy: 0.6870 - val_loss: 60.7488 - val_accuracy: 0.1261\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8355 - accuracy: 0.7389\n",
      "Epoch 00041: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.8355 - accuracy: 0.7389 - val_loss: 21.8281 - val_accuracy: 0.1554\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.8208\n",
      "Epoch 00042: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.5716 - accuracy: 0.8208 - val_loss: 14.7117 - val_accuracy: 0.2319\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.8633\n",
      "Epoch 00043: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.4407 - accuracy: 0.8633 - val_loss: 9.5353 - val_accuracy: 0.2488\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3551 - accuracy: 0.8908\n",
      "Epoch 00044: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.3551 - accuracy: 0.8908 - val_loss: 8.2228 - val_accuracy: 0.2988\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3030 - accuracy: 0.9065\n",
      "Epoch 00045: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.3030 - accuracy: 0.9065 - val_loss: 7.5049 - val_accuracy: 0.2612\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2357 - accuracy: 0.9279\n",
      "Epoch 00046: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.2357 - accuracy: 0.9279 - val_loss: 5.1681 - val_accuracy: 0.3704\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.9408\n",
      "Epoch 00047: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.1968 - accuracy: 0.9408 - val_loss: 5.0486 - val_accuracy: 0.3367\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.9339\n",
      "Epoch 00048: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.2216 - accuracy: 0.9339 - val_loss: 3.5635 - val_accuracy: 0.4062\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1999 - accuracy: 0.9395\n",
      "Epoch 00049: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.1999 - accuracy: 0.9395 - val_loss: 4.1770 - val_accuracy: 0.4005\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9525\n",
      "Epoch 00050: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.1620 - accuracy: 0.9525 - val_loss: 3.0249 - val_accuracy: 0.4660\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9642\n",
      "Epoch 00051: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.1255 - accuracy: 0.9642 - val_loss: 2.1999 - val_accuracy: 0.5372\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9634\n",
      "Epoch 00052: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.1273 - accuracy: 0.9634 - val_loss: 1.8889 - val_accuracy: 0.5917\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9708\n",
      "Epoch 00053: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.1046 - accuracy: 0.9708 - val_loss: 2.2752 - val_accuracy: 0.5558\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9722\n",
      "Epoch 00054: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0980 - accuracy: 0.9722 - val_loss: 1.9675 - val_accuracy: 0.6023\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9752\n",
      "Epoch 00055: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0850 - accuracy: 0.9752 - val_loss: 1.7779 - val_accuracy: 0.6195\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9780\n",
      "Epoch 00056: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.0797 - accuracy: 0.9780 - val_loss: 2.0043 - val_accuracy: 0.6058\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9819\n",
      "Epoch 00057: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.0689 - accuracy: 0.9819 - val_loss: 1.3229 - val_accuracy: 0.6793\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9870\n",
      "Epoch 00058: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0538 - accuracy: 0.9870 - val_loss: 1.3962 - val_accuracy: 0.6718\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9913\n",
      "Epoch 00059: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0409 - accuracy: 0.9913 - val_loss: 1.1633 - val_accuracy: 0.7240\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9923\n",
      "Epoch 00060: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0357 - accuracy: 0.9923 - val_loss: 1.0711 - val_accuracy: 0.7349\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9913\n",
      "Epoch 00061: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0353 - accuracy: 0.9913 - val_loss: 1.3936 - val_accuracy: 0.7035\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9884\n",
      "Epoch 00062: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0437 - accuracy: 0.9884 - val_loss: 1.1307 - val_accuracy: 0.7397\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9913\n",
      "Epoch 00063: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.0356 - accuracy: 0.9913 - val_loss: 1.1471 - val_accuracy: 0.7416\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9940\n",
      "Epoch 00064: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.0287 - accuracy: 0.9940 - val_loss: 0.8843 - val_accuracy: 0.7754\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9928\n",
      "Epoch 00065: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0293 - accuracy: 0.9928 - val_loss: 1.4836 - val_accuracy: 0.7351\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9921\n",
      "Epoch 00066: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0318 - accuracy: 0.9921 - val_loss: 0.7757 - val_accuracy: 0.8054\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9935\n",
      "Epoch 00067: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0295 - accuracy: 0.9935 - val_loss: 0.7985 - val_accuracy: 0.8017\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9958\n",
      "Epoch 00068: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0209 - accuracy: 0.9958 - val_loss: 0.8115 - val_accuracy: 0.8058\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9875\n",
      "Epoch 00069: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0485 - accuracy: 0.9875 - val_loss: 6.3307 - val_accuracy: 0.4227\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9120\n",
      "Epoch 00070: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.2849 - accuracy: 0.9120 - val_loss: 4.1562 - val_accuracy: 0.4569\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.9472\n",
      "Epoch 00071: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.1638 - accuracy: 0.9472 - val_loss: 1.4083 - val_accuracy: 0.6472\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9697\n",
      "Epoch 00072: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.1042 - accuracy: 0.9697 - val_loss: 1.1122 - val_accuracy: 0.7013\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9794\n",
      "Epoch 00073: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0748 - accuracy: 0.9794 - val_loss: 1.4401 - val_accuracy: 0.6760\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9823\n",
      "Epoch 00074: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0639 - accuracy: 0.9823 - val_loss: 1.0926 - val_accuracy: 0.7255\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9895\n",
      "Epoch 00075: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0432 - accuracy: 0.9895 - val_loss: 0.8353 - val_accuracy: 0.7947\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9939\n",
      "Epoch 00076: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0285 - accuracy: 0.9939 - val_loss: 0.7108 - val_accuracy: 0.8161\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9958\n",
      "Epoch 00077: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0216 - accuracy: 0.9958 - val_loss: 0.4990 - val_accuracy: 0.8595\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9963\n",
      "Epoch 00078: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0191 - accuracy: 0.9963 - val_loss: 1.2669 - val_accuracy: 0.7521\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9950\n",
      "Epoch 00079: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0215 - accuracy: 0.9950 - val_loss: 0.8991 - val_accuracy: 0.8058\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9958\n",
      "Epoch 00080: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0199 - accuracy: 0.9958 - val_loss: 0.5980 - val_accuracy: 0.8497\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9969\n",
      "Epoch 00081: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0150 - accuracy: 0.9969 - val_loss: 0.6774 - val_accuracy: 0.8469\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9975\n",
      "Epoch 00082: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0132 - accuracy: 0.9975 - val_loss: 0.4053 - val_accuracy: 0.8995\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9980\n",
      "Epoch 00083: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0106 - accuracy: 0.9980 - val_loss: 0.5484 - val_accuracy: 0.8688\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9992\n",
      "Epoch 00084: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0071 - accuracy: 0.9992 - val_loss: 0.4710 - val_accuracy: 0.8850\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9991\n",
      "Epoch 00085: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0072 - accuracy: 0.9991 - val_loss: 0.4901 - val_accuracy: 0.8886\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9987\n",
      "Epoch 00086: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0084 - accuracy: 0.9987 - val_loss: 1.0793 - val_accuracy: 0.8039\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9955\n",
      "Epoch 00087: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 0.6749 - val_accuracy: 0.8425\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9955\n",
      "Epoch 00088: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0182 - accuracy: 0.9955 - val_loss: 0.6404 - val_accuracy: 0.8573\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9976\n",
      "Epoch 00089: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0126 - accuracy: 0.9976 - val_loss: 0.5912 - val_accuracy: 0.8720\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9982\n",
      "Epoch 00090: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0102 - accuracy: 0.9982 - val_loss: 0.6589 - val_accuracy: 0.8608\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9975\n",
      "Epoch 00091: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0110 - accuracy: 0.9975 - val_loss: 0.4994 - val_accuracy: 0.8858\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9974\n",
      "Epoch 00092: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.0121 - accuracy: 0.9974 - val_loss: 0.8361 - val_accuracy: 0.8350\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9985\n",
      "Epoch 00093: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.4865 - val_accuracy: 0.8884\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9988\n",
      "Epoch 00094: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0071 - accuracy: 0.9988 - val_loss: 0.6528 - val_accuracy: 0.8628\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9989\n",
      "Epoch 00095: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0062 - accuracy: 0.9989 - val_loss: 0.4880 - val_accuracy: 0.8931\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9991\n",
      "Epoch 00096: val_accuracy did not improve from 0.90882\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0052 - accuracy: 0.9991 - val_loss: 0.4345 - val_accuracy: 0.9033\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9998\n",
      "Epoch 00097: val_accuracy improved from 0.90882 to 0.91128, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 0.4034 - val_accuracy: 0.9113\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9997\n",
      "Epoch 00098: val_accuracy improved from 0.91128 to 0.92960, saving model to best_model.h5\n",
      "10/10 [==============================] - 39s 4s/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.3346 - val_accuracy: 0.9296\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9999\n",
      "Epoch 00099: val_accuracy did not improve from 0.92960\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.4870 - val_accuracy: 0.8983\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 00100: val_accuracy improved from 0.92960 to 0.93558, saving model to best_model.h5\n",
      "10/10 [==============================] - 40s 4s/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.3274 - val_accuracy: 0.9356\n"
     ]
    }
   ],
   "source": [
    "# Configuration for architecture\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "\n",
    "# Initializing the model\n",
    "regressorModel = Sequential()\n",
    "# Add one layer to begin with \n",
    "regressorModel.add(Dense(input_dim = 1024, units=2048,kernel_initializer='he_normal'))\n",
    "regressorModel.add(BatchNormalization())  \n",
    "regressorModel.add(Activation('relu'))  \n",
    "regressorModel.add(Dense(units=2048,kernel_initializer='he_normal'))\n",
    "regressorModel.add(BatchNormalization())  \n",
    "regressorModel.add(Activation('relu'))  \n",
    "regressorModel.add(Dense(units=2048,kernel_initializer='he_normal'))\n",
    "regressorModel.add(BatchNormalization())  \n",
    "regressorModel.add(Activation('relu'))  \n",
    "regressorModel.add(Dense(units=10,activation='softmax',kernel_initializer='he_normal'))\n",
    "\n",
    "# Configuration for compiler\n",
    "from tensorflow.keras import optimizers\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "regressorModel.compile(optimizer=adam ,loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# simple early stopping\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1 , patience=25)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# Fit the model\n",
    "historyRegressorModel=regressorModel.fit(NN_X_Train,NN_Y_Train, epochs=100,batch_size=4200,validation_data=(NN_X_Val,NN_Y_Val), callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################  Evaluate Model On Test Data ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998809695243835 0.9355833530426025\n"
     ]
    }
   ],
   "source": [
    "best_train_score = max(historyRegressorModel.history['accuracy'])\n",
    "best_val_score = max(historyRegressorModel.history['val_accuracy'])\n",
    "print(best_train_score,best_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.3274 - accuracy: 0.9356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32739704847335815, 0.9355833530426025]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross verify score on Validation data of the best model\n",
    "\n",
    "saved_model.evaluate(np.array(NN_X_Val),np.array(NN_Y_Val))\n",
    "\n",
    "# Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 11s 19ms/step - loss: 0.9355 - accuracy: 0.8363\n"
     ]
    }
   ],
   "source": [
    "# Lets Look at score on Test Data\n",
    "\n",
    "(Test_Loss,Test_acc)=saved_model.evaluate(np.array(NN_X_Test),np.array(NN_Y_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Type</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "      <th>Test_Acc</th>\n",
       "      <th>Hidden layers</th>\n",
       "      <th>Hyper Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.9792857</td>\n",
       "      <td>0.9410333</td>\n",
       "      <td>0.8470556</td>\n",
       "      <td>3 layer(2048 units) with relu activation</td>\n",
       "      <td>optimizer-Adam with lr 0.001,Weights-He</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.9444762</td>\n",
       "      <td>0.9161000</td>\n",
       "      <td>0.8511111</td>\n",
       "      <td>3 layer(2048 units) with relu activation</td>\n",
       "      <td>optimizer-Adam with lr 0.001,Weights-He,L2 Reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.9998810</td>\n",
       "      <td>0.9355834</td>\n",
       "      <td>0.8362778</td>\n",
       "      <td>3 layer(2048 units) with relu activation</td>\n",
       "      <td>optimizer-Adam with lr 0.001,Weights-He,Batch ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model_Type  Train_Acc   Val_Acc  Test_Acc  \\\n",
       "0  Neural Network  0.9792857 0.9410333 0.8470556   \n",
       "1  Neural Network  0.9444762 0.9161000 0.8511111   \n",
       "2  Neural Network  0.9998810 0.9355834 0.8362778   \n",
       "\n",
       "                              Hidden layers  \\\n",
       "0  3 layer(2048 units) with relu activation   \n",
       "1  3 layer(2048 units) with relu activation   \n",
       "2  3 layer(2048 units) with relu activation   \n",
       "\n",
       "                                        Hyper Params  \n",
       "0            optimizer-Adam with lr 0.001,Weights-He  \n",
       "1  optimizer-Adam with lr 0.001,Weights-He,L2 Reg...  \n",
       "2  optimizer-Adam with lr 0.001,Weights-He,Batch ...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capturing best results from above epoch\n",
    "ResultsDF=ResultsDF.append({ 'Model_Type' :\"Neural Network\",'Train_Acc':best_train_score,'Val_Acc':best_val_score,'Test_Acc':Test_acc,'Hidden layers':\"3 layer(2048 units) with relu activation\" ,'Hyper Params':'optimizer-Adam with lr 0.001,Weights-He,Batch Normalisation'},ignore_index=True)\n",
    "ResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now in the overfit zone , lets try to regularize using L2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Blank Space\n",
    "\n",
    "\n",
    "#### Blank Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Out\n",
    "- Lets add Drop out for regulraization\n",
    "- Batch normalisation will be removed\n",
    "- Everything else remains same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 4.2460 - accuracy: 0.1000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.10000, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 4.2460 - accuracy: 0.1000 - val_loss: 2.3070 - val_accuracy: 0.1000\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.3138 - accuracy: 0.1009\n",
      "Epoch 00002: val_accuracy improved from 0.10000 to 0.11217, saving model to best_model.h5\n",
      "10/10 [==============================] - 31s 3s/step - loss: 2.3138 - accuracy: 0.1009 - val_loss: 2.3023 - val_accuracy: 0.1122\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.3044 - accuracy: 0.1059\n",
      "Epoch 00003: val_accuracy did not improve from 0.11217\n",
      "10/10 [==============================] - 31s 3s/step - loss: 2.3044 - accuracy: 0.1059 - val_loss: 2.3000 - val_accuracy: 0.1004\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.2990 - accuracy: 0.1161\n",
      "Epoch 00004: val_accuracy improved from 0.11217 to 0.15162, saving model to best_model.h5\n",
      "10/10 [==============================] - 33s 3s/step - loss: 2.2990 - accuracy: 0.1161 - val_loss: 2.2910 - val_accuracy: 0.1516\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.2863 - accuracy: 0.1403\n",
      "Epoch 00005: val_accuracy improved from 0.15162 to 0.22965, saving model to best_model.h5\n",
      "10/10 [==============================] - 35s 4s/step - loss: 2.2863 - accuracy: 0.1403 - val_loss: 2.2473 - val_accuracy: 0.2297\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.2286 - accuracy: 0.1831\n",
      "Epoch 00006: val_accuracy did not improve from 0.22965\n",
      "10/10 [==============================] - 35s 3s/step - loss: 2.2286 - accuracy: 0.1831 - val_loss: 2.1661 - val_accuracy: 0.2142\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.1042 - accuracy: 0.2411\n",
      "Epoch 00007: val_accuracy improved from 0.22965 to 0.33985, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 2.1042 - accuracy: 0.2411 - val_loss: 1.9542 - val_accuracy: 0.3399\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.9065 - accuracy: 0.3303\n",
      "Epoch 00008: val_accuracy improved from 0.33985 to 0.42492, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 1.9065 - accuracy: 0.3303 - val_loss: 1.7250 - val_accuracy: 0.4249\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.7620 - accuracy: 0.3877\n",
      "Epoch 00009: val_accuracy improved from 0.42492 to 0.47808, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 1.7620 - accuracy: 0.3877 - val_loss: 1.5798 - val_accuracy: 0.4781\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5784 - accuracy: 0.4666\n",
      "Epoch 00010: val_accuracy improved from 0.47808 to 0.57275, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 1.5784 - accuracy: 0.4666 - val_loss: 1.3711 - val_accuracy: 0.5727\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4771 - accuracy: 0.5076\n",
      "Epoch 00011: val_accuracy improved from 0.57275 to 0.60303, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 1.4771 - accuracy: 0.5076 - val_loss: 1.2812 - val_accuracy: 0.6030\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3937 - accuracy: 0.5390\n",
      "Epoch 00012: val_accuracy improved from 0.60303 to 0.61347, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 1.3937 - accuracy: 0.5390 - val_loss: 1.2202 - val_accuracy: 0.6135\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2868 - accuracy: 0.5809\n",
      "Epoch 00013: val_accuracy improved from 0.61347 to 0.63055, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 1.2868 - accuracy: 0.5809 - val_loss: 1.1514 - val_accuracy: 0.6306\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2021 - accuracy: 0.6151\n",
      "Epoch 00014: val_accuracy improved from 0.63055 to 0.66567, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 1.2021 - accuracy: 0.6151 - val_loss: 1.0714 - val_accuracy: 0.6657\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1229 - accuracy: 0.6446\n",
      "Epoch 00015: val_accuracy improved from 0.66567 to 0.67922, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 1.1229 - accuracy: 0.6446 - val_loss: 1.0270 - val_accuracy: 0.6792\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1471 - accuracy: 0.6330\n",
      "Epoch 00016: val_accuracy did not improve from 0.67922\n",
      "10/10 [==============================] - 30s 3s/step - loss: 1.1471 - accuracy: 0.6330 - val_loss: 1.0318 - val_accuracy: 0.6742\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0695 - accuracy: 0.6580\n",
      "Epoch 00017: val_accuracy improved from 0.67922 to 0.70755, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 1.0695 - accuracy: 0.6580 - val_loss: 0.9353 - val_accuracy: 0.7075\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0243 - accuracy: 0.6728\n",
      "Epoch 00018: val_accuracy improved from 0.70755 to 0.72387, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 1.0243 - accuracy: 0.6728 - val_loss: 0.8914 - val_accuracy: 0.7239\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9641 - accuracy: 0.6969\n",
      "Epoch 00019: val_accuracy improved from 0.72387 to 0.73833, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.9641 - accuracy: 0.6969 - val_loss: 0.8510 - val_accuracy: 0.7383\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9309 - accuracy: 0.7092\n",
      "Epoch 00020: val_accuracy improved from 0.73833 to 0.74975, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.9309 - accuracy: 0.7092 - val_loss: 0.8232 - val_accuracy: 0.7498\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8948 - accuracy: 0.7223\n",
      "Epoch 00021: val_accuracy improved from 0.74975 to 0.76200, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8948 - accuracy: 0.7223 - val_loss: 0.7818 - val_accuracy: 0.7620\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8792 - accuracy: 0.7275\n",
      "Epoch 00022: val_accuracy improved from 0.76200 to 0.76208, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8792 - accuracy: 0.7275 - val_loss: 0.7748 - val_accuracy: 0.7621\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8604 - accuracy: 0.7303\n",
      "Epoch 00023: val_accuracy improved from 0.76208 to 0.76543, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8604 - accuracy: 0.7303 - val_loss: 0.7588 - val_accuracy: 0.7654\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8331 - accuracy: 0.7380\n",
      "Epoch 00024: val_accuracy improved from 0.76543 to 0.76710, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8331 - accuracy: 0.7380 - val_loss: 0.7571 - val_accuracy: 0.7671\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8242 - accuracy: 0.7427\n",
      "Epoch 00025: val_accuracy improved from 0.76710 to 0.78130, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.8242 - accuracy: 0.7427 - val_loss: 0.7180 - val_accuracy: 0.7813\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8020 - accuracy: 0.7501\n",
      "Epoch 00026: val_accuracy improved from 0.78130 to 0.79458, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.8020 - accuracy: 0.7501 - val_loss: 0.6905 - val_accuracy: 0.7946\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7633 - accuracy: 0.7630\n",
      "Epoch 00027: val_accuracy improved from 0.79458 to 0.79620, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7633 - accuracy: 0.7630 - val_loss: 0.6736 - val_accuracy: 0.7962\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7528 - accuracy: 0.7650\n",
      "Epoch 00028: val_accuracy improved from 0.79620 to 0.79693, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7528 - accuracy: 0.7650 - val_loss: 0.6660 - val_accuracy: 0.7969\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7408 - accuracy: 0.7694\n",
      "Epoch 00029: val_accuracy did not improve from 0.79693\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7408 - accuracy: 0.7694 - val_loss: 0.6652 - val_accuracy: 0.7954\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7418 - accuracy: 0.7688\n",
      "Epoch 00030: val_accuracy did not improve from 0.79693\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7418 - accuracy: 0.7688 - val_loss: 0.6652 - val_accuracy: 0.7924\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7495 - accuracy: 0.7646\n",
      "Epoch 00031: val_accuracy improved from 0.79693 to 0.80357, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7495 - accuracy: 0.7646 - val_loss: 0.6397 - val_accuracy: 0.8036\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7204 - accuracy: 0.7742\n",
      "Epoch 00032: val_accuracy improved from 0.80357 to 0.80850, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.7204 - accuracy: 0.7742 - val_loss: 0.6267 - val_accuracy: 0.8085\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.7847\n",
      "Epoch 00033: val_accuracy improved from 0.80850 to 0.81862, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6886 - accuracy: 0.7847 - val_loss: 0.5974 - val_accuracy: 0.8186\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6633 - accuracy: 0.7944\n",
      "Epoch 00034: val_accuracy improved from 0.81862 to 0.82197, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.6633 - accuracy: 0.7944 - val_loss: 0.5822 - val_accuracy: 0.8220\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6505 - accuracy: 0.7971\n",
      "Epoch 00035: val_accuracy improved from 0.82197 to 0.82938, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6505 - accuracy: 0.7971 - val_loss: 0.5640 - val_accuracy: 0.8294\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6354 - accuracy: 0.8010\n",
      "Epoch 00036: val_accuracy improved from 0.82938 to 0.83417, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6354 - accuracy: 0.8010 - val_loss: 0.5537 - val_accuracy: 0.8342\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6311 - accuracy: 0.8026\n",
      "Epoch 00037: val_accuracy did not improve from 0.83417\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6311 - accuracy: 0.8026 - val_loss: 0.5510 - val_accuracy: 0.8330\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.8090\n",
      "Epoch 00038: val_accuracy improved from 0.83417 to 0.83988, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6152 - accuracy: 0.8090 - val_loss: 0.5321 - val_accuracy: 0.8399\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.8077\n",
      "Epoch 00039: val_accuracy did not improve from 0.83988\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6098 - accuracy: 0.8077 - val_loss: 0.5373 - val_accuracy: 0.8364\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6204 - accuracy: 0.8056\n",
      "Epoch 00040: val_accuracy did not improve from 0.83988\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.6204 - accuracy: 0.8056 - val_loss: 0.5304 - val_accuracy: 0.8389\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.8139\n",
      "Epoch 00041: val_accuracy improved from 0.83988 to 0.84398, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5956 - accuracy: 0.8139 - val_loss: 0.5139 - val_accuracy: 0.8440\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5772 - accuracy: 0.8186\n",
      "Epoch 00042: val_accuracy improved from 0.84398 to 0.84495, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5772 - accuracy: 0.8186 - val_loss: 0.5112 - val_accuracy: 0.8450\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5679 - accuracy: 0.8226\n",
      "Epoch 00043: val_accuracy improved from 0.84495 to 0.85188, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5679 - accuracy: 0.8226 - val_loss: 0.4972 - val_accuracy: 0.8519\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5629 - accuracy: 0.8250\n",
      "Epoch 00044: val_accuracy improved from 0.85188 to 0.85368, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5629 - accuracy: 0.8250 - val_loss: 0.4860 - val_accuracy: 0.8537\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.8264\n",
      "Epoch 00045: val_accuracy improved from 0.85368 to 0.85767, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.5540 - accuracy: 0.8264 - val_loss: 0.4771 - val_accuracy: 0.8577\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5468 - accuracy: 0.8302\n",
      "Epoch 00046: val_accuracy improved from 0.85767 to 0.85960, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5468 - accuracy: 0.8302 - val_loss: 0.4664 - val_accuracy: 0.8596\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5341 - accuracy: 0.8329\n",
      "Epoch 00047: val_accuracy improved from 0.85960 to 0.86263, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5341 - accuracy: 0.8329 - val_loss: 0.4558 - val_accuracy: 0.8626\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.8335\n",
      "Epoch 00048: val_accuracy did not improve from 0.86263\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5293 - accuracy: 0.8335 - val_loss: 0.4560 - val_accuracy: 0.8625\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5397 - accuracy: 0.8300\n",
      "Epoch 00049: val_accuracy improved from 0.86263 to 0.86788, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5397 - accuracy: 0.8300 - val_loss: 0.4448 - val_accuracy: 0.8679\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.8335\n",
      "Epoch 00050: val_accuracy did not improve from 0.86788\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5298 - accuracy: 0.8335 - val_loss: 0.4665 - val_accuracy: 0.8585\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.8354\n",
      "Epoch 00051: val_accuracy did not improve from 0.86788\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5220 - accuracy: 0.8354 - val_loss: 0.4439 - val_accuracy: 0.8657\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5303 - accuracy: 0.8308\n",
      "Epoch 00052: val_accuracy did not improve from 0.86788\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5303 - accuracy: 0.8308 - val_loss: 0.4445 - val_accuracy: 0.8640\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5137 - accuracy: 0.8368\n",
      "Epoch 00053: val_accuracy did not improve from 0.86788\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5137 - accuracy: 0.8368 - val_loss: 0.4535 - val_accuracy: 0.8641\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.8410\n",
      "Epoch 00054: val_accuracy improved from 0.86788 to 0.87322, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.5031 - accuracy: 0.8410 - val_loss: 0.4204 - val_accuracy: 0.8732\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4953 - accuracy: 0.8440\n",
      "Epoch 00055: val_accuracy improved from 0.87322 to 0.87632, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4953 - accuracy: 0.8440 - val_loss: 0.4119 - val_accuracy: 0.8763\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4822 - accuracy: 0.8463\n",
      "Epoch 00056: val_accuracy improved from 0.87632 to 0.87773, saving model to best_model.h5\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.4822 - accuracy: 0.8463 - val_loss: 0.4076 - val_accuracy: 0.8777\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4787 - accuracy: 0.8468\n",
      "Epoch 00057: val_accuracy did not improve from 0.87773\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.4787 - accuracy: 0.8468 - val_loss: 0.4109 - val_accuracy: 0.8770\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.8491\n",
      "Epoch 00058: val_accuracy improved from 0.87773 to 0.88315, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4781 - accuracy: 0.8491 - val_loss: 0.3923 - val_accuracy: 0.8831\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4663 - accuracy: 0.8525\n",
      "Epoch 00059: val_accuracy did not improve from 0.88315\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4663 - accuracy: 0.8525 - val_loss: 0.3978 - val_accuracy: 0.8794\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.8529\n",
      "Epoch 00060: val_accuracy did not improve from 0.88315\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4602 - accuracy: 0.8529 - val_loss: 0.3927 - val_accuracy: 0.8816\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4509 - accuracy: 0.8554\n",
      "Epoch 00061: val_accuracy did not improve from 0.88315\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4509 - accuracy: 0.8554 - val_loss: 0.3934 - val_accuracy: 0.8817\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4520 - accuracy: 0.8564\n",
      "Epoch 00062: val_accuracy improved from 0.88315 to 0.88357, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.4520 - accuracy: 0.8564 - val_loss: 0.3843 - val_accuracy: 0.8836\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.8547\n",
      "Epoch 00063: val_accuracy improved from 0.88357 to 0.88877, saving model to best_model.h5\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.4521 - accuracy: 0.8547 - val_loss: 0.3744 - val_accuracy: 0.8888\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.8581\n",
      "Epoch 00064: val_accuracy did not improve from 0.88877\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.4427 - accuracy: 0.8581 - val_loss: 0.3749 - val_accuracy: 0.8877\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4314 - accuracy: 0.8617\n",
      "Epoch 00065: val_accuracy improved from 0.88877 to 0.89072, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4314 - accuracy: 0.8617 - val_loss: 0.3663 - val_accuracy: 0.8907\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4294 - accuracy: 0.8640\n",
      "Epoch 00066: val_accuracy did not improve from 0.89072\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4294 - accuracy: 0.8640 - val_loss: 0.3784 - val_accuracy: 0.8861\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4334 - accuracy: 0.8614\n",
      "Epoch 00067: val_accuracy did not improve from 0.89072\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4334 - accuracy: 0.8614 - val_loss: 0.3669 - val_accuracy: 0.8899\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4161 - accuracy: 0.8653\n",
      "Epoch 00068: val_accuracy improved from 0.89072 to 0.89718, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4161 - accuracy: 0.8653 - val_loss: 0.3463 - val_accuracy: 0.8972\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4120 - accuracy: 0.8682\n",
      "Epoch 00069: val_accuracy did not improve from 0.89718\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4120 - accuracy: 0.8682 - val_loss: 0.3511 - val_accuracy: 0.8947\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4008 - accuracy: 0.8720\n",
      "Epoch 00070: val_accuracy improved from 0.89718 to 0.89992, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.4008 - accuracy: 0.8720 - val_loss: 0.3356 - val_accuracy: 0.8999\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3981 - accuracy: 0.8710\n",
      "Epoch 00071: val_accuracy did not improve from 0.89992\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3981 - accuracy: 0.8710 - val_loss: 0.3425 - val_accuracy: 0.8982\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3988 - accuracy: 0.8718\n",
      "Epoch 00072: val_accuracy did not improve from 0.89992\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.3988 - accuracy: 0.8718 - val_loss: 0.3382 - val_accuracy: 0.8995\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.8732\n",
      "Epoch 00073: val_accuracy improved from 0.89992 to 0.90247, saving model to best_model.h5\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3969 - accuracy: 0.8732 - val_loss: 0.3299 - val_accuracy: 0.9025\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3889 - accuracy: 0.8747\n",
      "Epoch 00074: val_accuracy did not improve from 0.90247\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3889 - accuracy: 0.8747 - val_loss: 0.3348 - val_accuracy: 0.8992\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3915 - accuracy: 0.8739\n",
      "Epoch 00075: val_accuracy did not improve from 0.90247\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3915 - accuracy: 0.8739 - val_loss: 0.3326 - val_accuracy: 0.9019\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3817 - accuracy: 0.8765\n",
      "Epoch 00076: val_accuracy improved from 0.90247 to 0.90337, saving model to best_model.h5\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3817 - accuracy: 0.8765 - val_loss: 0.3261 - val_accuracy: 0.9034\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8779\n",
      "Epoch 00077: val_accuracy improved from 0.90337 to 0.90650, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.3771 - accuracy: 0.8779 - val_loss: 0.3167 - val_accuracy: 0.9065\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3674 - accuracy: 0.8805\n",
      "Epoch 00078: val_accuracy improved from 0.90650 to 0.90858, saving model to best_model.h5\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.3674 - accuracy: 0.8805 - val_loss: 0.3104 - val_accuracy: 0.9086\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3670 - accuracy: 0.8799\n",
      "Epoch 00079: val_accuracy did not improve from 0.90858\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.3670 - accuracy: 0.8799 - val_loss: 0.3281 - val_accuracy: 0.9021\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3712 - accuracy: 0.8800\n",
      "Epoch 00080: val_accuracy did not improve from 0.90858\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.3712 - accuracy: 0.8800 - val_loss: 0.3106 - val_accuracy: 0.9085\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.8819\n",
      "Epoch 00081: val_accuracy improved from 0.90858 to 0.91018, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.3622 - accuracy: 0.8819 - val_loss: 0.3072 - val_accuracy: 0.9102\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.8842\n",
      "Epoch 00082: val_accuracy improved from 0.91018 to 0.91102, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.3509 - accuracy: 0.8842 - val_loss: 0.3041 - val_accuracy: 0.9110\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.8862\n",
      "Epoch 00083: val_accuracy improved from 0.91102 to 0.91177, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.3527 - accuracy: 0.8862 - val_loss: 0.3035 - val_accuracy: 0.9118\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3491 - accuracy: 0.8870\n",
      "Epoch 00084: val_accuracy did not improve from 0.91177\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.3491 - accuracy: 0.8870 - val_loss: 0.3053 - val_accuracy: 0.9098\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3540 - accuracy: 0.8838\n",
      "Epoch 00085: val_accuracy did not improve from 0.91177\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.3540 - accuracy: 0.8838 - val_loss: 0.3047 - val_accuracy: 0.9116\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.8865\n",
      "Epoch 00086: val_accuracy did not improve from 0.91177\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3493 - accuracy: 0.8865 - val_loss: 0.3026 - val_accuracy: 0.9116\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3604 - accuracy: 0.8809\n",
      "Epoch 00087: val_accuracy did not improve from 0.91177\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.3604 - accuracy: 0.8809 - val_loss: 0.3176 - val_accuracy: 0.9051\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3656 - accuracy: 0.8805\n",
      "Epoch 00088: val_accuracy improved from 0.91177 to 0.91278, saving model to best_model.h5\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.3656 - accuracy: 0.8805 - val_loss: 0.3000 - val_accuracy: 0.9128\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.8878\n",
      "Epoch 00089: val_accuracy improved from 0.91278 to 0.91820, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3443 - accuracy: 0.8878 - val_loss: 0.2848 - val_accuracy: 0.9182\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3337 - accuracy: 0.8926\n",
      "Epoch 00090: val_accuracy did not improve from 0.91820\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3337 - accuracy: 0.8926 - val_loss: 0.2905 - val_accuracy: 0.9146\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3267 - accuracy: 0.8932\n",
      "Epoch 00091: val_accuracy did not improve from 0.91820\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3267 - accuracy: 0.8932 - val_loss: 0.2874 - val_accuracy: 0.9173\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3261 - accuracy: 0.8932\n",
      "Epoch 00092: val_accuracy improved from 0.91820 to 0.92038, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3261 - accuracy: 0.8932 - val_loss: 0.2755 - val_accuracy: 0.9204\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.8968\n",
      "Epoch 00093: val_accuracy improved from 0.92038 to 0.92047, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3171 - accuracy: 0.8968 - val_loss: 0.2762 - val_accuracy: 0.9205\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.8971\n",
      "Epoch 00094: val_accuracy did not improve from 0.92047\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3126 - accuracy: 0.8971 - val_loss: 0.2716 - val_accuracy: 0.9203\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.8981\n",
      "Epoch 00095: val_accuracy improved from 0.92047 to 0.92315, saving model to best_model.h5\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.3135 - accuracy: 0.8981 - val_loss: 0.2691 - val_accuracy: 0.9232\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.8988\n",
      "Epoch 00096: val_accuracy improved from 0.92315 to 0.92458, saving model to best_model.h5\n",
      "10/10 [==============================] - 35s 4s/step - loss: 0.3073 - accuracy: 0.8988 - val_loss: 0.2645 - val_accuracy: 0.9246\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.9031\n",
      "Epoch 00097: val_accuracy did not improve from 0.92458\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.2972 - accuracy: 0.9031 - val_loss: 0.2784 - val_accuracy: 0.9193\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.9003\n",
      "Epoch 00098: val_accuracy improved from 0.92458 to 0.92500, saving model to best_model.h5\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.3008 - accuracy: 0.9003 - val_loss: 0.2631 - val_accuracy: 0.9250\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.9019\n",
      "Epoch 00099: val_accuracy improved from 0.92500 to 0.92603, saving model to best_model.h5\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.2994 - accuracy: 0.9019 - val_loss: 0.2606 - val_accuracy: 0.9260\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.9002\n",
      "Epoch 00100: val_accuracy improved from 0.92603 to 0.92808, saving model to best_model.h5\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.3010 - accuracy: 0.9002 - val_loss: 0.2550 - val_accuracy: 0.9281\n"
     ]
    }
   ],
   "source": [
    "# Configuration for architecture\n",
    "\n",
    "# Initializing the model\n",
    "regressorModel = Sequential()\n",
    "# Add one layer to begin with \n",
    "regressorModel.add(Dense(input_dim = 1024, units=2048,kernel_initializer='he_normal'))\n",
    "regressorModel.add(Activation('relu'))  \n",
    "regressorModel.add(Dropout(0.2))\n",
    "regressorModel.add(Dense(units=2048,kernel_initializer='he_normal')) \n",
    "regressorModel.add(Activation('relu'))  \n",
    "regressorModel.add(Dropout(0.2))\n",
    "regressorModel.add(Dense(units=2048,kernel_initializer='he_normal'))\n",
    "regressorModel.add(Activation('relu'))  \n",
    "regressorModel.add(Dropout(0.2))\n",
    "regressorModel.add(Dense(units=10,activation='softmax',kernel_initializer='he_normal'))\n",
    "\n",
    "# Configuration for compiler\n",
    "from tensorflow.keras import optimizers\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "regressorModel.compile(optimizer=adam ,loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# simple early stopping\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1 , patience=25)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# Fit the model\n",
    "historyRegressorModel=regressorModel.fit(NN_X_Train,NN_Y_Train, epochs=100,batch_size=4200,validation_data=(NN_X_Val,NN_Y_Val), callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################  Evaluate Model On Test Data ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9031428694725037 0.9280833601951599\n"
     ]
    }
   ],
   "source": [
    "best_train_score = max(historyRegressorModel.history['accuracy'])\n",
    "best_val_score = max(historyRegressorModel.history['val_accuracy'])\n",
    "print(best_train_score,best_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2550 - accuracy: 0.9281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25498467683792114, 0.9280833601951599]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross verify score on Validation data of the best model\n",
    "\n",
    "saved_model.evaluate(np.array(NN_X_Val),np.array(NN_Y_Val))\n",
    "\n",
    "# Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 11s 20ms/step - loss: 0.4715 - accuracy: 0.8673\n"
     ]
    }
   ],
   "source": [
    "# Lets Look at score on Test Data\n",
    "\n",
    "(Test_Loss,Test_acc)=saved_model.evaluate(np.array(NN_X_Test),np.array(NN_Y_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Type</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "      <th>Test_Acc</th>\n",
       "      <th>Hidden layers</th>\n",
       "      <th>Hyper Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.9792857</td>\n",
       "      <td>0.9410333</td>\n",
       "      <td>0.8470556</td>\n",
       "      <td>3 layer(2048 units) with relu activation</td>\n",
       "      <td>optimizer-Adam with lr 0.001,Weights-He</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.9444762</td>\n",
       "      <td>0.9161000</td>\n",
       "      <td>0.8511111</td>\n",
       "      <td>3 layer(2048 units) with relu activation</td>\n",
       "      <td>optimizer-Adam with lr 0.001,Weights-He,L2 Reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.9998810</td>\n",
       "      <td>0.9355834</td>\n",
       "      <td>0.8362778</td>\n",
       "      <td>3 layer(2048 units) with relu activation</td>\n",
       "      <td>optimizer-Adam with lr 0.001,Weights-He,Batch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.9031429</td>\n",
       "      <td>0.9280834</td>\n",
       "      <td>0.8673334</td>\n",
       "      <td>3 layer(2048 units) with relu activation</td>\n",
       "      <td>optimizer-Adam with lr 0.001,Weights-He,Drop out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model_Type  Train_Acc   Val_Acc  Test_Acc  \\\n",
       "0  Neural Network  0.9792857 0.9410333 0.8470556   \n",
       "1  Neural Network  0.9444762 0.9161000 0.8511111   \n",
       "2  Neural Network  0.9998810 0.9355834 0.8362778   \n",
       "3  Neural Network  0.9031429 0.9280834 0.8673334   \n",
       "\n",
       "                              Hidden layers  \\\n",
       "0  3 layer(2048 units) with relu activation   \n",
       "1  3 layer(2048 units) with relu activation   \n",
       "2  3 layer(2048 units) with relu activation   \n",
       "3  3 layer(2048 units) with relu activation   \n",
       "\n",
       "                                        Hyper Params  \n",
       "0            optimizer-Adam with lr 0.001,Weights-He  \n",
       "1  optimizer-Adam with lr 0.001,Weights-He,L2 Reg...  \n",
       "2  optimizer-Adam with lr 0.001,Weights-He,Batch ...  \n",
       "3   optimizer-Adam with lr 0.001,Weights-He,Drop out  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capturing best results from above epoch\n",
    "ResultsDF=ResultsDF.append({ 'Model_Type' :\"Neural Network\",'Train_Acc':best_train_score,'Val_Acc':best_val_score,'Test_Acc':Test_acc,'Hidden layers':\"3 layer(2048 units) with relu activation\" ,'Hyper Params':'optimizer-Adam with lr 0.001,Weights-He,Drop out'},ignore_index=True)\n",
    "ResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Blank Space\n",
    "\n",
    "## Blank Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model & Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We started off  with a 2 layer Neural Network model and were able to achieve a test accuracy of 83% to begin with               \n",
    "#### Post that we began tuning the model wrt its various hyper paramters , results of which are displayed in the table above along with the hyper params used                                                                                               \n",
    "#### We were able to bring up the Test Data accuracy to ~87% with the 4th trial as listed above and this will be chosen as the final model                \n",
    "#### The final results are Train -90.3%, Validation -92.8% and Test Data -86.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Plot Train Vs Validation ########################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cfaa19d6d0>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cfad7cfb80>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cfaa19d100>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss per iteration\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(historyRegressorModel.history['loss'], label='loss')\n",
    "plt.plot(historyRegressorModel.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cfa9ef5370>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cf92e67760>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cf92e67970>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy per iteration\n",
    "plt.plot(historyRegressorModel.history['accuracy'], label='accuracy')\n",
    "plt.plot(historyRegressorModel.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot  Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the Training loss Vs Validation Loss , we can see that there is smooth decrease is loss starting from the very first epoch and it continues to decrease right upto the last epoch\n",
    "#### The model could have been trained further as the loss continues to decrease \n",
    "#### Both train and validtion performance is comparable and we cna clearly see that there is no overfit yet\n",
    "#### A similar trend can be observed with the train Vs Validation accuracy hwere we can see that both sets can see a gradual increase in accuarcy upto the last epoch , further training could still be done "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: FinalModel\\assets\n"
     ]
    }
   ],
   "source": [
    "model = saved_model\n",
    "model.save('FinalModel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytf",
   "language": "python",
   "name": "mytf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
